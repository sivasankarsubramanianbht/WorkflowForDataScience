{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyOnj6IuYCwwNV9bn2j81NUV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":304},"id":"wPJ710VLppFx","executionInfo":{"status":"error","timestamp":1752602735776,"user_tz":-120,"elapsed":1230,"user":{"displayName":"Jai Kushwaha","userId":"01340704951735287618"}},"outputId":"a4d88684-7122-4b23-c69c-bd248b0accce"},"execution_count":14,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"Mountpoint must not already contain files","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-14-3329394316.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    197\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must not be a symlink'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must not already contain files'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must either be a directory or not exist'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Mountpoint must not already contain files"]}]},{"cell_type":"code","source":["# Access files in your Google Drive\n","import os\n","os.listdir('/content/drive/MyDrive/Colab Notebooks/flight_delay')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HyqDPuX0ppDe","executionInfo":{"status":"ok","timestamp":1752602747014,"user_tz":-120,"elapsed":23,"user":{"displayName":"Jai Kushwaha","userId":"01340704951735287618"}},"outputId":"498168e5-e687-49bb-cd1a-9d395aa66ca6"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['plots',\n"," 'model_params',\n"," '.gradio',\n"," 'data',\n"," '=2.0.0',\n"," 'models',\n"," 'reports',\n"," '=4.0.0']"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":142},"id":"BQFd5eWipf-U","executionInfo":{"status":"ok","timestamp":1752601571040,"user_tz":-120,"elapsed":5956,"user":{"displayName":"Jai Kushwaha","userId":"01340704951735287618"}},"outputId":"e7591aa9-ae43-4f23-aa02-9b2c9327f8ce"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-a76e03ed-1efc-41b6-8d7c-1d6df6c2452b\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-a76e03ed-1efc-41b6-8d7c-1d6df6c2452b\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving kaggle.json to kaggle.json\n","Uploaded kaggle.json\n","Current working directory: /content/drive/MyDrive/Colab Notebooks/flight_delay\n","Directories created and Python path updated.\n","Kaggle API setup complete. Ensure 'kaggle.json' was uploaded to the correct path.\n"]}],"source":["# Install required libraries\n","# !pip install pandas numpy scikit-learn tensorflow matplotlib seaborn joblib scikeras shap kaggle ydata-profiling>=4.0.0 category_encoders>=2.0.0 -q\n","\n","# Create project directories if they don't exist\n","import os\n","import sys\n","from google.colab import files\n","# Define base directory (e.g., flight_delay_project under /content/)\n","BASE_DIR = '/content/drive/MyDrive/Colab Notebooks/flight_delay'\n","# os.makedirs(os.path.join(BASE_DIR, 'data', 'raw'), exist_ok=True)\n","# os.makedirs(os.path.join(BASE_DIR, 'data', 'processed'), exist_ok=True)\n","# os.makedirs(os.path.join(BASE_DIR, 'scripts'), exist_ok=True)\n","# os.makedirs(os.path.join(BASE_DIR, 'models'), exist_ok=True)\n","# os.makedirs(os.path.join(BASE_DIR, 'model_params'), exist_ok=True)\n","# os.makedirs(os.path.join(BASE_DIR, 'plots', 'eda'), exist_ok=True)\n","# os.makedirs(os.path.join(BASE_DIR, 'plots', 'model_evaluation'), exist_ok=True)\n","# os.makedirs(os.path.join(BASE_DIR, 'plots', 'explainability'), exist_ok=True)\n","# os.makedirs(os.path.join(BASE_DIR, 'reports'), exist_ok=True)\n","\n","# Change current working directory to BASE_DIR\n","os.chdir(BASE_DIR)\n","\n","# Upload kaggle.json\n","uploaded = files.upload()\n","\n","# Verify file uploaded\n","for filename in uploaded:\n","    print(f'Uploaded {filename}')\n","# Add scripts directory to Python path\n","sys.path.insert(0, os.path.join(BASE_DIR, 'scripts'))\n","\n","print(f\"Current working directory: {os.getcwd()}\")\n","print(\"Directories created and Python path updated.\")\n","\n","# Kaggle API Setup (IMPORTANT!)\n","# Follow these steps:\n","# 1. Go to Kaggle.com -> Your Profile -> Account.\n","# 2. Under 'API', click 'Create New API Token'. This downloads 'kaggle.json'.\n","# 3. Upload 'kaggle.json' to your Colab environment.\n","#    You can click the folder icon on the left sidebar in Colab, then \"Upload to session storage\".\n","#    Make sure you upload it directly into the BASE_DIR path you defined above (e.g., /content/flight_delay_project/).\n","# 4. Run the following commands to move and secure the file:\n","!mkdir -p ~/.kaggle/\n","!mv kaggle.json ~/.kaggle/kaggle.json\n","!chmod 600 ~/.kaggle/kaggle.json\n","\n","print(\"Kaggle API setup complete. Ensure 'kaggle.json' was uploaded to the correct path.\")"]},{"cell_type":"code","source":["os.chdir(BASE_DIR)\n","print(f\"Current working directory: {os.getcwd()}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4V5QkPfusYyS","executionInfo":{"status":"ok","timestamp":1752319827619,"user_tz":-120,"elapsed":16,"user":{"displayName":"Jai Kushwaha","userId":"01340704951735287618"}},"outputId":"a678e1bd-c824-4dba-ee6d-a11f68e77520"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Current working directory: /content/drive/MyDrive/Colab Notebooks/flight_delay\n"]}]},{"cell_type":"markdown","source":["## Package Installation"],"metadata":{"id":"GgJfFrh7xpbk"}},{"cell_type":"code","source":["!pip install pandas numpy scikit-learn tensorflow matplotlib seaborn joblib scikeras shap kaggle ydata-profiling>=4.0.0 category_encoders>=2.0.0 gradio keras-tuner wandb -q"],"metadata":{"id":"OJ3e4wy7UDRH","executionInfo":{"status":"ok","timestamp":1752602725153,"user_tz":-120,"elapsed":19018,"user":{"displayName":"Jai Kushwaha","userId":"01340704951735287618"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["## Data Modelling\n"],"metadata":{"id":"piT2R-jSJSig"}},{"cell_type":"code","source":["%%writefile scripts/data_modelling.py\n","\n","import pandas as pd\n","import numpy as np\n","import logging\n","\n","# For Data Splitting and Preprocessing\n","from sklearn.model_selection import KFold\n","\n","# For Regression Models\n","from sklearn.linear_model import Ridge\n","from sklearn.ensemble import RandomForestRegressor\n","\n","# For Classification Models\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.dummy import DummyClassifier # NEW: Import for baseline classification model\n","\n","# For Neural Network\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.losses import MeanSquaredError, BinaryCrossentropy\n","from tensorflow.keras.metrics import MeanAbsoluteError, BinaryAccuracy, AUC # Ensure AUC is imported\n","from tensorflow.keras.callbacks import EarlyStopping # Explicitly import EarlyStopping\n","\n","# nn_utils for using in tuning and modelling\n","from scripts.nn_utils import build_nn_model # This is where your build_nn_model should be located\n","\n","# Import ModelTuner\n","from scripts.model_tuning import ModelTuner\n","\n","\n","# Configure a logger for this module\n","logger = logging.getLogger(__name__)\n","logger.setLevel(logging.INFO)\n","if not logger.handlers:\n","    console_handler = logging.StreamHandler()\n","    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n","    console_handler.setFormatter(formatter)\n","    logger.addHandler(console_handler)\n","\n","class DataModeling:\n","    \"\"\"\n","    Handles training of various regression and classification models.\n","    \"\"\"\n","    def __init__(self, tuner: ModelTuner): # Accept a ModelTuner instance\n","        self.tuner = tuner\n","        logger.info(\"DataModeling initialized.\")\n","\n","    def run_baseline_model(self, y_train: pd.Series, y_test: pd.Series) -> tuple[np.ndarray, None]:\n","        \"\"\"\n","        Runs a baseline model that predicts the average of the training target (for regression).\n","\n","        Args:\n","            y_train (pd.Series): Training target values.\n","            y_test (pd.Series): Test target values.\n","\n","        Returns:\n","            tuple[np.ndarray, None]: Predicted values (no model object for baseline).\n","        \"\"\"\n","        logger.info(\"--- Running Baseline Regression Model (Average Prediction) ---\")\n","        y_pred_baseline = np.full(len(y_test), y_train.mean())\n","        return y_pred_baseline, None # No model object to return for baseline (as it's a simple constant)\n","\n","    def run_baseline_model_classification(self, y_train_val: pd.Series, y_test: pd.Series) -> tuple[np.ndarray, DummyClassifier]:\n","        \"\"\"\n","        Trains and returns a baseline classification model (predicts majority class).\n","\n","        Args:\n","            y_train_val (pd.Series): Training + Validation target values to find majority class.\n","            y_test (pd.Series): Test target values (used for determining prediction array size).\n","\n","        Returns:\n","            tuple[np.ndarray, DummyClassifier]: Predicted class labels for the test set, and the trained DummyClassifier model.\n","        \"\"\"\n","        logger.info(\"--- Running Baseline Classification Model (Majority Class Classifier) ---\")\n","\n","        # Initialize DummyClassifier with 'most_frequent' strategy\n","        baseline_model = DummyClassifier(strategy='most_frequent', random_state=42)\n","\n","        # Fit the model. X is not truly used, but required for the fit method.\n","        # We just need to fit on y_train_val to determine the most frequent class.\n","        # We create a dummy X with the same number of samples as y_train_val.\n","        dummy_X = np.zeros((len(y_train_val), 1))\n","        baseline_model.fit(dummy_X, y_train_val)\n","\n","        # Predict on a dummy X for the test set to get the predictions\n","        dummy_X_test = np.zeros((len(y_test), 1))\n","        y_pred_baseline = baseline_model.predict(dummy_X_test)\n","\n","        majority_class = baseline_model.predict(np.array([[0]]))\n","        if majority_class[0] == 0:\n","            y_pred_proba_baseline = np.zeros(len(y_test))\n","        else: # majority_class[0] == 1\n","            y_pred_proba_baseline = np.ones(len(y_test))\n","\n","        logger.info(f\"Baseline (majority class) for training data: {majority_class[0]}\")\n","        logger.info(\"Baseline Classification Model training complete.\")\n","\n","        return y_pred_baseline, baseline_model # Return actual predictions and the model itself\n","\n","    def run_ridge_regression(self, X_train_val: pd.DataFrame, y_train_val: pd.Series) -> Ridge:\n","        \"\"\"\n","        Trains a Ridge Regression model using hyperparameter tuning on the combined\n","        training and validation set.\n","\n","        Args:\n","            X_train_val (pd.DataFrame): Training + Validation features.\n","            y_train_val (pd.Series): Training + Validation target.\n","\n","        Returns:\n","            Ridge: The best trained Ridge model.\n","        \"\"\"\n","        logger.info(\"\\n--- Running Ridge Regression ---\")\n","        ridge = Ridge(random_state=42)\n","        # Eased out param_grid for faster tuning\n","        param_grid = {'alpha': [1, 10]} # Smaller, more focused grid\n","\n","        # Attempt to load best params first\n","        best_params_loaded = self.tuner.get_best_params(\"ridge_regression\")\n","\n","        if best_params_loaded:\n","            logger.info(\"Using loaded best parameters for Ridge Regression to train final model.\")\n","            best_ridge = Ridge(**best_params_loaded, random_state=42)\n","            best_ridge.fit(X_train_val, y_train_val)\n","        else:\n","            logger.info(\"No saved best parameters found. Performing full hyperparameter tuning for Ridge Regression.\")\n","            best_ridge, _, _ = self.tuner.tune_model(\n","                model=ridge,\n","                param_grid=param_grid,\n","                X=X_train_val, # Use X_train_val for tuning\n","                y=y_train_val, # Use y_train_val for tuning\n","                model_name=\"ridge_regression\",\n","                scoring='neg_mean_absolute_error' # Ensure correct scoring for regression\n","            )\n","        return best_ridge\n","\n","    def run_random_forest_regression(self, X_train_val: pd.DataFrame, y_train_val: pd.Series) -> RandomForestRegressor:\n","        \"\"\"\n","        Trains a Random Forest Regressor using hyperparameter tuning on the combined\n","        training and validation set.\n","\n","        Args:\n","            X_train_val (pd.DataFrame): Training + Validation features.\n","            y_train_val (pd.Series): Training + Validation target.\n","\n","        Returns:\n","            RandomForestRegressor: The best trained Random Forest model.\n","        \"\"\"\n","        logger.info(\"\\n--- Running Random Forest Regression ---\")\n","        rf_model = RandomForestRegressor(random_state=42, n_jobs=-1)\n","\n","        # Eased out param_grid for faster tuning\n","        param_grid = {\n","            'n_estimators': [50, 100], # Fewer estimators\n","            'max_depth': [5, 10],      # Fewer max depths\n","            'min_samples_split': [2]   # Single value or very few\n","        }\n","\n","        # Attempt to load best params first\n","        best_params_loaded = self.tuner.get_best_params(\"random_forest_regression\")\n","\n","        if best_params_loaded:\n","            logger.info(\"Using loaded best parameters for Random Forest Regression to train final model.\")\n","            best_rf = RandomForestRegressor(**best_params_loaded, random_state=42, n_jobs=-1)\n","            best_rf.fit(X_train_val, y_train_val)\n","        else:\n","            logger.info(\"No saved best parameters found. Performing full hyperparameter tuning for Random Forest Regression.\")\n","            best_rf, _, _ = self.tuner.tune_model(\n","                model=rf_model,\n","                param_grid=param_grid,\n","                X=X_train_val, # Use X_train_val for tuning\n","                y=y_train_val, # Use y_train_val for tuning\n","                model_name=\"random_forest_regression\",\n","                search_method='random', # Using random search\n","                n_iter=3, # Reduced number of iterations for random search\n","                scoring='neg_mean_absolute_error' # Ensure correct scoring for regression\n","            )\n","        return best_rf\n","\n","    def run_logistic_regression(self, X_train_val: pd.DataFrame, y_train_val: pd.Series) -> LogisticRegression:\n","        \"\"\"\n","        Trains a Logistic Regression model using hyperparameter tuning for classification.\n","\n","        Args:\n","            X_train_val (pd.DataFrame): Training + Validation features.\n","            y_train_val (pd.Series): Training + Validation target.\n","\n","        Returns:\n","            LogisticRegression: The best trained Logistic Regression model.\n","        \"\"\"\n","        logger.info(\"\\n--- Running Logistic Regression (Classification) ---\")\n","        log_reg = LogisticRegression(random_state=42, solver='liblinear', max_iter=500, class_weight='balanced')\n","\n","        # Eased out param_grid for faster tuning\n","        param_grid = {\n","            'C': [1, 10],       # Fewer C values\n","            'penalty': ['l1']   # Single penalty type\n","        }\n","\n","        best_params_loaded = self.tuner.get_best_params(\"logistic_regression\")\n","\n","        if best_params_loaded:\n","            logger.info(\"Using loaded best parameters for Logistic Regression to train final model.\")\n","            best_log_reg = LogisticRegression(**best_params_loaded, random_state=42, solver='liblinear', max_iter=500)\n","            best_log_reg.fit(X_train_val, y_train_val)\n","        else:\n","            logger.info(\"No saved best parameters found. Performing full hyperparameter tuning for Logistic Regression.\")\n","            best_log_reg, _, _ = self.tuner.tune_model(\n","                model=log_reg,\n","                param_grid=param_grid,\n","                X=X_train_val,\n","                y=y_train_val,\n","                model_name=\"logistic_regression\",\n","                scoring='f1' # Common scoring for classification, especially with imbalance\n","            )\n","        return best_log_reg\n","\n","    def run_neural_network(self, X_train: pd.DataFrame, y_train: pd.Series,\n","                           X_val: pd.DataFrame, y_val: pd.Series, task_type: str = 'regression') -> tf.keras.models.Sequential:\n","        \"\"\"\n","        Trains a Neural Network model for either regression or classification.\n","\n","        Args:\n","            X_train (pd.DataFrame): Training features.\n","            y_train (pd.Series): Training target.\n","            X_val (pd.DataFrame): Validation features.\n","            y_val (pd.Series): Validation target.\n","            task_type (str): 'regression' or 'classification'.\n","\n","        Returns:\n","            tf.keras.models.Sequential: The trained Keras model.\n","        \"\"\"\n","        logger.info(f\"\\n--- Running Neural Network ({task_type.capitalize()}) ---\")\n","        tf.random.set_seed(42)\n","        np.random.seed(42)\n","\n","        # Default params, will be overridden by best_hyperparameters if tuning occurs\n","        nn_model_params = {\n","            'input_dim': X_train.shape[1],\n","            'hidden_layers': 2,\n","            'neurons': 64,\n","            'dropout_rate': 0.1,\n","            'learning_rate': 0.001,\n","            'task_type': task_type # Pass task type to build_nn_model\n","        }\n","\n","        # Check if we have best params for 'neural_network_regression' or 'neural_network_classification'\n","        model_name_for_params = f\"neural_network_{task_type}\"\n","        best_params_loaded = self.tuner.get_best_params(model_name_for_params)\n","\n","        if best_params_loaded:\n","            logger.info(f\"Using loaded best parameters for {model_name_for_params} to train final model.\")\n","            # Merge loaded params with fixed ones (like input_dim, task_type)\n","            nn_model_params.update(best_params_loaded)\n","            # Ensure task_type is correctly set for build_nn_model\n","            nn_model_params['task_type'] = task_type\n","\n","        nn_model = build_nn_model(**nn_model_params)\n","\n","        logger.info(f\"Neural Network Model Summary ({task_type}):\\n{nn_model.summary()}\")\n","\n","        early_stopping = EarlyStopping(\n","            monitor='val_loss',\n","            patience=5, # Reduced patience for faster convergence\n","            restore_best_weights=True\n","        )\n","\n","        history = nn_model.fit(X_train, y_train,\n","                               validation_data=(X_val, y_val),\n","                               epochs=50, # Reduced max epochs with early stopping\n","                               batch_size=32,\n","                               verbose=0, # Set to 1 for progress bar during training\n","                               callbacks=[early_stopping])\n","\n","        logger.info(f\"Neural Network {task_type.capitalize()} Model training complete.\")\n","        return nn_model\n","\n","    def run_neural_network_classification(self, X_train: pd.DataFrame, y_train: pd.Series, X_val: pd.DataFrame, y_val: pd.Series) -> tf.keras.models.Sequential:\n","        \"\"\"Trains and returns a Neural Network Classification model.\"\"\"\n","        logger.info(\"Training Neural Network Classification Model...\")\n","        input_dim = X_train.shape[1]\n","\n","        best_hyperparameters = None\n","        if self.tuner:\n","            best_hyperparameters = self.tuner.tune_neural_network_classification(X_train, y_train, X_val, y_val)\n","            logger.info(f\"Neural Network best hyperparameters: {best_hyperparameters}\")\n","\n","        if best_hyperparameters:\n","            # Use the build_nn_model function with the tuned hyperparameters\n","            model = build_nn_model(\n","                input_dim=input_dim,\n","                hidden_layers=best_hyperparameters['hidden_layers'],\n","                neurons=best_hyperparameters['neurons'],\n","                dropout_rate=best_hyperparameters['dropout_rate'],\n","                learning_rate=best_hyperparameters['learning_rate'],\n","                task_type='classification' # Explicitly set task_type\n","            )\n","        else:\n","            # Default model for non-tuned scenario (if tuner is None or returns no params)\n","            # These defaults should align with reasonable starting points for build_nn_model\n","            model = build_nn_model(\n","                input_dim=input_dim,\n","                hidden_layers=2,\n","                neurons=64,\n","                dropout_rate=0.3,\n","                learning_rate=0.001,\n","                task_type='classification'\n","            )\n","\n","        # Define epochs and early stopping based on whether tuning occurred\n","        # Note: epochs might be tuned in Keras Tuner, but here we just use early stopping to control\n","        epochs = 50 # Start with a moderate number, EarlyStopping will cut it short\n","        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True) # Reduced patience\n","\n","        model.fit(\n","            X_train, y_train,\n","            epochs=epochs,\n","            batch_size=32,\n","            validation_data=(X_val, y_val),\n","            callbacks=[early_stopping],\n","            verbose=0 # Set to 1 for progress bar during training\n","        )\n","        logger.info(\"Neural Network Classification Model training complete.\")\n","        return model"],"metadata":{"id":"ROZKs3M_sVWH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1752407818928,"user_tz":-120,"elapsed":235,"user":{"displayName":"Jai Kushwaha","userId":"01340704951735287618"}},"outputId":"f7ad77d0-f826-49f2-c14d-59ac9d286636"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting scripts/data_modelling.py\n"]}]},{"cell_type":"markdown","source":["## Data Preprocessing\n"],"metadata":{"id":"OOc0hLYTJWis"}},{"cell_type":"code","source":["%%writefile scripts/data_preprocessor.py\n","\n","import pandas as pd\n","import numpy as np\n","import logging\n","from sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.impute import SimpleImputer\n","import category_encoders as ce\n","from typing import Tuple, Set, Dict, List, Union\n","\n","# Set up logging\n","logger = logging.getLogger(__name__)\n","logger.setLevel(logging.INFO)\n","if not logger.handlers:\n","    console_handler = logging.StreamHandler()\n","    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n","    console_handler.setFormatter(formatter)\n","    logger.addHandler(console_handler)\n","\n","class DataPreprocessor:\n","    \"\"\"\n","    Handles various data preprocessing steps for the flight delay dataset.\n","    \"\"\"\n","    def __init__(self):\n","        self.label_encoders = {} # To store LabelEncoders for inverse transform if needed\n","        self.one_hot_encoder = None # To store the OneHotEncoder transformer\n","        self.numerical_imputer = SimpleImputer(strategy='mean')\n","        self.woe_encoder = None # To store the WOE encoder\n","        logger.info(\"DataPreprocessor initialized.\")\n","\n","    def handle_missing_values(self, df: pd.DataFrame, delay_cols: List[str]) -> pd.DataFrame:\n","        \"\"\"\n","        Handles missing values, specifically for delay reason columns.\n","        Fills NaN in delay reason columns with 0.\n","        Drops rows where 'ARR_DELAY' is NaN (as it's our target).\n","        \"\"\"\n","        logger.info(\"Handling missing values...\")\n","\n","        # Fill NaN in delay reason columns with 0 (assuming NaN means no delay reason from that category)\n","        for col in delay_cols:\n","            if col in df.columns:\n","                df[col] = df[col].fillna(0)\n","                logger.debug(f\"Filled NaN in '{col}' with 0.\")\n","\n","        # Drop rows where 'ARR_DELAY' is NaN, as it's the target variable\n","        initial_rows = df.shape[0]\n","        if 'ARR_DELAY' in df.columns:\n","            df.dropna(subset=['ARR_DELAY'], inplace=True)\n","            rows_dropped = initial_rows - df.shape[0]\n","            if rows_dropped > 0:\n","                logger.info(f\"Dropped {rows_dropped} rows due to NaN values in 'ARR_DELAY'.\")\n","        else:\n","            logger.warning(\"'ARR_DELAY' column not found, skipping NaN drop for target.\")\n","\n","        logger.info(f\"DataFrame shape after handling missing values: {df.shape}\")\n","        return df\n","\n","    def remove_outliers_iqr(self, df: pd.DataFrame, column: str = 'ARR_DELAY', iqr_multiplier: float = 3.0) -> pd.DataFrame:\n","        \"\"\"\n","        Removes outliers from a specified numerical column using the IQR method.\n","        \"\"\"\n","        if column not in df.columns or not pd.api.types.is_numeric_dtype(df[column]):\n","            logger.warning(f\"Column '{column}' not found or not numeric. Skipping outlier removal.\")\n","            return df\n","\n","        Q1 = df[column].quantile(0.25)\n","        Q3 = df[column].quantile(0.75)\n","        IQR = Q3 - Q1\n","\n","        lower_bound = Q1 - iqr_multiplier * IQR\n","        upper_bound = Q3 + iqr_multiplier * IQR\n","\n","        initial_rows = df.shape[0]\n","        df_filtered = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)].copy()\n","        rows_removed = initial_rows - df_filtered.shape[0]\n","\n","        logger.info(f\"Removed {rows_removed} outliers from '{column}' using IQR (multiplier={iqr_multiplier}).\")\n","        logger.info(f\"DataFrame shape after outlier removal: {df_filtered.shape}\")\n","        return df_filtered\n","\n","    def create_elapsed_time_diff(self, df: pd.DataFrame) -> pd.DataFrame:\n","        \"\"\"\n","        Calculates the difference between actual elapsed time and scheduled elapsed time.\n","        Handles missing values in time columns by dropping rows or imputing if necessary.\n","        \"\"\"\n","        logger.info(\"Creating 'ELAPSED_TIME_DIFF' feature...\")\n","\n","        # Define time columns that must be present and numeric\n","        required_time_cols = ['ACTUAL_ELAPSED_TIME', 'CRS_ELAPSED_TIME']\n","\n","        for col in required_time_cols:\n","            if col not in df.columns:\n","                logger.warning(f\"Required time column '{col}' not found. Cannot create 'ELAPSED_TIME_DIFF'.\")\n","                return df\n","            # Ensure they are numeric, coerce errors to NaN\n","            df[col] = pd.to_numeric(df[col], errors='coerce')\n","\n","        # Drop rows where essential time components are NaN\n","        initial_rows = df.shape[0]\n","        df.dropna(subset=required_time_cols, inplace=True)\n","        rows_dropped = initial_rows - df.shape[0]\n","        if rows_dropped > 0:\n","            logger.warning(f\"Dropped {rows_dropped} rows due to missing values in required time columns for 'ELAPSED_TIME_DIFF'.\")\n","\n","        if not df.empty:\n","            df['ELAPSED_TIME_DIFF'] = df['ACTUAL_ELAPSED_TIME'] - df['CRS_ELAPSED_TIME']\n","            logger.info(\"Created 'ELAPSED_TIME_DIFF' column.\")\n","        else:\n","            logger.warning(\"DataFrame is empty after dropping rows for time features; 'ELAPSED_TIME_DIFF' not created.\")\n","\n","        return df\n","\n","    def apply_cyclical_encoding(self, df: pd.DataFrame, time_columns: List[str]) -> pd.DataFrame:\n","        \"\"\"\n","        Applies cyclical (sine and cosine) encoding to time-based features.\n","        Assumes time columns are in minutes (0-1439 for a day).\n","        \"\"\"\n","        logger.info(\"Applying cyclical encoding to time columns...\")\n","        for col in time_columns:\n","            if col in df.columns:\n","                # Convert to numeric, errors will be NaN\n","                df[col] = pd.to_numeric(df[col], errors='coerce')\n","                # Fill NaNs with mean before encoding to avoid NaNs in sin/cos\n","                if df[col].isnull().any():\n","                    col_mean = df[col].mean()\n","                    df[col].fillna(col_mean, inplace=True)\n","                    logger.warning(f\"Filled NaN in '{col}' with mean {col_mean:.2f} before cyclical encoding.\")\n","\n","                max_val = 2359 if 'TIME' in col.upper() else 1439 # Max minutes in 24 hours (HHMM format)\n","                if 'TIME' in col.upper(): # Heuristic for HHMM format\n","                    # Convert HHMM to minutes\n","                    df[f'{col}_MINUTES'] = (df[col] // 100) * 60 + (df[col] % 100)\n","                    col_to_encode = f'{col}_MINUTES'\n","                    max_val_for_sin_cos = 1440 # 24 * 60 minutes\n","                else: # Assume already in minutes or similar scale (e.g. wheels_on/off might be unix timestamp or similar, adjust max_val if needed)\n","                    col_to_encode = col\n","                    max_val_for_sin_cos = df[col].max() if df[col].max() > 0 else 1 # Avoid division by zero\n","\n","                if max_val_for_sin_cos > 0:\n","                    df[f'{col_to_encode}_SIN'] = np.sin(2 * np.pi * df[col_to_encode] / max_val_for_sin_cos)\n","                    df[f'{col_to_encode}_COS'] = np.cos(2 * np.pi * df[col_to_encode] / max_val_for_sin_cos)\n","                    logger.debug(f\"Applied cyclical encoding to '{col_to_encode}'.\")\n","                    # Optionally drop the original column to avoid multicollinearity\n","                    # df.drop(columns=[col_to_encode], inplace=True, errors='ignore')\n","                else:\n","                    logger.warning(f\"Max value for '{col_to_encode}' is zero, skipping cyclical encoding.\")\n","            else:\n","                logger.warning(f\"Time column '{col}' not found. Skipping cyclical encoding.\")\n","        return df\n","\n","    def split_city_state(self, df: pd.DataFrame, city_state_columns: List[str]) -> pd.DataFrame:\n","        \"\"\"\n","        Splits 'CITY, STATE' columns into 'CITY' and 'STATE' columns.\n","        \"\"\"\n","        logger.info(\"Splitting city and state columns...\")\n","        for col in city_state_columns:\n","            if col in df.columns and df[col].astype(str).str.contains(',').any():\n","                df[[f'{col}_CITY', f'{col}_STATE']] = df[col].astype(str).str.split(', ', expand=True)\n","                df.drop(columns=[col], inplace=True)\n","                logger.debug(f\"Split '{col}' into '{col}_CITY' and '{col}_STATE'.\")\n","            else:\n","                logger.warning(f\"Column '{col}' not found or does not contain 'CITY, STATE' format. Skipping split.\")\n","        return df\n","\n","    def add_weekday_weekend_columns(self, df: pd.DataFrame, date_columns: List[str]) -> pd.DataFrame:\n","        \"\"\"\n","        Adds 'DAY_OF_WEEK' and 'IS_WEEKEND' columns from date columns.\n","        \"\"\"\n","        logger.info(\"Adding weekday and weekend columns...\")\n","        for col in date_columns:\n","            if col in df.columns:\n","                try:\n","                    df[col] = pd.to_datetime(df[col])\n","                    df['DAY_OF_WEEK'] = df[col].dt.dayofweek # Monday=0, Sunday=6\n","                    df['IS_WEEKEND'] = df['DAY_OF_WEEK'].isin([5, 6]).astype(int) # Saturday=5, Sunday=6\n","                    logger.debug(f\"Added 'DAY_OF_WEEK' and 'IS_WEEKEND' from '{col}'.\")\n","                except Exception as e:\n","                    logger.error(f\"Error converting '{col}' to datetime or extracting day features: {e}. Skipping.\", exc_info=True)\n","            else:\n","                logger.warning(f\"Date column '{col}' not found. Skipping weekday/weekend features.\")\n","        return df\n","\n","    def encode_categorical_features(self, df: pd.DataFrame) -> pd.DataFrame:\n","        \"\"\"\n","        Applies WOE (Weight of Evidence) encoding to all categorical features based on target variable FLIGHT_STATUS.\n","\n","        Args:\n","            df (pd.DataFrame): The input DataFrame.\n","\n","        Returns:\n","            pd.DataFrame: The DataFrame with WOE encoded categorical features.\n","        \"\"\"\n","        df_copy = df.copy()\n","        logger.info(\"Applying WOE encoding to all categorical features based on FLIGHT_STATUS.\")\n","\n","        # Check if target variable exists\n","        target_variable = 'FLIGHT_STATUS'\n","        if target_variable not in df_copy.columns:\n","            logger.error(f\"Target variable '{target_variable}' not found in DataFrame. Cannot apply WOE encoding.\")\n","            return df_copy\n","\n","        # Identify all categorical columns (excluding the target variable)\n","        categorical_columns = []\n","\n","        # Get columns with object dtype (string/categorical)\n","        object_cols = df_copy.select_dtypes(include=['object']).columns.tolist()\n","\n","        # Get columns with category dtype\n","        category_cols = df_copy.select_dtypes(include=['category']).columns.tolist()\n","\n","        # Combine and remove target variable\n","        categorical_columns = list(set(object_cols + category_cols))\n","        if target_variable in categorical_columns:\n","            categorical_columns.remove(target_variable)\n","\n","        # You can also explicitly specify columns you want to encode\n","        # Uncomment and modify this list if you want to be more specific:\n","        # categorical_columns = [\n","        #     'FL_DATE_day_name', 'AIRLINE', 'FL_NUMBER', 'ORIGIN', 'ORIGIN_CITY',\n","        #     'DEST', 'DEST_CITY', 'origin_state', 'dest_state'\n","        # ]\n","        # categorical_columns = [col for col in categorical_columns if col in df_copy.columns]\n","\n","        if not categorical_columns:\n","            logger.warning(\"No categorical columns found for WOE encoding.\")\n","            return df_copy\n","\n","        logger.info(f\"Found {len(categorical_columns)} categorical columns for WOE encoding: {categorical_columns}\")\n","\n","        # Apply WOE encoding to all categorical columns\n","        try:\n","            # Handle missing values in target variable\n","            if df_copy[target_variable].isnull().any():\n","                logger.warning(f\"Target variable '{target_variable}' contains NaN values. This may affect WOE encoding.\")\n","                # You might want to handle NaNs in target variable before encoding\n","                # df_copy = df_copy.dropna(subset=[target_variable])  # Option 1: Drop rows with NaN target\n","                # df_copy[target_variable] = df_copy[target_variable].fillna('Unknown')  # Option 2: Fill NaNs\n","\n","            # Handle missing values in categorical columns\n","            # for col in categorical_columns:\n","            #     if df_copy[col].isnull().any():\n","            #         logger.info(f\"Column '{col}' contains NaN values. Filling with 'Missing' before WOE encoding.\")\n","            #         df_copy[col] = df_copy[col].fillna('Missing')\n","\n","            # Initialize WOE encoder\n","            self.woe_encoder = ce.WOEEncoder(cols=categorical_columns, handle_unknown='value', handle_missing='value')\n","\n","            # Apply WOE encoding\n","            df_encoded = self.woe_encoder.fit_transform(df_copy[categorical_columns], df_copy[target_variable])\n","\n","            # Replace original categorical columns with WOE encoded versions\n","            # Add '_WOE' suffix to distinguish encoded columns\n","            woe_columns = {}\n","            for i, col in enumerate(categorical_columns):\n","                woe_col_name = f'{col}_WOE'\n","                df_copy[woe_col_name] = df_encoded.iloc[:, i]\n","                woe_columns[col] = woe_col_name\n","\n","            logger.info(f\"Successfully applied WOE encoding to {len(categorical_columns)} categorical columns.\")\n","            logger.info(f\"Created WOE encoded columns: {list(woe_columns.values())}\")\n","\n","            # Drop original categorical columns\n","            df_copy = df_copy.drop(columns=categorical_columns)\n","            logger.info(f\"Dropped original categorical columns: {categorical_columns}\")\n","\n","        except Exception as e:\n","            logger.error(f\"Error applying WOE encoding: {e}\", exc_info=True)\n","            return df_copy\n","\n","        return df_copy\n","\n","    def identify_high_correlation_pairs(self, df: pd.DataFrame, threshold: float = 0.95) -> Tuple[Set[str], Dict[Tuple[str, str], float]]:\n","        \"\"\"\n","        Identifies pairs of highly correlated features and suggests one to drop.\n","        Returns a set of columns to drop and a dictionary of correlated pairs.\n","        \"\"\"\n","        logger.info(f\"Identifying high correlation features (threshold={threshold})...\")\n","\n","        # Select only numeric columns for correlation calculation\n","        numeric_df = df.select_dtypes(include=np.number)\n","\n","        if numeric_df.empty:\n","            logger.warning(\"No numeric columns found for correlation analysis.\")\n","            return set(), {}\n","\n","        corr_matrix = numeric_df.corr().abs()\n","        upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n","\n","        to_drop = set()\n","        correlated_pairs = {}\n","\n","        for i in range(len(upper_tri.columns)):\n","            for j in range(i + 1, len(upper_tri.columns)):\n","                col1 = upper_tri.columns[i]\n","                col2 = upper_tri.columns[j]\n","                correlation_value = upper_tri.iloc[i, j]\n","\n","                if pd.notna(correlation_value) and correlation_value >= threshold:\n","                    correlated_pairs[(col1, col2)] = correlation_value\n","                    # Simple heuristic: remove the second column in the pair\n","                    to_drop.add(col2)\n","                    logger.debug(f\"High correlation found: {col1} and {col2} (Corr: {correlation_value:.2f}). Suggesting to drop {col2}.\")\n","\n","        logger.info(f\"Found {len(to_drop)} columns to drop due to high correlation.\")\n","        return to_drop, correlated_pairs\n","\n","    def exclude_columns(self, df: pd.DataFrame, columns_to_exclude: List[str]) -> pd.DataFrame:\n","        \"\"\"\n","        Excludes specified columns from the DataFrame.\n","        \"\"\"\n","        logger.info(\"Excluding specified columns...\")\n","        existing_cols_to_drop = [col for col in columns_to_exclude if col in df.columns]\n","        if existing_cols_to_drop:\n","            df.drop(columns=existing_cols_to_drop, inplace=True)\n","            logger.info(f\"Excluded columns: {existing_cols_to_drop}\")\n","        else:\n","            logger.info(\"No specified columns found to exclude.\")\n","        return df\n","\n","    def create_classification_target(self, df: pd.DataFrame, delay_column: str, delay_threshold_minutes: int) -> pd.DataFrame:\n","        \"\"\"\n","        Creates a binary classification target based on arrival delay.\n","        1 if ARR_DELAY > delay_threshold_minutes, 0 otherwise.\n","        \"\"\"\n","        if delay_column not in df.columns:\n","            logger.error(f\"Delay column '{delay_column}' not found for creating classification target.\")\n","            return df\n","\n","        logger.info(f\"Creating classification target: FLIGHT_STATUS_CLASSIFICATION (1 if {delay_column} > {delay_threshold_minutes} mins, else 0).\")\n","        df['FLIGHT_STATUS_CLASSIFICATION'] = (df[delay_column] > delay_threshold_minutes).astype(int)\n","        logger.info(f\"Classification target value counts:\\n{df['FLIGHT_STATUS_CLASSIFICATION'].value_counts().to_string()}\")\n","        return df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bIjHdJKiSn-a","executionInfo":{"status":"ok","timestamp":1752301395339,"user_tz":-120,"elapsed":37,"user":{"displayName":"Jai Kushwaha","userId":"01340704951735287618"}},"outputId":"2220df73-59f1-407b-9ad9-a7ba993c6460"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting scripts/data_preprocessor.py\n"]}]},{"cell_type":"markdown","source":["## data Evaluate"],"metadata":{"id":"eR9vAQwfNHLE"}},{"cell_type":"code","source":["%%writefile scripts/data_evaluate.py\n","import pandas as pd\n","import numpy as np\n","import os\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, confusion_matrix, classification_report\n","import pickle\n","import logging\n","import tensorflow as tf\n","import wandb # NEW: Import wandb\n","\n","logger = logging.getLogger(__name__)\n","logger.setLevel(logging.INFO)\n","if not logger.handlers:\n","    console_handler = logging.StreamHandler()\n","    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n","    console_handler.setFormatter(formatter)\n","    logger.addHandler(console_handler)\n","\n","class ModelEvaluator:\n","    \"\"\"\n","    Handles evaluation and logging of machine learning model performance.\n","    \"\"\"\n","    def __init__(self, output_dir=\"models\", plots_dir=\"plots/model_evaluation\", wandb_run=None): # NEW: Add wandb_run\n","        self.output_dir = output_dir\n","        self.plots_dir = plots_dir\n","        self.wandb_run = wandb_run # NEW: Store wandb_run\n","        os.makedirs(self.plots_dir, exist_ok=True)\n","        os.makedirs(self.output_dir, exist_ok=True)\n","        self.results_df = pd.DataFrame(columns=['Model', 'Stage', 'Metric', 'Value'])\n","        logger.info(f\"ModelEvaluator initialized. Models will be saved to '{self.output_dir}', plots to '{self.plots_dir}'.\")\n","\n","\n","    def save_model(self, model, filename_prefix: str):\n","        \"\"\"\n","        Saves a trained model to the specified output directory.\n","        Handles both scikit-learn models (pickle) and Keras models (HDF5).\n","\n","        Args:\n","            model: The trained model object.\n","            filename_prefix (str): Prefix for the filename (e.g., 'logistic_regression_model').\n","        \"\"\"\n","        if isinstance(model, tf.keras.Model):\n","            file_path = os.path.join(self.output_dir, f\"{filename_prefix}.h5\")\n","            model.save(file_path)\n","            logger.info(f\"Keras model saved to {file_path}\")\n","        else:\n","            file_path = os.path.join(self.output_dir, f\"{filename_prefix}.pkl\")\n","            with open(file_path, 'wb') as f:\n","                pickle.dump(model, f)\n","            logger.info(f\"Scikit-learn model saved to {file_path}\")\n","\n","        # NEW: Log model artifact to WandB\n","        if self.wandb_run:\n","            artifact = wandb.Artifact(name=f\"{filename_prefix}-model\", type=\"model\")\n","            artifact.add_file(file_path)\n","            self.wandb_run.log_artifact(artifact)\n","            logger.info(f\"Model artifact '{filename_prefix}' logged to WandB.\")\n","\n","\n","    def load_model(self, filename_prefix: str):\n","        \"\"\"\n","        Loads a trained model from the specified output directory.\n","        Handles both scikit-learn models (pickle) and Keras models (HDF5).\n","\n","        Args:\n","            filename_prefix (str): Prefix of the filename (e.g., 'logistic_regression_model').\n","\n","        Returns:\n","            The loaded model object, or None if not found/error.\n","        \"\"\"\n","        keras_path = os.path.join(self.output_dir, f\"{filename_prefix}.h5\")\n","        sklearn_path = os.path.join(self.output_dir, f\"{filename_prefix}.pkl\")\n","\n","        if os.path.exists(keras_path):\n","            try:\n","                # Keras model loading requires custom objects if used\n","                # For this pipeline, assuming standard layers\n","                model = tf.keras.models.load_model(keras_path)\n","                logger.info(f\"Keras model loaded from {keras_path}\")\n","                return model\n","            except Exception as e:\n","                logger.error(f\"Error loading Keras model from {keras_path}: {e}\")\n","                return None\n","        elif os.path.exists(sklearn_path):\n","            try:\n","                with open(sklearn_path, 'rb') as f:\n","                    model = pickle.load(f)\n","                logger.info(f\"Scikit-learn model loaded from {sklearn_path}\")\n","                return model\n","            except Exception as e:\n","                logger.error(f\"Error loading scikit-learn model from {sklearn_path}: {e}\")\n","                return None\n","        else:\n","            logger.info(f\"No model found for '{filename_prefix}' at {self.output_dir}\")\n","            return None\n","\n","    def evaluate_classification_model(self, model_name: str, y_true: np.ndarray, y_pred: np.ndarray, y_pred_proba: np.ndarray, stage: str = \"Test\"):\n","        \"\"\"\n","        Evaluates a classification model and logs various metrics.\n","\n","        Args:\n","            model_name (str): Name of the model being evaluated.\n","            y_true (np.ndarray): True labels.\n","            y_pred (np.ndarray): Predicted labels.\n","            y_pred_proba (np.ndarray): Predicted probabilities for the positive class.\n","            stage (str): The stage of evaluation (e.g., \"Train\", \"Validation\", \"Test\").\n","        \"\"\"\n","        logger.info(f\"--- {model_name} Evaluation ({stage} Set) ---\")\n","\n","        accuracy = accuracy_score(y_true, y_pred)\n","        precision = precision_score(y_true, y_pred, zero_division=0)\n","        recall = recall_score(y_true, y_pred, zero_division=0)\n","        f1 = f1_score(y_true, y_pred, zero_division=0)\n","        roc_auc = 0.0\n","        try:\n","            fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n","            roc_auc = auc(fpr, tpr)\n","        except ValueError as e:\n","            logger.warning(f\"Could not calculate ROC AUC: {e}. Ensure y_true contains at least two classes.\")\n","\n","        metrics = {\n","            'Accuracy': accuracy,\n","            'Precision': precision,\n","            'Recall': recall,\n","            'F1 Score': f1,\n","            'ROC AUC': roc_auc\n","        }\n","\n","        for metric_name, value in metrics.items():\n","            logger.info(f\"{metric_name}: {value:.4f}\")\n","            # NEW: Log metrics to WandB\n","            if self.wandb_run:\n","                self.wandb_run.log({f\"{model_name}/{stage}_{metric_name.lower().replace(' ', '_')}\": value})\n","\n","            # Append to internal results_df\n","            self.results_df.loc[len(self.results_df)] = [model_name, stage, metric_name, value]\n","\n","        logger.info(\"\\nClassification Report:\\n\" + classification_report(y_true, y_pred, zero_division=0))\n","\n","    def plot_confusion_matrix(self, model_name: str, y_true: np.ndarray, y_pred: np.ndarray, stage: str = \"Test\"):\n","        \"\"\"\n","        Plots and saves the confusion matrix.\n","\n","        Args:\n","            model_name (str): Name of the model.\n","            y_true (np.ndarray): True labels.\n","            y_pred (np.ndarray): Predicted labels.\n","            stage (str): The stage of evaluation.\n","        \"\"\"\n","        cm = confusion_matrix(y_true, y_pred)\n","        plt.figure(figsize=(8, 6))\n","        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n","                    xticklabels=['Predicted 0', 'Predicted 1'],\n","                    yticklabels=['Actual 0', 'Actual 1'])\n","        plt.title(f'Confusion Matrix for {model_name} ({stage} Set)')\n","        plt.xlabel('Predicted Label')\n","        plt.ylabel('True Label')\n","        plt.tight_layout()\n","        plot_path = os.path.join(self.plots_dir, f\"{model_name.lower().replace(' ', '_')}_confusion_matrix_{stage.lower()}.png\")\n","        plt.savefig(plot_path)\n","        plt.close()\n","        logger.info(f\"Saved confusion matrix plot to {plot_path}\")\n","\n","        # NEW: Log plot to WandB\n","        if self.wandb_run:\n","            self.wandb_run.log({f\"{model_name}/{stage}_Confusion_Matrix\": wandb.Image(plot_path)})\n","            logger.info(f\"Confusion Matrix plot logged to WandB for {model_name}.\")\n","\n","\n","    def plot_roc_curve(self, model_name: str, y_true: np.ndarray, y_pred_proba: np.ndarray, stage: str = \"Test\"):\n","        \"\"\"\n","        Plots and saves the ROC curve.\n","\n","        Args:\n","            model_name (str): Name of the model.\n","            y_true (np.ndarray): True labels.\n","            y_pred_proba (np.ndarray): Predicted probabilities for the positive class.\n","            stage (str): The stage of evaluation.\n","        \"\"\"\n","        try:\n","            fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n","            roc_auc = auc(fpr, tpr)\n","\n","            plt.figure(figsize=(8, 6))\n","            plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n","            plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n","            plt.xlim([0.0, 1.0])\n","            plt.ylim([0.0, 1.05])\n","            plt.xlabel('False Positive Rate')\n","            plt.ylabel('True Positive Rate')\n","            plt.title(f'Receiver Operating Characteristic (ROC) Curve for {model_name} ({stage} Set)')\n","            plt.legend(loc=\"lower right\")\n","            plt.tight_layout()\n","            plot_path = os.path.join(self.plots_dir, f\"{model_name.lower().replace(' ', '_')}_roc_curve_{stage.lower()}.png\")\n","            plt.savefig(plot_path)\n","            plt.close()\n","            logger.info(f\"Saved ROC curve plot to {plot_path}\")\n","\n","            # NEW: Log plot to WandB\n","            if self.wandb_run:\n","                self.wandb_run.log({f\"{model_name}/{stage}_ROC_Curve\": wandb.Image(plot_path)})\n","                logger.info(f\"ROC Curve plot logged to WandB for {model_name}.\")\n","\n","        except ValueError as e:\n","            logger.warning(f\"Could not plot ROC curve for {model_name}: {e}. Ensure y_true contains at least two classes.\")\n","\n","    def display_results_table(self):\n","        \"\"\"Displays the collected evaluation results in a formatted table.\"\"\"\n","        if not self.results_df.empty:\n","            logger.info(\"\\n--- Overall Model Evaluation Results ---\")\n","            # Pivot table for better readability\n","            pivot_df = self.results_df.pivot_table(\n","                index=['Model', 'Stage'],\n","                columns='Metric',\n","                values='Value',\n","                aggfunc='first'\n","            ).round(4)\n","            logger.info(f\"\\n{pivot_df.to_string()}\")\n","        else:\n","            logger.info(\"\\nNo evaluation results to display.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cVDFOBx8TLdM","executionInfo":{"status":"ok","timestamp":1752595559005,"user_tz":-120,"elapsed":283,"user":{"displayName":"Jai Kushwaha","userId":"01340704951735287618"}},"outputId":"9faa23bb-8ad6-4301-dab5-c199ab361e67"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting scripts/data_evaluate.py\n"]}]},{"cell_type":"markdown","source":["## nn_utils"],"metadata":{"id":"EBM0nVAc0oFa"}},{"cell_type":"code","source":["%%writefile scripts/nn_utils.py\n","\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.losses import MeanSquaredError, BinaryCrossentropy\n","from tensorflow.keras.metrics import MeanAbsoluteError, BinaryAccuracy, AUC # Ensure AUC is imported for classification metrics\n","\n","def build_nn_model(input_dim, hidden_layers=1, neurons=64, dropout_rate=0.0, learning_rate=0.001, task_type='regression'):\n","    \"\"\"\n","    Builds a Keras Sequential model for regression or classification.\n","    \"\"\"\n","    model = Sequential()\n","    model.add(Dense(neurons, activation='relu', input_shape=(input_dim,)))\n","    model.add(Dropout(dropout_rate))\n","\n","    for _ in range(hidden_layers - 1): # Add additional hidden layers\n","        model.add(Dense(neurons, activation='relu'))\n","        model.add(Dropout(dropout_rate))\n","\n","    if task_type == 'regression':\n","        model.add(Dense(1, activation='linear')) # Single output for regression\n","        model.compile(optimizer=Adam(learning_rate=learning_rate),\n","                      loss=MeanSquaredError(),\n","                      metrics=[MeanAbsoluteError()])\n","    elif task_type == 'classification':\n","        model.add(Dense(1, activation='sigmoid')) # Single output for binary classification\n","        model.compile(optimizer=Adam(learning_rate=learning_rate),\n","                      loss=BinaryCrossentropy(),\n","                      metrics=[BinaryAccuracy(), AUC()]) # Add AUC for classification\n","    else:\n","        raise ValueError(\"task_type must be 'regression' or 'classification'\")\n","\n","    return model"],"metadata":{"id":"_NI6wuq80n77"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model Tuner"],"metadata":{"id":"IUA5HhghxBUF"}},{"cell_type":"code","source":["%%writefile scripts/model_tuning.py\n","\n","import logging\n","import os\n","import json # To save best parameters\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import KFold, GridSearchCV, RandomizedSearchCV\n","from sklearn.metrics import make_scorer, mean_absolute_error, f1_score # Import f1_score for classification\n","\n","# For Neural Network Tuning\n","import tensorflow as tf\n","from tensorflow import keras\n","import keras_tuner # Make sure this is imported for keras_tuner.HyperModel\n","from keras_tuner.tuners import RandomSearch, Hyperband\n","\n","# Import build_nn_model from nn_utils (assuming it's been refactored there)\n","from scripts.nn_utils import build_nn_model\n","\n","# Configure a logger for this module\n","logger = logging.getLogger(__name__)\n","logger.setLevel(logging.INFO)\n","if not logger.handlers:\n","    console_handler = logging.StreamHandler()\n","    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n","    console_handler.setFormatter(formatter)\n","    logger.addHandler(console_handler)\n","\n","class ModelTuner:\n","    \"\"\"\n","    Handles hyperparameter tuning and cross-validation for machine learning models.\n","    Saves and loads best parameters to/from a JSON file.\n","    \"\"\"\n","    def __init__(self, output_dir=\"model_params\"):\n","        self.output_dir = output_dir\n","        os.makedirs(self.output_dir, exist_ok=True)\n","        self.params_file = os.path.join(self.output_dir, \"all_best_params.json\")\n","        self.best_params_cache = self._load_all_best_params()\n","        logger.info(f\"ModelTuner initialized. Best parameters will be saved to '{self.output_dir}'.\")\n","\n","    def _load_all_best_params(self):\n","        \"\"\"Loads all best parameters from the single JSON file.\"\"\"\n","        if os.path.exists(self.params_file):\n","            try:\n","                with open(self.params_file, 'r') as f:\n","                    return json.load(f)\n","            except json.JSONDecodeError as e:\n","                logger.error(f\"Error decoding {self.params_file}: {e}. Returning empty cache.\", exc_info=True)\n","                return {}\n","        return {}\n","\n","    def _save_all_best_params(self):\n","        \"\"\"Saves current best parameters cache to the single JSON file.\"\"\"\n","        try:\n","            with open(self.params_file, 'w') as f:\n","                json.dump(self.best_params_cache, f, indent=4)\n","            logger.info(f\"All best parameters saved to {self.params_file}\")\n","        except Exception as e:\n","            logger.error(f\"Error saving all best parameters to {self.params_file}: {e}\", exc_info=True)\n","\n","    def get_best_params(self, model_name: str) -> dict:\n","        \"\"\"Retrieves best parameters for a specific model from the cache.\"\"\"\n","        return self.best_params_cache.get(model_name, None)\n","\n","    def tune_model(self, model, param_grid: dict, X: pd.DataFrame, y: pd.Series,\n","                   model_name: str, cv_splits: int = 3, search_method: str = 'grid', # Reduced cv_splits\n","                   n_iter: int = 3, scoring: str = 'neg_mean_absolute_error') -> tuple: # Reduced n_iter\n","        \"\"\"\n","        Performs hyperparameter tuning and cross-validation for a given scikit-learn model.\n","        (Parameters eased out for faster convergence during HPO).\n","\n","        Args:\n","            model: The scikit-learn estimator to tune.\n","            param_grid (dict): Dictionary with parameters names (str) as keys and lists of parameter\n","                                settings to try as values.\n","            X (pd.DataFrame): Training features.\n","            y (pd.Series): Training target.\n","            model_name (str): A descriptive name for the model (e.g., 'ridge_regression').\n","            cv_splits (int): Number of cross-validation splits. (Reduced for faster HPO)\n","            search_method (str): 'grid' for GridSearchCV or 'random' for RandomizedSearchCV.\n","            n_iter (int): Number of parameter settings that are sampled if search_method is 'random'. (Reduced for faster HPO)\n","            scoring (str): Scoring metric to optimize (e.g., 'neg_mean_absolute_error').\n","\n","        Returns:\n","            tuple: A tuple containing (best_estimator, best_params, best_score).\n","        \"\"\"\n","        logger.info(f\"Starting hyperparameter tuning for {model_name} using {search_method} search...\")\n","\n","        best_params_loaded = self.get_best_params(model_name)\n","        if best_params_loaded:\n","            logger.info(f\"Using loaded best parameters for {model_name}: {best_params_loaded}. Skipping tuning.\")\n","            model.set_params(**best_params_loaded)\n","            return model, best_params_loaded, None\n","\n","        cv_strategy = KFold(n_splits=cv_splits, shuffle=True, random_state=42)\n","\n","        if search_method == 'grid':\n","            search = GridSearchCV(model, param_grid, cv=cv_strategy, scoring=scoring, verbose=0, n_jobs=-1) # Reduced verbose\n","        elif search_method == 'random':\n","            search = RandomizedSearchCV(model, param_distro=param_grid, n_iter=n_iter, cv=cv_strategy, scoring=scoring, verbose=0, n_jobs=-1, random_state=42) # Reduced verbose\n","        else:\n","            raise ValueError(f\"Unknown search_method: {search_method}. Must be 'grid' or 'random'.\")\n","\n","        search.fit(X, y)\n","\n","        best_estimator = search.best_estimator_\n","        best_params = search.best_params_\n","        best_score = search.best_score_\n","\n","        logger.info(f\"Best parameters for {model_name}: {best_params}\")\n","        logger.info(f\"Best CV score for {model_name} ({scoring}): {best_score:.4f}\")\n","\n","        self.best_params_cache[model_name] = best_params\n","        self._save_all_best_params()\n","\n","        return best_estimator, best_params, best_score\n","\n","    class NeuralNetworkHyperModel(keras_tuner.HyperModel):\n","        def __init__(self, input_dim: int, task_type: str = 'regression'):\n","            self.input_dim = input_dim\n","            self.task_type = task_type\n","            super().__init__()\n","\n","        def build(self, hp):\n","            \"\"\"\n","            Builds a Keras model based on hyperparameters chosen by the tuner.\n","            (Parameters eased out for faster convergence during HPO).\n","            \"\"\"\n","            # No need to import build_nn_model here if it's imported at the module level.\n","            # However, if it's explicitly needed here due to circular import issues,\n","            # ensure it's from the correct place (nn_utils).\n","            # If `from scripts.nn_utils import build_nn_model` is at the top of this file, remove this local import.\n","            # If build_nn_model is still in data_modelling.py and there's a circular import, keep this.\n","            # Given the previous error, it was likely due to a circular import that\n","            # has now been resolved by suggesting to move build_nn_model to nn_utils.\n","            # So, assuming build_nn_model is now correctly imported from nn_utils at the top.\n","            # If not, add: from scripts.nn_utils import build_nn_model here\n","\n","            # Eased out search space for faster tuning\n","            num_hidden_layers = hp.Int('num_hidden_layers', min_value=1, max_value=2, default=1) # Reduced max layers\n","            neurons = hp.Int('neurons', min_value=32, max_value=64, step=32, default=32) # Reduced range\n","            dropout_rate = hp.Float('dropout_rate', min_value=0.0, max_value=0.3, step=0.1, default=0.1) # Reduced max dropout\n","            learning_rate = hp.Choice('learning_rate', values=[1e-3, 1e-4], default=1e-3) # Fewer choices\n","\n","            model = build_nn_model(\n","                input_dim=self.input_dim,\n","                hidden_layers=num_hidden_layers,\n","                neurons=neurons,\n","                dropout_rate=dropout_rate,\n","                learning_rate=learning_rate,\n","                task_type=self.task_type\n","            )\n","            return model\n","\n","    def tune_neural_network_classification(self, X_train: pd.DataFrame, y_train: pd.Series,\n","                                           X_val: pd.DataFrame, y_val: pd.Series) -> dict:\n","        \"\"\"\n","        Performs hyperparameter tuning for the Neural Network Classification model\n","        using Keras Tuner. (Parameters eased out for faster convergence during HPO).\n","\n","        Args:\n","            X_train (pd.DataFrame): Training features.\n","            y_train (pd.Series): Training target.\n","            X_val (pd.DataFrame): Validation features.\n","            y_val (pd.Series): Validation target.\n","\n","        Returns:\n","            dict: The best hyperparameters found.\n","        \"\"\"\n","        logger.info(\"\\n--- Tuning Neural Network Classification Model ---\")\n","\n","        model_name = \"neural_network_classification\"\n","        best_params_loaded = self.get_best_params(model_name)\n","\n","        if best_params_loaded:\n","            logger.info(f\"Using loaded best parameters for Neural Network Classification: {best_params_loaded}. Skipping tuning.\")\n","            return best_params_loaded\n","\n","        tuner_dir = os.path.join(self.output_dir, \"keras_tuner_nn_classification\")\n","        os.makedirs(tuner_dir, exist_ok=True)\n","\n","        hypermodel = self.NeuralNetworkHyperModel(input_dim=X_train.shape[1], task_type='classification')\n","\n","        tuner = RandomSearch(\n","            hypermodel,\n","            objective='val_auc',\n","            max_trials=5,       # Reduced number of different models to try\n","            executions_per_trial=1,\n","            directory=tuner_dir,\n","            project_name='nn_classification_tuning_eased', # Changed project name to avoid conflicts if old runs exist\n","            overwrite=True\n","        )\n","\n","        logger.info(\"Running Keras Tuner search...\")\n","        early_stopping = tf.keras.callbacks.EarlyStopping(\n","            monitor='val_loss', patience=5, restore_best_weights=True # Reduced patience\n","        )\n","\n","        tuner.search(\n","            X_train, y_train,\n","            epochs=20, # Reduced max epochs for each trial\n","            validation_data=(X_val, y_val),\n","            callbacks=[early_stopping],\n","            verbose=0\n","        )\n","\n","        best_hp = tuner.get_best_hyperparameters(num_trials=1)[0].values\n","\n","        mapped_params = {\n","            'hidden_layers': best_hp['num_hidden_layers'],\n","            'neurons': best_hp['neurons'],\n","            'dropout_rate': best_hp['dropout_rate'],\n","            'learning_rate': best_hp['learning_rate']\n","        }\n","\n","        self.best_params_cache[model_name] = mapped_params\n","        self._save_all_best_params()\n","\n","        logger.info(f\"Best hyperparameters for Neural Network Classification: {mapped_params}\")\n","\n","        return mapped_params"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eeftLMC0xBFH","executionInfo":{"status":"ok","timestamp":1752407823537,"user_tz":-120,"elapsed":262,"user":{"displayName":"Jai Kushwaha","userId":"01340704951735287618"}},"outputId":"ea08daf4-430d-46ce-a07c-d5dc689afee3"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting scripts/model_tuning.py\n"]}]},{"cell_type":"markdown","source":["## model_explainability"],"metadata":{"id":"NaEbxWRW2AX9"}},{"cell_type":"code","source":["%%writefile scripts/model_explainability.py\n","import pandas as pd\n","import numpy as np\n","import logging\n","import os\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import shap\n","from sklearn.linear_model import LogisticRegression, Ridge\n","from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n","import tensorflow as tf\n","import wandb # NEW: Import wandb\n","\n","logger = logging.getLogger(__name__)\n","logger.setLevel(logging.INFO)\n","if not logger.handlers:\n","    console_handler = logging.StreamHandler()\n","    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n","    console_handler.setFormatter(formatter)\n","    logger.addHandler(console_handler)\n","\n","class ModelExplainer:\n","    \"\"\"\n","    Handles model explainability for trained machine learning models.\n","    \"\"\"\n","    def __init__(self, output_dir=\"plots/explainability\", wandb_run=None): # NEW: Add wandb_run\n","        self.output_dir = output_dir\n","        self.wandb_run = wandb_run # NEW: Store wandb_run\n","        os.makedirs(self.output_dir, exist_ok=True)\n","        logger.info(f\"ModelExplainer initialized. Explainability plots will be saved to '{self.output_dir}'.\")\n","\n","    def plot_feature_importance(self, model, feature_names: list, filename: str):\n","        \"\"\"\n","        Plots feature importance for tree-based models (RandomForest).\n","\n","        Args:\n","            model: Trained scikit-learn model (must have .feature_importances_ attribute).\n","            feature_names (list): List of feature names corresponding to model's input.\n","            filename (str): Name of the file to save the plot.\n","        \"\"\"\n","        if not hasattr(model, 'feature_importances_'):\n","            logger.warning(f\"Model of type {type(model).__name__} does not have 'feature_importances_'. Skipping feature importance plot.\")\n","            return\n","\n","        try:\n","            importances = model.feature_importances_\n","            feature_importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\n","            feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)\n","\n","            plt.figure(figsize=(12, 8))\n","            sns.barplot(x='importance', y='feature', data=feature_importance_df.head(20)) # Top 20 features\n","            plt.title(f'Top 20 Feature Importance for {type(model).__name__}')\n","            plt.xlabel('Importance')\n","            plt.ylabel('Feature')\n","            plt.tight_layout()\n","            plot_path = os.path.join(self.output_dir, filename)\n","            plt.savefig(plot_path)\n","            plt.close()\n","            logger.info(f\"Saved feature importance plot to {plot_path}\")\n","\n","            # NEW: Log plot to WandB\n","            if self.wandb_run:\n","                self.wandb_run.log({f\"Explainability/{filename.replace('.png', '')}\": wandb.Image(plot_path)})\n","                logger.info(f\"Feature importance plot logged to WandB for {type(model).__name__}.\")\n","\n","        except Exception as e:\n","            logger.error(f\"Error generating feature importance plot: {e}\")\n","\n","    def plot_coefficient_importance(self, model, feature_names: list, filename: str):\n","        \"\"\"\n","        Plots coefficients for linear models (LogisticRegression, Ridge).\n","\n","        Args:\n","            model: Trained scikit-learn linear model (must have .coef_ attribute).\n","            feature_names (list): List of feature names.\n","            filename (str): Name of the file to save the plot.\n","        \"\"\"\n","        if not hasattr(model, 'coef_'):\n","            logger.warning(f\"Model of type {type(model).__name__} does not have 'coef_'. Skipping coefficient importance plot.\")\n","            return\n","\n","        try:\n","            coefficients = model.coef_.flatten() if model.coef_.ndim > 1 else model.coef_\n","\n","            coef_df = pd.DataFrame({'feature': feature_names, 'coefficient': coefficients})\n","            coef_df['abs_coefficient'] = np.abs(coef_df['coefficient'])\n","            coef_df = coef_df.sort_values(by='abs_coefficient', ascending=False)\n","\n","            plt.figure(figsize=(12, 8))\n","            sns.barplot(x='coefficient', y='feature', data=coef_df.head(20)) # Top 20 features by absolute coefficient\n","            plt.title(f'Top 20 Feature Coefficients for {type(model).__name__}')\n","            plt.xlabel('Coefficient Value')\n","            plt.ylabel('Feature')\n","            plt.tight_layout()\n","            plot_path = os.path.join(self.output_dir, filename)\n","            plt.savefig(plot_path)\n","            plt.close()\n","            logger.info(f\"Saved coefficient importance plot to {plot_path}\")\n","\n","            # NEW: Log plot to WandB\n","            if self.wandb_run:\n","                self.wandb_run.log({f\"Explainability/{filename.replace('.png', '')}\": wandb.Image(plot_path)})\n","                logger.info(f\"Coefficient importance plot logged to WandB for {type(model).__name__}.\")\n","\n","        except Exception as e:\n","            logger.error(f\"Error generating coefficient importance plot: {e}\")\n","\n","\n","    def plot_shap_summary(self, model, X: pd.DataFrame, filename: str, task_type: str = 'regression', num_features: int = 20):\n","        \"\"\"\n","        Generates and saves a SHAP summary plot.\n","\n","        Args:\n","            model: The trained model (scikit-learn or Keras).\n","            X (pd.DataFrame): The data used to generate SHAP values (e.g., X_test or a sample).\n","            filename (str): Name of the file to save the plot.\n","            task_type (str): 'regression' or 'classification'.\n","            num_features (int): Number of features to display in the summary plot.\n","        \"\"\"\n","        try:\n","            if isinstance(model, (RandomForestRegressor, RandomForestClassifier)):\n","                explainer = shap.TreeExplainer(model)\n","            elif isinstance(model, (LogisticRegression, Ridge)):\n","                explainer = shap.LinearExplainer(model, X)\n","            elif isinstance(model, tf.keras.Model):\n","                explainer = shap.DeepExplainer(model, X.values)\n","            else:\n","                logger.warning(f\"SHAP Explainer for model type {type(model).__name__} not explicitly supported/optimized. Using KernelExplainer (can be slow).\")\n","                predict_fn = model.predict_proba if task_type == 'classification' and hasattr(model, 'predict_proba') else model.predict\n","                explainer = shap.KernelExplainer(predict_fn, shap.sample(X, 100, random_state=42))\n","\n","\n","            if isinstance(explainer, shap.DeepExplainer) and task_type == 'classification':\n","                shap_values = explainer.shap_values(X.values)\n","                if isinstance(shap_values, list) and len(shap_values) > 1:\n","                    shap_values = shap_values[1]\n","            elif isinstance(explainer, shap.LinearExplainer) and task_type == 'classification':\n","                shap_values = explainer.shap_values(X.values)\n","                if isinstance(shap_values, list) and len(shap_values) > 1:\n","                    shap_values = shap_values[1]\n","            elif isinstance(explainer, shap.KernelExplainer) and task_type == 'classification':\n","                shap_values = explainer.shap_values(X)\n","                if isinstance(shap_values, list) and len(shap_values) > 1:\n","                    shap_values = shap_values[1]\n","            else:\n","                shap_values = explainer.shap_values(X.values if isinstance(explainer, (shap.DeepExplainer, shap.LinearExplainer)) else X)\n","\n","            plt.figure(figsize=(10, 7))\n","            shap.summary_plot(shap_values, X, plot_type=\"bar\", show=False, max_display=num_features)\n","            plt.title(f'SHAP Summary Bar Plot for {type(model).__name__} ({task_type.capitalize()})')\n","            plt.tight_layout()\n","            plot_path = os.path.join(self.output_dir, filename)\n","            plt.savefig(plot_path)\n","            plt.close()\n","            logger.info(f\"Saved SHAP summary bar plot to {plot_path}\")\n","\n","            # NEW: Log plot to WandB\n","            if self.wandb_run:\n","                self.wandb_run.log({f\"Explainability/{filename.replace('.png', '')}_bar\": wandb.Image(plot_path)})\n","                logger.info(f\"SHAP summary bar plot logged to WandB for {type(model).__name__}.\")\n","\n","\n","            plt.figure(figsize=(10, 7))\n","            shap.summary_plot(shap_values, X, show=False, max_display=num_features)\n","            plt.title(f'SHAP Summary Dot Plot for {type(model).__name__} ({task_type.capitalize()})')\n","            plt.tight_layout()\n","            dot_filename = filename.replace(\".png\", \"_dot.png\")\n","            dot_plot_path = os.path.join(self.output_dir, dot_filename)\n","            plt.savefig(dot_plot_path)\n","            plt.close()\n","            logger.info(f\"Saved SHAP summary dot plot to {dot_plot_path}\")\n","\n","            # NEW: Log plot to WandB\n","            if self.wandb_run:\n","                self.wandb_run.log({f\"Explainability/{dot_filename.replace('.png', '')}_dot\": wandb.Image(dot_plot_path)})\n","                logger.info(f\"SHAP summary dot plot logged to WandB for {type(model).__name__}.\")\n","\n","        except Exception as e:\n","            logger.error(f\"Error generating SHAP summary plot for {type(model).__name__}: {e}\", exc_info=True)\n","\n","\n","    def plot_shap_dependence(self, model, X: pd.DataFrame, feature: str, filename: str, task_type: str = 'regression', interaction_feature: str = None):\n","        \"\"\"\n","        Generates and saves a SHAP dependence plot for a specific feature.\n","\n","        Args:\n","            model: The trained model (scikit-learn or Keras).\n","            X (pd.DataFrame): The data used to generate SHAP values (e.g., X_test or a sample).\n","            feature (str): The name of the feature for which to plot dependence.\n","            filename (str): Name of the file to save the plot.\n","            task_type (str): 'regression' or 'classification'.\n","            interaction_feature (str, optional): A feature to color the dependence plot by, revealing interactions.\n","        \"\"\"\n","        if feature not in X.columns:\n","            logger.warning(f\"Feature '{feature}' not found in X. Skipping SHAP dependence plot.\")\n","            return\n","        if interaction_feature and interaction_feature not in X.columns:\n","            logger.warning(f\"Interaction feature '{interaction_feature}' not found in X. Skipping SHAP dependence plot with interaction.\")\n","            interaction_feature = None\n","\n","        try:\n","            if isinstance(model, (RandomForestRegressor, RandomForestClassifier)):\n","                explainer = shap.TreeExplainer(model)\n","            elif isinstance(model, (LogisticRegression, Ridge)):\n","                explainer = shap.LinearExplainer(model, X)\n","            elif isinstance(model, tf.keras.Model):\n","                explainer = shap.DeepExplainer(model, X.values)\n","            else:\n","                predict_fn = model.predict_proba if task_type == 'classification' and hasattr(model, 'predict_proba') else model.predict\n","                explainer = shap.KernelExplainer(predict_fn, shap.sample(X, 100, random_state=42))\n","\n","            if isinstance(explainer, shap.DeepExplainer) and task_type == 'classification':\n","                shap_values = explainer.shap_values(X.values)\n","                if isinstance(shap_values, list) and len(shap_values) > 1:\n","                    shap_values = shap_values[1]\n","            elif isinstance(explainer, shap.LinearExplainer) and task_type == 'classification':\n","                shap_values = explainer.shap_values(X.values)\n","                if isinstance(shap_values, list) and len(shap_values) > 1:\n","                    shap_values = shap_values[1]\n","            elif isinstance(explainer, shap.KernelExplainer) and task_type == 'classification':\n","                shap_values = explainer.shap_values(X)\n","                if isinstance(shap_values, list) and len(shap_values) > 1:\n","                    shap_values = shap_values[1]\n","            else:\n","                shap_values = explainer.shap_values(X.values if isinstance(explainer, (shap.DeepExplainer, shap.LinearExplainer)) else X)\n","\n","            if shap_values.ndim == 1:\n","                shap_values = shap_values.reshape(-1, 1)\n","\n","\n","            plt.figure(figsize=(10, 7))\n","            shap.dependence_plot(\n","                ind=feature,\n","                shap_values=shap_values,\n","                features=X,\n","                feature_names=X.columns.tolist(),\n","                interaction_index=interaction_feature,\n","                show=False,\n","                title=f'SHAP Dependence Plot for {feature} ({type(model).__name__})'\n","            )\n","            plt.tight_layout()\n","            plot_path = os.path.join(self.output_dir, filename)\n","            plt.savefig(plot_path)\n","            plt.close()\n","            logger.info(f\"Saved SHAP dependence plot for '{feature}' to {plot_path}\")\n","\n","            # NEW: Log plot to WandB\n","            if self.wandb_run:\n","                self.wandb_run.log({f\"Explainability/{filename.replace('.png', '')}\": wandb.Image(plot_path)})\n","                logger.info(f\"SHAP dependence plot logged to WandB for '{feature}' ({type(model).__name__}).\")\n","\n","        except Exception as e:\n","            logger.error(f\"Error generating SHAP dependence plot for feature '{feature}': {e}\", exc_info=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AdQVv34bTnCu","executionInfo":{"status":"ok","timestamp":1752595566046,"user_tz":-120,"elapsed":341,"user":{"displayName":"Jai Kushwaha","userId":"01340704951735287618"}},"outputId":"d898edcb-3b80-4716-fb68-bef9f3585756"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting scripts/model_explainability.py\n"]}]},{"cell_type":"markdown","source":["## Main.py"],"metadata":{"id":"qtcqfJUIHM1z"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import os\n","import logging\n","import sys\n","import importlib\n","import wandb\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","import tensorflow as tf\n","from sklearn.linear_model import LogisticRegression # Ensure this is imported for isinstance checks\n","\n","# Reload custom modules to ensure the latest changes are picked up\n","# This is crucial in environments like Jupyter/Colab where modules might be cached\n","import scripts.data_loading\n","importlib.reload(scripts.data_loading)\n","import scripts.data_profiling\n","importlib.reload(scripts.data_profiling)\n","import scripts.data_visualizer\n","importlib.reload(scripts.data_visualizer)\n","import scripts.data_preprocessor\n","importlib.reload(scripts.data_preprocessor)\n","import scripts.data_modelling\n","importlib.reload(scripts.data_modelling)\n","import scripts.data_evaluate\n","importlib.reload(scripts.data_evaluate)\n","import scripts.model_tuning\n","importlib.reload(scripts.model_tuning)\n","import scripts.model_explainability\n","importlib.reload(scripts.model_explainability)\n","\n","\n","# Custom Scripts - ensure these are available in your 'scripts' directory\n","from scripts.data_loading import DataDownload\n","from scripts.data_profiling import DataProfiler\n","from scripts.data_visualizer import DataVisualizer\n","from scripts.data_preprocessor import DataPreprocessor\n","from scripts.data_modelling import DataModeling\n","from scripts.data_evaluate import ModelEvaluator\n","from scripts.model_tuning import ModelTuner\n","from scripts.model_explainability import ModelExplainer\n","\n","\n","# --- Configure Global Logging ---\n","logger = logging.getLogger(__name__)\n","logger.setLevel(logging.INFO)\n","\n","# Clear existing handlers to prevent duplicate logs if script is run multiple times\n","if logger.handlers:\n","    for handler in logger.handlers[:]:\n","        logger.removeHandler(handler)\n","\n","file_handler = logging.FileHandler('pipeline.log')\n","file_handler.setLevel(logging.INFO)\n","\n","console_handler = logging.StreamHandler(sys.stdout) # Use sys.stdout for console output\n","console_handler.setLevel(logging.INFO)\n","\n","file_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n","file_handler.setFormatter(file_formatter)\n","\n","console_formatter = logging.Formatter('%(levelname)s: %(message)s')\n","console_handler.setFormatter(console_formatter)\n","\n","logger.addHandler(file_handler)\n","logger.addHandler(console_handler)\n","\n","\n","class FlightDelayPipeline:\n","    \"\"\"\n","    Orchestrates the entire machine learning pipeline for flight delay prediction,\n","    from data download to model explainability, with WandB integration.\n","    \"\"\"\n","    def __init__(self, config: dict):\n","        self.config = config\n","        self.wandb_run = None # Will be initialized in _init_wandb\n","\n","        self._setup_directories()\n","\n","        # Initialize core components\n","        self.downloader = DataDownload(\n","            dataset_name=self.config[\"dataset_name\"],\n","            download_path=self.raw_data_dir\n","        )\n","        self.preprocessor = DataPreprocessor()\n","        self.profiler = DataProfiler(output_dir=self.reports_dir)\n","        self.visualizer = DataVisualizer(output_dir=self.eda_plots_dir)\n","        self.tuner = ModelTuner(output_dir=self.model_params_dir)\n","        self.modeler = DataModeling(tuner=self.tuner)\n","\n","        # Evaluator and Explainer depend on wandb_run, so initialize them later\n","        self.evaluator = None\n","        self.explainer = None\n","\n","        logger.info(\"FlightDelayPipeline initialized with provided configuration.\")\n","\n","    def _setup_directories(self):\n","        \"\"\"Defines and creates all necessary output directories.\"\"\"\n","        self.raw_data_dir = \"data/raw\"\n","        self.processed_data_dir = \"data/processed\"\n","        self.reports_dir = \"reports\"\n","        self.eda_plots_dir = \"plots/eda\"\n","        self.model_eval_plots_dir = \"plots/model_evaluation\"\n","        self.models_dir = \"models\"\n","        self.model_params_dir = \"model_params\"\n","        self.explainability_plots_dir = \"plots/explainability\"\n","\n","        for directory in [\n","            self.raw_data_dir, self.processed_data_dir, self.reports_dir,\n","            self.eda_plots_dir, self.model_eval_plots_dir, self.models_dir,\n","            self.model_params_dir, self.explainability_plots_dir\n","        ]:\n","            os.makedirs(directory, exist_ok=True)\n","        logger.info(\"All pipeline directories ensured to exist.\")\n","\n","    def _handle_colab_environment(self):\n","        \"\"\"Adjusts the working directory and installs Kaggle if running in Colab.\"\"\"\n","        if 'google.colab' in sys.modules:\n","            base_dir = '/content/drive/MyDrive/Colab Notebooks/flight_delay'\n","            os.makedirs(base_dir, exist_ok=True)\n","            os.chdir(base_dir)\n","            logger.info(f\"Running in Google Colab. Changed current working directory to {os.getcwd()}\")\n","            try:\n","                import kaggle\n","            except ImportError:\n","                logger.info(\"Kaggle library not found, installing...\")\n","                os.system(\"pip install kaggle\")\n","                logger.info(\"Kaggle library installed.\")\n","        else:\n","            logger.info(\"Not running in Google Colab environment.\")\n","\n","\n","    def _init_wandb(self):\n","        \"\"\"Initializes Weights & Biases run and sets up dependent components.\"\"\"\n","        try:\n","            self.wandb_run = wandb.init(\n","                project=self.config[\"project_name\"],\n","                name=self.config[\"run_name\"],\n","                config=self.config # Log pipeline configuration\n","            )\n","            logger.info(f\"WandB run initialized: {self.wandb_run.url}\")\n","            # Initialize evaluator and explainer now that wandb_run is available\n","            self.evaluator = ModelEvaluator(\n","                output_dir=self.models_dir,\n","                plots_dir=self.model_eval_plots_dir,\n","                wandb_run=self.wandb_run\n","            )\n","            self.explainer = ModelExplainer(\n","                output_dir=self.explainability_plots_dir,\n","                wandb_run=self.wandb_run\n","            )\n","            logger.info(\"ModelEvaluator and ModelExplainer initialized with WandB integration.\")\n","        except Exception as e:\n","            logger.error(f\"Failed to initialize WandB: {e}. Proceeding without WandB logging.\", exc_info=True)\n","            self.wandb_run = None # Ensure it's None if init fails\n","            # Still initialize evaluators/explainers, but without wandb_run\n","            self.evaluator = ModelEvaluator(\n","                output_dir=self.models_dir,\n","                plots_dir=self.model_eval_plots_dir,\n","                wandb_run=None\n","            )\n","            self.explainer = ModelExplainer(\n","                output_dir=self.explainability_plots_dir,\n","                wandb_run=None\n","            )\n","            logger.warning(\"ModelEvaluator and ModelExplainer initialized without WandB integration due to error.\")\n","\n","\n","    def run(self):\n","        \"\"\"Executes the entire machine learning pipeline.\"\"\"\n","        self._handle_colab_environment()\n","        self._init_wandb() # Initialize WandB at the start of the run\n","        logger.info(\"--- Starting ML Project Pipeline ---\")\n","\n","        try:\n","            df_raw = self._step_data_download()\n","            df_filtered = self._step_data_load_and_filter(df_raw)\n","            self._step_data_profiling(df_filtered)\n","            df_eda = self._step_data_preprocessing_eda(df_filtered)\n","            self._step_data_visualization_eda(df_eda)\n","            df_model = self._step_data_preprocessing_modeling(df_filtered)\n","            self._step_data_modeling_and_evaluation(df_model)\n","\n","        except Exception as e:\n","            logger.critical(f\"Pipeline run failed: {e}\", exc_info=True)\n","            if self.wandb_run:\n","                self.wandb_run.log_code(\".\") # Log code on pipeline failure\n","                self.wandb_run.finish(exit_code=1)\n","            sys.exit(1) # Exit with an error code\n","\n","        logger.info(\"\\n--- ML Project Pipeline Complete ---\")\n","        if self.wandb_run:\n","            self.wandb_run.log_code(\".\") # Log code on successful completion\n","            self.wandb_run.finish() # End the WandB run\n","\n","    def _step_data_download(self) -> pd.DataFrame:\n","        \"\"\"Step 1: Downloads data and loads the raw CSV.\"\"\"\n","        logger.info(\"\\n--- Step 1: Data Download ---\")\n","        try:\n","            self.downloader.data_download()\n","            csv_file_path = os.path.join(self.raw_data_dir, self.config[\"data_sample_file\"])\n","            df_raw = pd.read_csv(csv_file_path)\n","            logger.info(f\"Raw dataset loaded. Shape: {df_raw.shape}\")\n","            if self.wandb_run:\n","                artifact = wandb.Artifact(name=\"raw-flights-data\", type=\"dataset\")\n","                artifact.add_dir(self.raw_data_dir)\n","                self.wandb_run.log_artifact(artifact)\n","                logger.info(\"Raw data artifact logged to WandB.\")\n","            return df_raw\n","        except FileNotFoundError:\n","            logger.critical(f\"Error: The expected CSV file '{self.config['data_sample_file']}' was not found after download.\", exc_info=True)\n","            raise\n","        except Exception as e:\n","            logger.critical(f\"An error occurred during data download or initial raw loading: {e}\", exc_info=True)\n","            raise\n","\n","    def _step_data_load_and_filter(self, df_raw: pd.DataFrame) -> pd.DataFrame:\n","        \"\"\"Step 2: Applies initial filtering for non-cancelled flights.\"\"\"\n","        logger.info(\"\\n--- Step 2: Data Loading & Initial Filtering ---\")\n","        initial_shape = df_raw.shape\n","        df_filtered = df_raw[df_raw['CANCELLED'] == 0].copy()\n","        logger.info(f\"Filtered for non-cancelled flights. Original shape: {initial_shape}, Filtered shape: {df_filtered.shape}\")\n","        return df_filtered\n","\n","    def _step_data_profiling(self, df: pd.DataFrame):\n","        \"\"\"Step 3: Generates and logs a data profiling report.\"\"\"\n","        logger.info(\"\\n--- Step 3: Data Profiling (Pre-preprocessing) ---\")\n","        report_name = \"flight_data_profile_report_pre_processing.html\"\n","        report_path = os.path.join(self.reports_dir, report_name)\n","\n","        if os.path.exists(report_path):\n","            logger.info(f\"Profiling report already exists at: {report_path}. Skipping generation, logging existing.\")\n","        else:\n","            try:\n","                self.profiler.generate_profile_report(df, report_name=report_name)\n","                logger.info(f\"Pre-processing data profiling complete. Report saved to: {report_path}\")\n","            except Exception as e:\n","                logger.error(f\"Error during pre-processing data profiling: {e}\", exc_info=True)\n","                return # Do not log artifact if generation failed\n","\n","        if self.wandb_run and os.path.exists(report_path):\n","            artifact = wandb.Artifact(name=\"pre_processing_data_profile\", type=\"report\")\n","            artifact.add_file(report_path)\n","            self.wandb_run.log_artifact(artifact)\n","            logger.info(\"Profiling report artifact logged to WandB.\")\n","\n","    def _step_data_preprocessing_eda(self, df_filtered: pd.DataFrame) -> pd.DataFrame:\n","        \"\"\"Step 4: Preprocesses data specifically for EDA visualizations.\"\"\"\n","        logger.info(\"\\n--- Step 4: Data Preprocessing for Visualization (EDA) ---\")\n","        df_eda = df_filtered.copy()\n","        try:\n","            # Convert to EDA specific target (e.g., ARR_DELAY > 10 for broad EDA)\n","            if 'ARR_DELAY' in df_eda.columns:\n","                df_eda['FLIGHT_STATUS_EDA'] = (df_eda['ARR_DELAY'] > 10).astype(int)\n","                logger.info(\"Added 'FLIGHT_STATUS_EDA' column for EDA.\")\n","\n","            # Add temporal features for EDA\n","            if 'FL_DATE' in df_eda.columns:\n","                df_eda['YEAR'] = pd.to_datetime(df_eda['FL_DATE']).dt.year\n","                df_eda['MONTH'] = pd.to_datetime(df_eda['FL_DATE']).dt.month\n","                logger.info(\"Added 'YEAR' and 'MONTH' columns for EDA time-series plots.\")\n","\n","            df_eda = self.preprocessor.create_elapsed_time_diff(df_eda) # Example of using a preprocessor method\n","            logger.info(f\"EDA preprocessing complete. EDA DataFrame shape: {df_eda.shape}\")\n","        except Exception as e:\n","            logger.error(f\"Error during EDA data preprocessing: {e}\", exc_info=True)\n","            df_eda = None # Indicate failure\n","        return df_eda\n","\n","    def _step_data_visualization_eda(self, df_eda: pd.DataFrame):\n","        \"\"\"Step 5: Generates and logs various EDA plots.\"\"\"\n","        if df_eda is None:\n","            logger.warning(\"\\n--- Data Visualization Skipped (EDA DataFrame is None) ---\")\n","            return\n","\n","        logger.info(\"\\n--- Step 5: Data Visualization (EDA) ---\")\n","        try:\n","            # Define plots and their filenames\n","            plots = [\n","                (self.visualizer.plot_column_distribution, [df_eda, 15, 4, \"all_column_distributions_eda.png\"], \"EDA/column_distributions\"),\n","                (self.visualizer.plot_airline_counts, [df_eda, \"airline_flight_counts_eda.png\"], \"EDA/airline_flight_counts\"),\n","                (self.visualizer.plot_destination_visits, [df_eda, self.config[\"eda_top_n_destinations\"], \"top_20_destination_visits_eda.png\"], \"EDA/top_destination_visits\"),\n","                (self.visualizer.plot_average_arrival_delay_by_airline, [df_eda, self.config[\"eda_min_flight_count_airline_delay\"], \"avg_arrival_delay_by_airline_eda.png\"], \"EDA/avg_arrival_delay_by_airline\"),\n","                (self.visualizer.plot_total_delays_by_year, [df_eda, \"total_delays_by_year_eda.png\"], \"EDA/total_delays_by_year\"),\n","                (self.visualizer.plot_monthly_delays_by_year, [df_eda, \"monthly_delays_by_year_eda.png\"], \"EDA/monthly_delays_by_year\"),\n","                (self.visualizer.plot_monthly_trend_with_highlight, [df_eda, 'ARR_DELAY', 'Monthly Total Delays Over Time', 'Total Delays (minutes)', \"monthly_delay_trend_highlight_eda.png\"], \"EDA/monthly_delay_trend\"),\n","                (self.visualizer.plot_delay_reason_analysis, [df_eda, \"delay_reason_breakdown_eda.png\"], \"EDA/delay_reason_breakdown\")\n","            ]\n","\n","            for plot_func, args, wandb_key in plots:\n","                filename = args[-1] # Filename is always the last argument\n","                plot_path = os.path.join(self.eda_plots_dir, filename)\n","                try:\n","                    plot_func(*args)\n","                    logger.info(f\"Generated {filename}\")\n","                    if self.wandb_run:\n","                        self.wandb_run.log({wandb_key: wandb.Image(plot_path)})\n","                        logger.info(f\"Logged {filename} to WandB.\")\n","                except Exception as p_e:\n","                    logger.warning(f\"Failed to generate or log {filename}: {p_e}\", exc_info=True)\n","\n","            logger.info(\"Data visualization complete. Plots saved to 'plots/eda/' directory and logged to WandB.\")\n","        except Exception as e:\n","            logger.error(f\"Error during data visualization: {e}\", exc_info=True)\n","\n","    def _step_data_preprocessing_modeling(self, df_filtered: pd.DataFrame) -> pd.DataFrame:\n","        \"\"\"Step 6: Preprocesses data for machine learning modeling.\"\"\"\n","        if df_filtered is None:\n","            logger.critical(\"\\n--- Modeling Data Preprocessing Skipped (Data not loaded) ---\")\n","            raise ValueError(\"Filtered DataFrame is None, cannot proceed with modeling preprocessing.\")\n","\n","        logger.info(\"\\n--- Step 6: Data Preprocessing for Modeling ---\")\n","        df_model = df_filtered.copy()\n","\n","        try:\n","            # Convert to Classification Target\n","            if 'ARR_DELAY' in df_model.columns:\n","                df_model[self.config[\"target_column\"]] = (df_model['ARR_DELAY'] > self.config[\"arr_delay_threshold_mins\"]).astype(int)\n","                logger.info(f\"Created '{self.config['target_column']}' binary target (1 if ARR_DELAY > {self.config['arr_delay_threshold_mins']}, 0 otherwise).\")\n","                logger.info(f\"{self.config['target_column']} value counts:\\n{df_model[self.config['target_column']].value_counts().to_string()}\")\n","            else:\n","                logger.critical(\"'ARR_DELAY' column not found for creating classification target. Exiting.\")\n","                raise ValueError(\"'ARR_DELAY' column missing.\")\n","\n","            delay_cols = ['DELAY_DUE_CARRIER', 'DELAY_DUE_WEATHER',\n","                          'DELAY_DUE_NAS', 'DELAY_DUE_SECURITY',\n","                          'DELAY_DUE_LATE_AIRCRAFT']\n","            df_model = self.preprocessor.handle_missing_values(df_model, delay_cols)\n","            logger.info(\"Handled missing values in delay columns.\")\n","\n","            df_model = self.preprocessor.create_elapsed_time_diff(df_model)\n","            logger.info(\"Created elapsed time difference feature.\")\n","\n","            time_columns = ['CRS_DEP_TIME', 'CRS_ARR_TIME', 'WHEELS_OFF', 'WHEELS_ON', 'DEP_TIME', 'ARR_TIME']\n","            df_model = self.preprocessor.apply_cyclical_encoding(df_model, time_columns)\n","            logger.info(\"Applied cyclical encoding to time columns.\")\n","\n","            city_state_columns = ['DEST_CITY', 'ORIGIN_CITY']\n","            df_model = self.preprocessor.split_city_state(df_model, city_state_columns)\n","            logger.info(\"Split city-state columns.\")\n","\n","            date_columns = ['FL_DATE']\n","            df_model = self.preprocessor.add_weekday_weekend_columns(df_model, date_columns)\n","            logger.info(\"Added weekday/weekend columns.\")\n","\n","            columns_to_exclude_model = [\n","                'AIRLINE_DOT', 'AIRLINE_CODE', 'DOT_CODE',\n","                'CANCELLED', 'DIVERTED', 'CANCELLATION_CODE',\n","                'FLIGHT_STATUS_EDA', # Drop EDA specific column\n","                'CRS_DEP_TIME', 'CRS_ARR_TIME', 'WHEELS_OFF', 'WHEELS_ON', 'DEP_TIME', 'ARR_TIME', # Original time columns\n","                'FL_DATE', 'ORIGIN', 'DEST', # Original location/date columns\n","                'DELAY_DUE_CARRIER', 'DELAY_DUE_WEATHER', 'DELAY_DUE_NAS', # Original delay reason columns (after handling missing)\n","                'DELAY_DUE_SECURITY', 'DELAY_DUE_LATE_AIRCRAFT',\n","                'ARR_DELAY' # Original target continuous column\n","            ]\n","            df_model = self.preprocessor.exclude_columns(df_model, columns_to_exclude_model)\n","            logger.info(f\"Excluded specified columns. Current shape: {df_model.shape}\")\n","\n","            df_model = self.preprocessor.encode_categorical_features(df_model)\n","            logger.info(f\"Encoded categorical features. Current shape: {df_model.shape}\")\n","\n","\n","            df_for_corr_check = df_model.drop(columns=[self.config[\"target_column\"]], errors='ignore')\n","            columns_to_drop_high_corr, _ = self.preprocessor.identify_high_correlation_pairs(df_for_corr_check, threshold=self.config[\"high_correlation_threshold\"])\n","\n","            if columns_to_drop_high_corr:\n","                logger.info(f\"Removing {len(columns_to_drop_high_corr)} columns due to high correlation (> {self.config['high_correlation_threshold']}): {columns_to_drop_high_corr}\")\n","                df_model = df_model.drop(columns=list(columns_to_drop_high_corr), errors='ignore')\n","            else:\n","                logger.info(\"No columns identified for removal due to high correlation.\")\n","\n","            logger.info(\"Data preprocessing for modeling complete.\")\n","            logger.info(f\"Final Modeling DataFrame shape: {df_model.shape}\")\n","            logger.info(f\"Final Modeling DataFrame columns (first 5):\\n{df_model.columns.tolist()[:5]}...\")\n","\n","            output_filepath_model = os.path.join(self.processed_data_dir, 'preprocessed_flight_data_for_modeling.csv')\n","            df_model.to_csv(output_filepath_model, index=False)\n","            logger.info(f\"Preprocessed data for modeling saved to: {output_filepath_model}\")\n","            if self.wandb_run:\n","                artifact = wandb.Artifact(name=\"preprocessed-modeling-data\", type=\"processed_data\")\n","                artifact.add_file(output_filepath_model)\n","                self.wandb_run.log_artifact(artifact)\n","                logger.info(\"Preprocessed data artifact logged to WandB.\")\n","\n","        except Exception as e:\n","            logger.critical(f\"Critical error during data preprocessing for modeling: {e}\", exc_info=True)\n","            df_model = None # Indicate failure\n","            raise # Re-raise for pipeline termination\n","        return df_model\n","\n","    def _step_data_modeling_and_evaluation(self, df_model: pd.DataFrame):\n","        \"\"\"Step 7: Handles data splitting, scaling, model training, and evaluation.\"\"\"\n","        if df_model is None or df_model.empty:\n","            logger.critical(\"\\n--- Data Modeling Skipped (Modeling DataFrame is None or empty) ---\")\n","            raise ValueError(\"Modeling DataFrame is None or empty, cannot proceed with modeling.\")\n","\n","        logger.info(\"\\n--- Step 7: Data Modeling and Evaluation ---\")\n","\n","        if self.config[\"target_column\"] not in df_model.columns:\n","            logger.critical(f\"Target column '{self.config['target_column']}' not found in the modeling DataFrame.\")\n","            raise ValueError(\"Target column missing from modeling DataFrame.\")\n","\n","        X = df_model.drop(columns=[self.config[\"target_column\"]])\n","        y = df_model[self.config[\"target_column\"]]\n","\n","        logger.info(f\"Features (X) shape: {X.shape}, Target (y) shape: {y.shape}\")\n","        logger.info(f\"Target variable distribution:\\n{y.value_counts().to_string()}\")\n","\n","        # Final NaN drop (for any remaining NaNs after encoding or other transforms)\n","        rows_before_final_nan_drop = X.shape[0]\n","        X = X.dropna(axis=0, how='any') # Drop rows with ANY NaN in features\n","        y = y.loc[X.index] # Align y with X\n","        if X.shape[0] < rows_before_final_nan_drop:\n","            rows_removed_nan = rows_before_final_nan_drop - X.shape[0]\n","            logger.warning(f\"Removed {rows_removed_nan} rows due to NaN values in features after final preprocessing.\")\n","            logger.info(f\"Updated Features (X) shape: {X.shape}, Target (y) shape: {y.shape}\")\n","\n","        if X.empty or y.empty:\n","            logger.critical(\"Features or target DataFrame is empty after final NaN handling. Cannot proceed with modeling.\")\n","            raise ValueError(\"Empty DataFrame after final NaN handling.\")\n","\n","        # Identify numerical columns for scaling (excluding binary and cyclical)\n","        numerical_features = X.select_dtypes(include=np.number).columns.tolist()\n","        cols_to_scale = [\n","            col for col in numerical_features\n","            if not (X[col].dropna().isin([0, 1]).all() or col.endswith('_SIN') or col.endswith('_COS'))\n","        ]\n","        logger.info(f\"Identified {len(cols_to_scale)} numerical features for scaling.\")\n","\n","        scaler_name = 'feature_scaler_classification'\n","        scaler = self.evaluator.load_model(scaler_name)\n","\n","        if scaler:\n","            X_scaled = X.copy()\n","            if cols_to_scale:\n","                X_scaled[cols_to_scale] = scaler.transform(X_scaled[cols_to_scale])\n","            logger.info(\"Loaded and applied existing StandardScaler to numerical features.\")\n","        else:\n","            scaler = StandardScaler()\n","            X_scaled = X.copy()\n","            if cols_to_scale:\n","                X_scaled[cols_to_scale] = scaler.fit_transform(X_scaled[cols_to_scale])\n","            self.evaluator.save_model(scaler, scaler_name)\n","            logger.info(\"Trained and saved new StandardScaler for numerical features.\")\n","\n","        # Train-Validation-Test Split on SCALED data\n","        X_train_val, X_test, y_train_val, y_test = train_test_split(X_scaled, y, test_size=self.config[\"test_size\"], random_state=self.config[\"random_state\"], stratify=y)\n","        # Handle cases where stratify might fail for very small classes (unlikely with 10k samples)\n","        if len(np.unique(y_train_val)) < 2 or len(np.unique(y_test)) < 2:\n","             logger.warning(\"Stratified split resulted in fewer than two classes in train/test set. Skipping further validation split if necessary.\")\n","             X_train, X_val, y_train, y_val = X_train_val, None, y_train_val, None # No separate validation set\n","        else:\n","            # Ensure the validation split ratio is correctly applied to the train_val set\n","            # test_size_for_val = self.config[\"validation_split_ratio\"] / (1 - self.config[\"test_size\"])\n","            X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=self.config[\"validation_split_ratio\"], random_state=self.config[\"random_state\"], stratify=y_train_val)\n","            logger.info(f\"Further split of Train-Val: Train={X_train.shape[0]} samples, Validation={X_val.shape[0]} samples\")\n","\n","\n","        logger.info(f\"Data split: Train-Val (for HPO/Training)={X_train_val.shape[0]} samples, Test (final evaluation)={X_test.shape[0]} samples\")\n","        logger.info(f\"Test Set Target Distribution:\\n{y_test.value_counts().to_string()}\")\n","\n","\n","        models_config = {\n","            \"baseline_model\": {\n","                \"func\": self.modeler.run_baseline_model_classification,\n","                \"train_args\": (y_train_val, y_test),\n","                \"predict_data\": y_test\n","            },\n","            \"logistic_regression_model\": {\n","                \"func\": self.modeler.run_logistic_regression,\n","                \"train_args\": (X_train_val, y_train_val),\n","                \"predict_data\": X_test\n","            },\n","            \"neural_network_classification_model\": {\n","                \"func\": self.modeler.run_neural_network_classification,\n","                \"train_args\": (X_train, y_train, X_val, y_val),\n","                \"predict_data\": X_test\n","            }\n","        }\n","\n","        trained_models = {}\n","        for model_name, cfg in models_config.items():\n","            logger.info(f\"\\n--- Processing {model_name.replace('_', ' ').title()} ---\")\n","\n","            model = self.evaluator.load_model(model_name)\n","\n","            if model is not None:\n","                logger.info(f\"Loaded existing model: {model_name}\")\n","            else:\n","                logger.info(f\"Training new model: {model_name}\")\n","                model = cfg[\"func\"](*cfg[\"train_args\"])\n","                self.evaluator.save_model(model, model_name)\n","\n","            trained_models[model_name] = model\n","\n","            # Make predictions\n","            y_pred_proba = None\n","            y_pred = None\n","\n","            if model_name == \"baseline_model\":\n","                majority_class = y_train_val.mode()[0]\n","                y_pred_proba = np.full(len(cfg[\"predict_data\"]), float(majority_class))\n","                y_pred = y_pred_proba.astype(int)\n","            elif isinstance(model, tf.keras.Model):\n","                y_pred_proba = model.predict(cfg[\"predict_data\"], verbose=0).flatten()\n","                y_pred = (y_pred_proba > 0.5).astype(int)\n","            else: # Scikit-learn models\n","                if hasattr(model, 'predict_proba'):\n","                    y_pred_proba = model.predict_proba(cfg[\"predict_data\"])[:, 1]\n","                else: # For models without predict_proba (e.g., some simple estimators)\n","                    logger.warning(f\"Model {model_name} does not have predict_proba. Using predict for probabilities (may not be well-calibrated).\")\n","                    y_pred_proba = model.predict(cfg[\"predict_data\"]).astype(float)\n","                y_pred = model.predict(cfg[\"predict_data\"])\n","\n","            # Evaluate\n","            self.evaluator.evaluate_classification_model(\n","                model_name.replace('_', ' ').title(),\n","                y_test, y_pred, y_pred_proba, stage=\"Test\"\n","            )\n","\n","            # Plot ROC and Confusion Matrix (skip for baseline)\n","            if model_name != \"baseline_model\":\n","                self.evaluator.plot_roc_curve(model_name.replace('_', ' ').title(), y_test, y_pred_proba, stage=\"Test\")\n","                self.evaluator.plot_confusion_matrix(model_name.replace('_', ' ').title(), y_test, y_pred, stage=\"Test\")\n","\n","            # Model Explainability (skip for baseline)\n","            if model_name != \"baseline_model\" and trained_models[model_name] is not None:\n","                self._step_model_explainability(model_name, trained_models[model_name], X_test)\n","            else:\n","                logger.info(f\"Model explainability skipped for {model_name}.\")\n","\n","        self.evaluator.display_results_table()\n","\n","    def _step_model_explainability(self, model_name: str, model, X_test: pd.DataFrame):\n","        \"\"\"Step 8: Generates and logs SHAP and coefficient plots for trained models.\"\"\"\n","        if self.explainer is None:\n","            logger.warning(\"Model explainer not initialized (likely due to WandB issue). Skipping explainability plots.\")\n","            return\n","        if model is None:\n","            logger.warning(f\"Model {model_name} is None. Cannot perform explainability.\")\n","            return\n","\n","        logger.info(f\"\\n--- Step 8: Model Explainability ({model_name.replace('_', ' ').title()}) ---\")\n","        try:\n","            X_explain_sample = X_test.sample(n=min(self.config[\"shap_explain_sample_size\"], X_test.shape[0]), random_state=self.config[\"random_state\"])\n","            task_type = 'classification'\n","\n","            # SHAP Summary Plot\n","            self.explainer.plot_shap_summary(\n","                model=model,\n","                X=X_explain_sample,\n","                filename=f\"{model_name}_shap_summary_plot.png\",\n","                task_type=task_type\n","            )\n","            logger.info(f\"SHAP summary plots generated for {model_name}.\")\n","\n","            # SHAP Dependence Plots for a few features\n","            # Prioritize features based on anticipated importance or from initial SHAP summary\n","            potential_top_features = X_explain_sample.columns.tolist()\n","            features_for_dependence = []\n","\n","            # Smart selection of features for dependence plots\n","            # You might want to get top N features dynamically after running SHAP summary once\n","            # For now, keeping a sensible list based on expected features from your pipeline\n","            if 'CRS_ELAPSED_TIME' in potential_top_features: features_for_dependence.append('CRS_ELAPSED_TIME')\n","            if 'DISTANCE' in potential_top_features: features_for_dependence.append('DISTANCE')\n","            if 'AIRLINE_WN' in potential_top_features: features_for_dependence.append('AIRLINE_WN') # Example OHE feature\n","            if 'ORIGIN_STATE_CA' in potential_top_features: features_for_dependence.append('ORIGIN_STATE_CA') # Example OHE feature\n","            if 'DAY_OF_WEEK_is_weekend' in potential_top_features: features_for_dependence.append('DAY_OF_WEEK_is_weekend')\n","            if 'CRS_DEP_TIME_SIN' in potential_top_features: features_for_dependence.append('CRS_DEP_TIME_SIN')\n","            if 'MONTH_8' in potential_top_features: features_for_dependence.append('MONTH_8') # Example month from OHE\n","\n","            if features_for_dependence:\n","                logger.info(f\"Generating SHAP dependence plots for: {features_for_dependence}\")\n","                for feature in features_for_dependence:\n","                    if feature in X_explain_sample.columns: # Final check\n","                        self.explainer.plot_shap_dependence(\n","                            model=model,\n","                            X=X_explain_sample,\n","                            feature=feature,\n","                            filename=f\"{model_name}_shap_dependence_plot_{feature}.png\",\n","                            task_type=task_type\n","                        )\n","                    else:\n","                        logger.warning(f\"Feature '{feature}' not found in the explainability sample. Skipping dependence plot.\")\n","            else:\n","                logger.info(f\"No predefined or suitable features found for SHAP dependence plots for {model_name}.\")\n","\n","            # Coefficient Importance for linear models\n","            if isinstance(model, LogisticRegression):\n","                self.explainer.plot_coefficient_importance(\n","                    model=model,\n","                    feature_names=X_explain_sample.columns.tolist(),\n","                    filename=f\"{model_name}_coefficient_importance.png\"\n","                )\n","                logger.info(f\"Coefficient importance plot generated for {model_name}.\")\n","\n","        except Exception as e:\n","            logger.error(f\"Error during model explainability for {model_name.replace('_', ' ').title()}: {e}\", exc_info=True)\n","\n","\n","if __name__ == \"__main__\":\n","    # Define common configuration for WandB and pipeline\n","    # These parameters can be easily changed for new experiments\n","    pipeline_config = {\n","        \"project_name\": \"flight-delay-prediction-final\",\n","        \"run_name\": \"full_pipeline_refactored_v1\",\n","        \"dataset_name\": \"patrickzel/flight-delay-and-cancellation-data-2019-2023-v2\",\n","        \"data_sample_file\": \"flights_sample_10k.csv\",\n","        \"target_column\": \"FLIGHT_STATUS\",\n","        \"arr_delay_threshold_mins\": 15, # For binary classification target: delay > 15 mins\n","        \"test_size\": 0.15, # Fraction of data for final test set\n","        \"validation_split_ratio\": 0.15, # Fraction of (train+val) for validation set, e.g. 0.15 of train_val\n","        \"random_state\": 42,\n","        \"high_correlation_threshold\": 0.9, # For feature removal\n","        \"eda_top_n_destinations\": 20,\n","        \"eda_min_flight_count_airline_delay\": 500,\n","        \"shap_explain_sample_size\": 1000, # Number of samples for SHAP calculations\n","        \"nn_epochs\": 10, # Neural Network training epochs\n","        \"nn_batch_size\": 32 # Neural Network training batch size\n","    }\n","\n","    # Create and run the pipeline\n","    pipeline = FlightDelayPipeline(pipeline_config)\n","    pipeline.run()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":384},"id":"t4UK85wJT8-i","executionInfo":{"status":"error","timestamp":1752602899568,"user_tz":-120,"elapsed":225,"user":{"displayName":"Jai Kushwaha","userId":"01340704951735287618"}},"outputId":"18d88cbc-bce4-41cd-855c-9e992fd11a21"},"execution_count":16,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'scripts'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-16-784434001.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Reload custom modules to ensure the latest changes are picked up\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# This is crucial in environments like Jupyter/Colab where modules might be cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mscripts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscripts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_loading\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscripts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_profiling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scripts'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["%%python pipeline.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"r_r1LNZW40Oy","executionInfo":{"status":"error","timestamp":1752596382108,"user_tz":-120,"elapsed":17353,"user":{"displayName":"Jai Kushwaha","userId":"01340704951735287618"}},"outputId":"8ac07e42-4e29-4a6d-d0bf-ca6d41e32ab1"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset URL: https://www.kaggle.com/datasets/patrickzel/flight-delay-and-cancellation-data-2019-2023-v2\n","License(s): other\n","flight-delay-and-cancellation-data-2019-2023-v2.zip: Skipping, found more recently modified local copy (use --force to force download)\n","\u001b[1;34mUpgrade to ydata-sdk\u001b[0m\n","Improve your data and profiling with ydata-sdk, featuring data quality scoring, redundancy detection, outlier identification, text validation, and synthetic data generation.\n","Register at https://ydata.ai/register\n","Generating column distribution plots and saving to all_column_distributions_eda.png...\n","Generating airline counts plot and saving to airline_flight_counts_eda.png...\n","Generating top 20 destination visits plot and saving to top_20_destination_visits_eda.png...\n","Generating average arrival delay by airline plot and saving to avg_arrival_delay_by_airline_eda.png...\n","\n","Additional Insights (Average Delay by Airline):\n","- Worst performing airline: Southwest Airlines Co. (30.6 min avg delay)\n","- Best performing airline: Southwest Airlines Co. (30.6 min avg delay)\n","- Total flights analyzed for this plot: 699\n","- Percentage of flights considered 'delayed' for this analysis: 32.9%\n","Generating total delays by year plot and saving to total_delays_by_year_eda.png...\n","The year with the most delays was 2022 with 15,680 total delay minutes.\n","Generating monthly delays by year plot and saving to monthly_delays_by_year_eda.png...\n","Generating monthly trend with highlight plot and saving to monthly_delay_trend_highlight_eda.png...\n","The year with the least (minutes) was 2020 with -7,355 total.\n","Generating delay reason analysis plots and saving to delay_reason_breakdown_eda.png...\n","\n","Delay Reason Analysis Summary:\n","------------------------------\n","Late Aircraft       :       48,461 minutes (40.8%)\n","Carrier             :       40,923 minutes (34.4%)\n","Nas                 :       22,747 minutes (19.1%)\n","Weather             :        6,438 minutes (5.4%)\n","Security            :          347 minutes (0.3%)\n","\n","The primary reason for delays is: Late Aircraft (40.8% of all delays)\n"]},{"output_type":"stream","name":"stderr","text":["2025-07-15 16:19:28.308564: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n","2025-07-15 16:19:28.313056: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n","2025-07-15 16:19:28.325203: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1752596368.345141   12097 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1752596368.351091   12097 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1752596368.366981   12097 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1752596368.367005   12097 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1752596368.367007   12097 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1752596368.367009   12097 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","2025-07-15 16:19:28.371844: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","INFO: Running in Google Colab: False\n","ERROR: Failed to initialize WandB: api_key not configured (no-tty). call wandb.login(key=[your_api_key]). Proceeding without WandB logging.\n","Traceback (most recent call last):\n","  File \"/content/drive/MyDrive/Colab Notebooks/flight_delay/pipeline.py\", line 82, in <module>\n","    wandb_run = wandb.init(\n","                ^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_init.py\", line 1620, in init\n","    wandb._sentry.reraise(e)\n","  File \"/usr/local/lib/python3.11/dist-packages/wandb/analytics/sentry.py\", line 157, in reraise\n","    raise exc.with_traceback(sys.exc_info()[2])\n","  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_init.py\", line 1548, in init\n","    wi.maybe_login(init_settings)\n","  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_init.py\", line 191, in maybe_login\n","    wandb_login._login(\n","  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_login.py\", line 315, in _login\n","    key, key_status = wlogin.prompt_api_key(referrer=referrer)\n","                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_login.py\", line 243, in prompt_api_key\n","    raise UsageError(\"api_key not configured (no-tty). call \" + directive)\n","wandb.errors.errors.UsageError: api_key not configured (no-tty). call wandb.login(key=[your_api_key])\n","INFO: --- Starting ML Project Pipeline ---\n","2025-07-15 16:19:32,292 - scripts.data_preprocessor - INFO - DataPreprocessor initialized.\n","2025-07-15 16:19:32,295 - scripts.data_evaluate - INFO - ModelEvaluator initialized. Models will be saved to 'models', plots to 'plots/model_evaluation'.\n","2025-07-15 16:19:32,296 - scripts.model_tuning - INFO - ModelTuner initialized. Best parameters will be saved to 'model_params'.\n","2025-07-15 16:19:32,297 - scripts.data_modelling - INFO - DataModeling initialized.\n","2025-07-15 16:19:32,297 - scripts.model_explainability - INFO - ModelExplainer initialized. Explainability plots will be saved to 'plots/explainability'.\n","INFO: \n","--- Step 1: Data Download ---\n","2025-07-15 16:19:32,297 - scripts.data_loading - INFO - DataDownload initialized. Running in Colab: False\n","2025-07-15 16:19:32,298 - scripts.data_loading - INFO - Downloading dataset 'patrickzel/flight-delay-and-cancellation-data-2019-2023-v2' to 'data/raw'...\n","2025-07-15 16:19:33,728 - scripts.data_loading - INFO - Dataset downloaded and unzipped successfully!\n","2025-07-15 16:19:33,730 - scripts.data_loading - ERROR - Could not find the unzipped dataset directory containing 'flights_sample_100k.csv' in data/raw.\n","INFO: Dataset download/check complete. Data should be accessible via: data/raw/flight_delay_and_cancellation_data_2019_2023_v2\n","INFO: \n","--- Step 2: Data Loading & Initial Filtering ---\n","INFO: Attempting to load CSV from: data/raw/flights_sample_10k.csv\n","INFO: Dataset loaded successfully!\n","INFO: First 5 rows of the raw dataframe:\n","      FL_DATE                AIRLINE                AIRLINE_DOT AIRLINE_CODE  DOT_CODE  FL_NUMBER ORIGIN     ORIGIN_CITY DEST          DEST_CITY  CRS_DEP_TIME  DEP_TIME  DEP_DELAY  TAXI_OUT  WHEELS_OFF  WHEELS_ON  TAXI_IN  CRS_ARR_TIME  ARR_TIME  ARR_DELAY  CANCELLED CANCELLATION_CODE  DIVERTED  CRS_ELAPSED_TIME  ELAPSED_TIME  AIR_TIME  DISTANCE  DELAY_DUE_CARRIER  DELAY_DUE_WEATHER  DELAY_DUE_NAS  DELAY_DUE_SECURITY  DELAY_DUE_LATE_AIRCRAFT\n","0  2022-07-19  SkyWest Airlines Inc.  SkyWest Airlines Inc.: OO           OO     20304       3371    SAN   San Diego, CA  SFO  San Francisco, CA          1705    1700.0       -5.0      13.0      1713.0     1816.0     11.0          1834    1827.0       -7.0        0.0               NaN       0.0              89.0          87.0      63.0     447.0                NaN                NaN            NaN                 NaN                      NaN\n","1  2022-09-13       Republic Airline       Republic Airline: YX           YX     20452       3552    CMH    Columbus, OH  ORD        Chicago, IL          1119    1118.0       -1.0      16.0      1134.0     1125.0     11.0          1147    1136.0      -11.0        0.0               NaN       0.0              88.0          78.0      51.0     296.0                NaN                NaN            NaN                 NaN                      NaN\n","2  2022-07-09  SkyWest Airlines Inc.  SkyWest Airlines Inc.: OO           OO     20304       4660    CVG  Cincinnati, OH  ORD        Chicago, IL          1118    1113.0       -5.0      18.0      1131.0     1124.0      8.0          1155    1132.0      -23.0        0.0               NaN       0.0              97.0          79.0      53.0     264.0                NaN                NaN            NaN                 NaN                      NaN\n","3  2021-03-19  United Air Lines Inc.  United Air Lines Inc.: UA           UA     19977        325    DEN      Denver, CO  MCI    Kansas City, MO          1815    1815.0        0.0      15.0      1830.0     2042.0      5.0          2051    2047.0       -4.0        0.0               NaN       0.0              96.0          92.0      72.0     533.0                NaN                NaN            NaN                 NaN                      NaN\n","4  2020-01-03  United Air Lines Inc.  United Air Lines Inc.: UA           UA     19977       1561    IAH     Houston, TX  SFO  San Francisco, CA          1000    1001.0        1.0      21.0      1022.0     1205.0      9.0          1227    1214.0      -13.0        0.0               NaN       0.0             267.0         253.0     223.0    1635.0                NaN                NaN            NaN                 NaN                      NaN\n","INFO: Shape of the raw dataframe: (10000, 32)\n","INFO: Filtered for non-cancelled flights. Original shape: (10000, 32), Filtered shape: (9758, 32)\n","INFO: \n","--- Step 3: Data Profiling (Pre-preprocessing) ---\n","INFO: Profiling report already exists at: reports/flight_data_profile_report_pre_processing.html. Skipping generation.\n","INFO: \n","--- Step 4: Data Preprocessing for Visualization (EDA) ---\n","INFO: Added 'FLIGHT_STATUS_EDA' column for EDA. Value counts:\n","FLIGHT_STATUS_EDA\n","0    7703\n","1    2055\n","INFO: Added 'YEAR' and 'MONTH' columns for EDA time-series plots.\n","2025-07-15 16:19:33,804 - scripts.data_preprocessor - INFO - Creating 'ELAPSED_TIME_DIFF' feature...\n","2025-07-15 16:19:33,804 - scripts.data_preprocessor - WARNING - Required time column 'ACTUAL_ELAPSED_TIME' not found. Cannot create 'ELAPSED_TIME_DIFF'.\n","INFO: EDA preprocessing complete. EDA DataFrame shape: (9758, 35)\n","INFO: \n","--- Step 5: Data Visualization (EDA) ---\n","/content/drive/MyDrive/Colab Notebooks/flight_delay/scripts/data_visualizer.py:86: FutureWarning: \n","\n","Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n","\n","  g = sns.countplot(y=\"AIRLINE\", data=df_category, order=order_desc, palette='viridis')\n","/content/drive/MyDrive/Colab Notebooks/flight_delay/scripts/data_visualizer.py:209: UserWarning: Attempting to set identical low and high xlims makes transformation singular; automatically expanding.\n","  plt.xlim(min_delay - padding, max_delay + padding)\n","/content/drive/MyDrive/Colab Notebooks/flight_delay/scripts/data_visualizer.py:251: FutureWarning: \n","\n","Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n","\n","  barplot = sns.barplot(\n","/content/drive/MyDrive/Colab Notebooks/flight_delay/scripts/data_visualizer.py:438: FutureWarning: \n","\n","Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n","\n","  ax = sns.barplot(x=total_delays.values, y=total_delays.index,\n","INFO: Data visualization complete. Plots saved to 'plots/eda/' directory and logged to WandB.\n","INFO: \n","--- Step 6: Data Preprocessing for Modeling ---\n","INFO: Created 'FLIGHT_STATUS' binary target (1 if ARR_DELAY > 15, 0 otherwise).\n","INFO: FLIGHT_STATUS value counts:\n","FLIGHT_STATUS\n","0    8094\n","1    1664\n","2025-07-15 16:19:36,870 - scripts.data_preprocessor - INFO - Handling missing values...\n","2025-07-15 16:19:36,875 - scripts.data_preprocessor - INFO - Dropped 19 rows due to NaN values in 'ARR_DELAY'.\n","2025-07-15 16:19:36,875 - scripts.data_preprocessor - INFO - DataFrame shape after handling missing values: (9739, 33)\n","2025-07-15 16:19:36,875 - scripts.data_preprocessor - INFO - Creating 'ELAPSED_TIME_DIFF' feature...\n","2025-07-15 16:19:36,875 - scripts.data_preprocessor - WARNING - Required time column 'ACTUAL_ELAPSED_TIME' not found. Cannot create 'ELAPSED_TIME_DIFF'.\n","2025-07-15 16:19:36,875 - scripts.data_preprocessor - INFO - Applying cyclical encoding to time columns...\n","2025-07-15 16:19:36,888 - scripts.data_preprocessor - INFO - Splitting city and state columns...\n","2025-07-15 16:19:36,919 - scripts.data_preprocessor - INFO - Adding weekday and weekend columns...\n","2025-07-15 16:19:36,922 - scripts.data_preprocessor - INFO - Excluding specified columns...\n","2025-07-15 16:19:36,925 - scripts.data_preprocessor - INFO - Excluded columns: ['AIRLINE_DOT', 'AIRLINE_CODE', 'DOT_CODE', 'CANCELLED', 'DIVERTED', 'CANCELLATION_CODE', 'CRS_DEP_TIME', 'CRS_ARR_TIME', 'WHEELS_OFF', 'WHEELS_ON', 'DEP_TIME', 'ARR_TIME', 'FL_DATE', 'ORIGIN', 'DEST', 'DELAY_DUE_CARRIER', 'DELAY_DUE_WEATHER', 'DELAY_DUE_NAS', 'DELAY_DUE_SECURITY', 'DELAY_DUE_LATE_AIRCRAFT', 'ARR_DELAY']\n","2025-07-15 16:19:36,927 - scripts.data_preprocessor - INFO - Applying WOE encoding to all categorical features based on FLIGHT_STATUS.\n","2025-07-15 16:19:36,928 - scripts.data_preprocessor - INFO - Found 5 categorical columns for WOE encoding: ['ORIGIN_CITY_CITY', 'DEST_CITY_CITY', 'ORIGIN_CITY_STATE', 'DEST_CITY_STATE', 'AIRLINE']\n","2025-07-15 16:19:36,998 - scripts.data_preprocessor - INFO - Successfully applied WOE encoding to 5 categorical columns.\n","2025-07-15 16:19:36,998 - scripts.data_preprocessor - INFO - Created WOE encoded columns: ['ORIGIN_CITY_CITY_WOE', 'DEST_CITY_CITY_WOE', 'ORIGIN_CITY_STATE_WOE', 'DEST_CITY_STATE_WOE', 'AIRLINE_WOE']\n","2025-07-15 16:19:37,000 - scripts.data_preprocessor - INFO - Dropped original categorical columns: ['ORIGIN_CITY_CITY', 'DEST_CITY_CITY', 'ORIGIN_CITY_STATE', 'DEST_CITY_STATE', 'AIRLINE']\n","2025-07-15 16:19:37,001 - scripts.data_preprocessor - INFO - Identifying high correlation features (threshold=0.9)...\n","2025-07-15 16:19:37,034 - scripts.data_preprocessor - INFO - Found 12 columns to drop due to high correlation.\n","INFO: Removing 12 columns due to high correlation: {'ARR_TIME_MINUTES_COS', 'AIR_TIME', 'ELAPSED_TIME', 'WHEELS_OFF_SIN', 'DISTANCE', 'ARR_TIME_MINUTES_SIN', 'WHEELS_OFF_COS', 'DEP_TIME_MINUTES_COS', 'WHEELS_ON_COS', 'DEP_TIME_MINUTES_SIN', 'DEP_TIME_MINUTES', 'WHEELS_ON_SIN'}\n","INFO: Data preprocessing for modeling complete.\n","INFO: Final Modeling DataFrame shape: (9739, 20)\n","INFO: Final Modeling DataFrame columns (first 10):\n","['FL_NUMBER', 'DEP_DELAY', 'TAXI_OUT', 'TAXI_IN', 'CRS_ELAPSED_TIME', 'FLIGHT_STATUS', 'CRS_DEP_TIME_MINUTES', 'CRS_DEP_TIME_MINUTES_SIN', 'CRS_DEP_TIME_MINUTES_COS', 'CRS_ARR_TIME_MINUTES']...\n","INFO: Final Modeling DataFrame columns (last 10):\n","['CRS_ARR_TIME_MINUTES_SIN', 'CRS_ARR_TIME_MINUTES_COS', 'ARR_TIME_MINUTES', 'DAY_OF_WEEK', 'IS_WEEKEND', 'ORIGIN_CITY_CITY_WOE', 'DEST_CITY_CITY_WOE', 'ORIGIN_CITY_STATE_WOE', 'DEST_CITY_STATE_WOE', 'AIRLINE_WOE']\n","INFO: Preprocessed data for modeling saved to: data/processed/preprocessed_flight_data_for_modeling.csv\n","INFO: \n","--- Step 7: Data Modeling and Evaluation ---\n","INFO: Features (X) shape: (9739, 19), Target (y) shape: (9739,)\n","INFO: Target variable distribution:\n","FLIGHT_STATUS\n","0    8075\n","1    1664\n","Name: count, dtype: int64\n","2025-07-15 16:19:37,226 - scripts.data_evaluate - INFO - No model found for 'feature_scaler_classification' at models\n","2025-07-15 16:19:37,236 - scripts.data_evaluate - INFO - Scikit-learn model saved to models/feature_scaler_classification.pkl\n","INFO: Trained and saved new StandardScaler for numerical features.\n","INFO: Data split: Train-Val (for HPO/Training)=8278 samples, Test (final evaluation)=1461 samples\n","INFO: Further split of Train-Val: Train=6817 samples, Validation=1461 samples\n","INFO: Test Set Target Distribution:\n","FLIGHT_STATUS\n","0    1211\n","1     250\n","Name: count, dtype: int64\n","INFO: \n","--- Processing Baseline Model ---\n","2025-07-15 16:19:37,247 - scripts.data_evaluate - INFO - No model found for 'baseline_model' at models\n","INFO: Training new model: baseline_model\n","2025-07-15 16:19:37,247 - scripts.data_modelling - INFO - --- Running Baseline Classification Model (Majority Class Classifier) ---\n","2025-07-15 16:19:37,249 - scripts.data_modelling - INFO - Baseline (majority class) for training data: 0\n","2025-07-15 16:19:37,249 - scripts.data_modelling - INFO - Baseline Classification Model training complete.\n","2025-07-15 16:19:37,252 - scripts.data_evaluate - INFO - Scikit-learn model saved to models/baseline_model.pkl\n","2025-07-15 16:19:37,253 - scripts.data_evaluate - INFO - --- Baseline Model Evaluation (Test Set) ---\n","2025-07-15 16:19:37,261 - scripts.data_evaluate - INFO - Accuracy: 0.8289\n","2025-07-15 16:19:37,261 - scripts.data_evaluate - INFO - Precision: 0.0000\n","2025-07-15 16:19:37,262 - scripts.data_evaluate - INFO - Recall: 0.0000\n","2025-07-15 16:19:37,263 - scripts.data_evaluate - INFO - F1 Score: 0.0000\n","2025-07-15 16:19:37,264 - scripts.data_evaluate - INFO - ROC AUC: 0.5000\n","2025-07-15 16:19:37,270 - scripts.data_evaluate - INFO - \n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.83      1.00      0.91      1211\n","           1       0.00      0.00      0.00       250\n","\n","    accuracy                           0.83      1461\n","   macro avg       0.41      0.50      0.45      1461\n","weighted avg       0.69      0.83      0.75      1461\n","\n","INFO: Model explainability skipped for other models or if model not trained.\n","INFO: \n","--- Processing Logistic Regression Model ---\n","2025-07-15 16:19:37,271 - scripts.data_evaluate - INFO - No model found for 'logistic_regression_model' at models\n","INFO: Training new model: logistic_regression_model\n","2025-07-15 16:19:37,271 - scripts.data_modelling - INFO - \n","--- Running Logistic Regression (Classification) ---\n","2025-07-15 16:19:37,271 - scripts.data_modelling - INFO - Using loaded best parameters for Logistic Regression to train final model.\n","2025-07-15 16:19:37,304 - scripts.data_evaluate - INFO - Scikit-learn model saved to models/logistic_regression_model.pkl\n","2025-07-15 16:19:37,307 - scripts.data_evaluate - INFO - --- Logistic Regression Model Evaluation (Test Set) ---\n","2025-07-15 16:19:37,314 - scripts.data_evaluate - INFO - Accuracy: 0.9541\n","2025-07-15 16:19:37,315 - scripts.data_evaluate - INFO - Precision: 0.9552\n","2025-07-15 16:19:37,315 - scripts.data_evaluate - INFO - Recall: 0.7680\n","2025-07-15 16:19:37,316 - scripts.data_evaluate - INFO - F1 Score: 0.8514\n","2025-07-15 16:19:37,317 - scripts.data_evaluate - INFO - ROC AUC: 0.9723\n","2025-07-15 16:19:37,324 - scripts.data_evaluate - INFO - \n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.95      0.99      0.97      1211\n","           1       0.96      0.77      0.85       250\n","\n","    accuracy                           0.95      1461\n","   macro avg       0.95      0.88      0.91      1461\n","weighted avg       0.95      0.95      0.95      1461\n","\n","2025-07-15 16:19:37,931 - scripts.data_evaluate - INFO - Saved ROC curve plot to plots/model_evaluation/logistic_regression_model_roc_curve_test.png\n","2025-07-15 16:19:38,717 - scripts.data_evaluate - INFO - Saved confusion matrix plot to plots/model_evaluation/logistic_regression_model_confusion_matrix_test.png\n","INFO: \n","--- Step 8: Model Explainability (Logistic Regression Model) ---\n","2025-07-15 16:19:39,999 - scripts.model_explainability - INFO - Saved SHAP summary bar plot to plots/explainability/logistic_regression_model_shap_summary_plot.png\n","2025-07-15 16:19:40,912 - scripts.model_explainability - INFO - Saved SHAP summary dot plot to plots/explainability/logistic_regression_model_shap_summary_plot_dot.png\n","2025-07-15 16:19:41,266 - scripts.model_explainability - INFO - Saved SHAP dependence plot for 'CRS_ELAPSED_TIME' to plots/explainability/logistic_regression_model_shap_dependence_plot_CRS_ELAPSED_TIME.png\n","2025-07-15 16:19:41,748 - scripts.model_explainability - INFO - Saved coefficient importance plot to plots/explainability/logistic_regression_model_coefficient_importance.png\n","INFO: Coefficient importance plot generated for Logistic Regression Model.\n","INFO: \n","--- Processing Neural Network Classification Model ---\n","2025-07-15 16:19:41,749 - scripts.data_evaluate - INFO - No model found for 'neural_network_classification_model' at models\n","INFO: Training new model: neural_network_classification_model\n","Traceback (most recent call last):\n","  File \"/content/drive/MyDrive/Colab Notebooks/flight_delay/pipeline.py\", line 443, in <module>\n","    model = cfg[\"func\"](*cfg[\"train_args\"])\n","            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","TypeError: DataModeling.run_neural_network_classification() takes 5 positional arguments but 7 were given\n"]},{"output_type":"error","ename":"CalledProcessError","evalue":"Command 'b' \\n'' returned non-zero exit status 1.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-13-2470989330.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'python'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pipeline.py'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m       \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2471\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2472\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2473\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2474\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/decorator.py\u001b[0m in \u001b[0;36mfun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkwsyntax\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcaller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextras\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mCalledProcessError\u001b[0m: Command 'b' \\n'' returned non-zero exit status 1."]}]},{"cell_type":"code","source":["import pandas as pd\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/flight_delay/data/processed/preprocessed_flight_data_for_modeling.csv')\n"],"metadata":{"id":"X6LDJgaGUL_U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i5JBCh28ZbFc","executionInfo":{"status":"ok","timestamp":1752298503198,"user_tz":-120,"elapsed":17,"user":{"displayName":"Jai Kushwaha","userId":"01340704951735287618"}},"outputId":"db5b3b2a-9175-4c20-95a1-2ada0d4bbbcc"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(97148, 22)"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["df.columns"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ny4VcSCTIVq4","executionInfo":{"status":"ok","timestamp":1752298604002,"user_tz":-120,"elapsed":16,"user":{"displayName":"Jai Kushwaha","userId":"01340704951735287618"}},"outputId":"ce4ea5cc-0e78-4b9e-ab1a-07612fbdba1e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['AIRLINE', 'FL_NUMBER', 'DEST', 'DEP_DELAY', 'TAXI_OUT', 'TAXI_IN',\n","       'CRS_ELAPSED_TIME', 'DELAY_DUE_CARRIER', 'FLIGHT_STATUS',\n","       'CRS_DEP_TIME_MINUTES', 'CRS_DEP_TIME_MINUTES_SIN',\n","       'CRS_DEP_TIME_MINUTES_COS', 'CRS_ARR_TIME_MINUTES',\n","       'CRS_ARR_TIME_MINUTES_SIN', 'CRS_ARR_TIME_MINUTES_COS',\n","       'ARR_TIME_MINUTES', 'DEST_CITY_CITY', 'DEST_CITY_STATE',\n","       'ORIGIN_CITY_CITY', 'ORIGIN_CITY_STATE', 'DAY_OF_WEEK', 'IS_WEEKEND'],\n","      dtype='object')"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["df.info()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yyNdwQ86Itik","executionInfo":{"status":"ok","timestamp":1752298613210,"user_tz":-120,"elapsed":323,"user":{"displayName":"Jai Kushwaha","userId":"01340704951735287618"}},"outputId":"7cec5b02-03e9-4444-a5fc-4428e679dc91"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 97148 entries, 0 to 97147\n","Data columns (total 22 columns):\n"," #   Column                    Non-Null Count  Dtype  \n","---  ------                    --------------  -----  \n"," 0   AIRLINE                   97148 non-null  object \n"," 1   FL_NUMBER                 97148 non-null  int64  \n"," 2   DEST                      97148 non-null  object \n"," 3   DEP_DELAY                 97148 non-null  float64\n"," 4   TAXI_OUT                  97148 non-null  float64\n"," 5   TAXI_IN                   97148 non-null  float64\n"," 6   CRS_ELAPSED_TIME          97148 non-null  float64\n"," 7   DELAY_DUE_CARRIER         97148 non-null  float64\n"," 8   FLIGHT_STATUS             97148 non-null  int64  \n"," 9   CRS_DEP_TIME_MINUTES      97148 non-null  int64  \n"," 10  CRS_DEP_TIME_MINUTES_SIN  97148 non-null  float64\n"," 11  CRS_DEP_TIME_MINUTES_COS  97148 non-null  float64\n"," 12  CRS_ARR_TIME_MINUTES      97148 non-null  int64  \n"," 13  CRS_ARR_TIME_MINUTES_SIN  97148 non-null  float64\n"," 14  CRS_ARR_TIME_MINUTES_COS  97148 non-null  float64\n"," 15  ARR_TIME_MINUTES          97148 non-null  float64\n"," 16  DEST_CITY_CITY            97148 non-null  object \n"," 17  DEST_CITY_STATE           97148 non-null  object \n"," 18  ORIGIN_CITY_CITY          97148 non-null  object \n"," 19  ORIGIN_CITY_STATE         97148 non-null  object \n"," 20  DAY_OF_WEEK               97148 non-null  int64  \n"," 21  IS_WEEKEND                97148 non-null  int64  \n","dtypes: float64(10), int64(6), object(6)\n","memory usage: 16.3+ MB\n"]}]},{"cell_type":"code","source":["# Test script to verify category_encoders import\n","try:\n","    import category_encoders as ce\n","    print(\"✓ category_encoders imported successfully\")\n","    print(f\"Available encoders: {[attr for attr in dir(ce) if 'Encoder' in attr]}\")\n","\n","    # Test WOE Encoder specifically\n","    woe_encoder = ce.WOEEncoder()\n","    print(\"✓ WOEEncoder created successfully\")\n","\n","except ImportError as e:\n","    print(f\"✗ Import error: {e}\")\n","    print(\"Installing category_encoders...\")\n","    import subprocess\n","    subprocess.check_call(['pip', 'install', 'category_encoders'])\n","\n","    # Try importing again\n","    import category_encoders as ce\n","    print(\"✓ category_encoders installed and imported successfully\")\n","\n","except Exception as e:\n","    print(f\"✗ Unexpected error: {e}\")\n","\n","# Test other required imports\n","required_imports = [\n","    ('pandas', 'pd'),\n","    ('numpy', 'np'),\n","    ('sklearn.preprocessing', 'LabelEncoder'),\n","    ('sklearn.preprocessing', 'OneHotEncoder'),\n","    ('sklearn.preprocessing', 'OrdinalEncoder'),\n","]\n","\n","for module, item in required_imports:\n","    try:\n","        exec(f\"from {module} import {item}\")\n","        print(f\"✓ {module}.{item} imported successfully\")\n","    except ImportError as e:\n","        print(f\"✗ Failed to import {module}.{item}: {e}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HXnnRZQTaQYv","executionInfo":{"status":"ok","timestamp":1752300335023,"user_tz":-120,"elapsed":803,"user":{"displayName":"Jai Kushwaha","userId":"01340704951735287618"}},"outputId":"833d223e-f4dd-497e-f07e-614b28a11ba9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["✓ category_encoders imported successfully\n","Available encoders: ['BackwardDifferenceEncoder', 'BaseNEncoder', 'BinaryEncoder', 'CatBoostEncoder', 'CountEncoder', 'GLMMEncoder', 'GrayEncoder', 'HashingEncoder', 'HelmertEncoder', 'JamesSteinEncoder', 'LeaveOneOutEncoder', 'MEstimateEncoder', 'OneHotEncoder', 'OrdinalEncoder', 'PolynomialEncoder', 'QuantileEncoder', 'RankHotEncoder', 'SumEncoder', 'SummaryEncoder', 'TargetEncoder', 'WOEEncoder']\n","✓ WOEEncoder created successfully\n","✗ Failed to import pandas.pd: cannot import name 'pd' from 'pandas' (/usr/local/lib/python3.11/dist-packages/pandas/__init__.py)\n","✗ Failed to import numpy.np: cannot import name 'np' from 'numpy' (/usr/local/lib/python3.11/dist-packages/numpy/__init__.py)\n","✓ sklearn.preprocessing.LabelEncoder imported successfully\n","✓ sklearn.preprocessing.OneHotEncoder imported successfully\n","✓ sklearn.preprocessing.OrdinalEncoder imported successfully\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Js1XNHSDee7u","executionInfo":{"status":"ok","timestamp":1752321625029,"user_tz":-120,"elapsed":41,"user":{"displayName":"Jai Kushwaha","userId":"01340704951735287618"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["pd.options.display.max_columns=100"],"metadata":{"id":"-NoU10PDSGX-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Testing\n"],"metadata":{"id":"_mPOMfeuSrCf"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import os\n","import logging\n","import tensorflow as tf\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","import importlib\n","\n","# Import your custom scripts\n","from scripts.data_loading import DataDownload\n","from scripts.data_preprocessor import DataPreprocessor\n","from scripts.model_explainability import ModelExplainer\n","from scripts.data_evaluate import ModelEvaluator # To load scaler and model\n","\n","# Set random seed for reproducibility\n","np.random.seed(42)\n","tf.random.set_seed(42)\n","\n","# --- Configure Logging ---\n","logger = logging.getLogger(__name__)\n","logger.setLevel(logging.INFO)\n","if not logger.handlers:\n","    console_handler = logging.StreamHandler()\n","    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n","    console_handler.setFormatter(formatter)\n","    logger.addHandler(console_handler)\n","\n","# --- Define Paths ---\n","raw_data_dir = \"data/raw\"\n","processed_data_dir = \"data/processed\" # Ensure this exists if you need to save preprocessed data\n","models_dir = \"models\"\n","explainability_plots_dir = \"plots/explainability\"\n","\n","nn_model_filename = \"neural_network_classification_model\"\n","nn_model_path = os.path.join(models_dir, nn_model_filename)\n","scaler_filename = \"feature_scaler_classification\" # Name of your saved scaler\n","scaler_path = os.path.join(models_dir, f\"{scaler_filename}.pkl\") # Assuming .pkl extension for scaler\n","\n","# Ensure directories exist\n","os.makedirs(raw_data_dir, exist_ok=True)\n","os.makedirs(processed_data_dir, exist_ok=True)\n","os.makedirs(models_dir, exist_ok=True)\n","os.makedirs(explainability_plots_dir, exist_ok=True)\n","\n","\n","\n","logger.info(\"--- Starting SHAP Explainability Test for Neural Network with Flights Data ---\")\n","\n","# Initialize necessary components from your pipeline\n","downloader = DataDownload(dataset_name=\"patrickzel/flight-delay-and-cancellation-data-2019-2023-v2\",\n","                          download_path=raw_data_dir)\n","preprocessor = DataPreprocessor()\n","evaluator = ModelEvaluator(output_dir=models_dir, plots_dir=\"plots/model_evaluation\") # Need evaluator to load model/scaler\n","explainer = ModelExplainer(output_dir=explainability_plots_dir)\n","\n","# --- 1. Data Download & Initial Filtering (mimic pipeline.py) ---\n","logger.info(\"\\n--- Step 1: Data Download ---\")\n","try:\n","    downloader.data_download()\n","    csv_file_path = os.path.join(raw_data_dir, 'flights_sample_10k.csv')\n","    df_raw = pd.read_csv(csv_file_path)\n","    df_filtered_cancelled = df_raw[df_raw['CANCELLED'] == 0].copy()\n","    logger.info(f\"Loaded and filtered flights data. Shape: {df_filtered_cancelled.shape}\")\n","except Exception as e:\n","    logger.critical(f\"Error during data loading: {e}\", exc_info=True)\n","    exit(1)\n","\n","# --- 2. Data Preprocessing for Modeling (mimic pipeline.py) ---\n","logger.info(\"\\n--- Step 2: Data Preprocessing for Modeling ---\")\n","df_model = df_filtered_cancelled.copy()\n","\n","try:\n","    # --- Convert to Classification Target ---\n","    df_model['FLIGHT_STATUS'] = (df_model['ARR_DELAY'] > 15).astype(int)\n","\n","    delay_cols = ['DELAY_DUE_CARRIER', 'DELAY_DUE_WEATHER',\n","                  'DELAY_DUE_NAS', 'DELAY_DUE_SECURITY',\n","                  'DELAY_DUE_LATE_AIRCRAFT']\n","    df_model = preprocessor.handle_missing_values(df_model, delay_cols)\n","\n","    df_model = preprocessor.create_elapsed_time_diff(df_model)\n","\n","    time_columns = ['CRS_DEP_TIME', 'CRS_ARR_TIME', 'WHEELS_OFF', 'WHEELS_ON', 'DEP_TIME', 'ARR_TIME']\n","    df_model = preprocessor.apply_cyclical_encoding(df_model, time_columns)\n","\n","    city_state_columns = ['DEST_CITY', 'ORIGIN_CITY']\n","    df_model = preprocessor.split_city_state(df_model, city_state_columns)\n","\n","    date_columns = ['FL_DATE']\n","    df_model = preprocessor.add_weekday_weekend_columns(df_model, date_columns)\n","\n","    columns_to_exclude_model = [\n","        'AIRLINE_DOT', 'AIRLINE_CODE', 'DOT_CODE',\n","        'CANCELLED', 'DIVERTED', 'CANCELLATION_CODE',\n","        'FLIGHT_STATUS_EDA', # Drop EDA specific column from pipeline's EDA step\n","        'CRS_DEP_TIME', 'CRS_ARR_TIME', 'WHEELS_OFF', 'WHEELS_ON', 'DEP_TIME', 'ARR_TIME',\n","        'FL_DATE', 'ORIGIN', 'DEST',\n","        'DELAY_DUE_CARRIER', 'DELAY_DUE_WEATHER', 'DELAY_DUE_NAS',\n","        'DELAY_DUE_SECURITY', 'DELAY_DUE_LATE_AIRCRAFT',\n","        'ARR_DELAY'\n","    ]\n","    df_model = preprocessor.exclude_columns(df_model, columns_to_exclude_model)\n","\n","    df_model = preprocessor.encode_categorical_features(df_model)\n","\n","    # High Correlation Feature Removal (re-evaluate this with your actual features if necessary)\n","    df_for_corr_check = df_model.drop(columns=['FLIGHT_STATUS'], errors='ignore')\n","    columns_to_drop_high_corr, _ = preprocessor.identify_high_correlation_pairs(df_for_corr_check, threshold=0.9)\n","    if columns_to_drop_high_corr:\n","        df_model = df_model.drop(columns=list(columns_to_drop_high_corr), errors='ignore')\n","        logger.info(f\"Removed {len(columns_to_drop_high_corr)} columns due to high correlation during preprocessing for modeling.\")\n","\n","\n","    logger.info(f\"Preprocessing for modeling complete. Final DataFrame shape: {df_model.shape}\")\n","    # Make sure all columns that were available during training are present,\n","    # and any new columns are handled (e.g., if one-hot encoding creates more)\n","    # It's crucial that X.columns match exactly what the model was trained on.\n","\n","except Exception as e:\n","    logger.critical(f\"Critical error during data preprocessing for modeling: {e}\", exc_info=True)\n","    exit(1)\n","\n","# --- 3. Prepare Data for Model (mimic pipeline.py train/test split and scaling) ---\n","logger.info(\"\\n--- Step 3: Preparing Data for Model ---\")\n","if 'FLIGHT_STATUS' not in df_model.columns:\n","    logger.critical(\"Target column 'FLIGHT_STATUS' not found. Cannot proceed.\")\n","    exit(1)\n","\n","X = df_model.drop(columns=['FLIGHT_STATUS'])\n","y = df_model['FLIGHT_STATUS']\n","\n","# Final NaN drop from X (as done in pipeline)\n","X = X.dropna(axis=1, how='all')\n","rows_before_final_nan_drop = X.shape[0]\n","X = X.dropna()\n","y = y[X.index] # Align y with X\n","if X.shape[0] < rows_before_final_nan_drop:\n","    logger.warning(f\"Removed {rows_before_final_nan_drop - X.shape[0]} rows due to NaN values in features during final data prep.\")\n","\n","# Train-Test Split (use the same random_state and split ratio as in pipeline.py)\n","# We only need X_test for explainability\n","_, X_test, _, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n","logger.info(f\"Test set shape before scaling: X_test {X_test.shape}, y_test {y_test.shape}\")\n","\n","# Load the trained scaler\n","scaler = evaluator.load_model(scaler_filename)\n","if scaler is None:\n","    logger.critical(f\"Scaler '{scaler_filename}' not found at {scaler_path}. Please run pipeline.py first to train and save the scaler.\")\n","    exit(1)\n","\n","# Identify numerical columns that were scaled during pipeline run\n","numerical_features_to_scale = [col for col in X_test.select_dtypes(include=np.number).columns if not (X_test[col].dropna().isin([0, 1]).all() or col.endswith('_SIN') or col.endswith('_COS'))]\n","\n","# Apply scaling to the test set\n","X_test_scaled = X_test.copy()\n","if numerical_features_to_scale:\n","    X_test_scaled[numerical_features_to_scale] = scaler.transform(X_test_scaled[numerical_features_to_scale])\n","    logger.info(f\"Applied StandardScaler to {len(numerical_features_to_scale)} numerical features in X_test.\")\n","else:\n","    logger.info(\"No numerical features to scale in X_test or already handled.\")\n","\n","# Crucially, ensure X_test_scaled is a DataFrame with original column names\n","X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n","logger.info(f\"Scaled X_test_df shape: {X_test_scaled_df.shape}\")\n","\n","\n","# --- 4. Load the Neural Network Model ---\n","logger.info(\"\\n--- Step 4: Loading Neural Network Model ---\")\n","model = evaluator.load_model(nn_model_filename)\n","if model is None:\n","    logger.critical(f\"Neural Network model '{nn_model_filename}' not found at {nn_model_path}. Please run pipeline.py first to train and save the model.\")\n","    exit(1)\n","logger.info(\"Neural Network model loaded successfully.\")\n","\n","# --- 5. Generate SHAP Explainability Plots ---\n","logger.info(\"\\n--- Step 5: Generating SHAP Plots ---\")\n","try:\n","    # Use a sample of the test data for SHAP to speed up calculation for DeepExplainer\n","    # DeepExplainer is recommended for Keras models and can handle more samples than KernelExplainer\n","    # without needing a separate background dataset for its initialization.\n","    # However, for very large test sets, a sample is still good.\n","    X_explain_sample = X_test_scaled_df.sample(n=min(1000, X_test_scaled_df.shape[0]), random_state=42)\n","    logger.info(f\"Using {X_explain_sample.shape[0]} samples from scaled test data for SHAP explainability.\")\n","\n","    task_type = 'classification' # Flight delay is a binary classification\n","\n","    # SHAP Summary Plot\n","    explainer.plot_shap_summary(\n","        model=model,\n","        X=X_explain_sample,\n","        filename=f\"{nn_model_filename.replace('.h5', '')}_shap_summary_plot.png\",\n","        task_type=task_type\n","    )\n","\n","    # SHAP Dependence Plots for a few features\n","    # It's best to identify truly impactful features from the summary plot after its run.\n","    # For demonstration, let's pick a few features that are often relevant.\n","    features_for_dependence = []\n","    if 'CRS_ELAPSED_TIME' in X_explain_sample.columns: features_for_dependence.append('CRS_ELAPSED_TIME')\n","    if 'DISTANCE' in X_explain_sample.columns: features_for_dependence.append('DISTANCE')\n","    if 'AIRLINE_WN' in X_explain_sample.columns: features_for_dependence.append('AIRLINE_WN') # Example airline\n","    if 'ORIGIN_STATE_CA' in X_explain_sample.columns: features_for_dependence.append('ORIGIN_STATE_CA') # Example state\n","    if 'DAY_OF_WEEK_is_weekend' in X_explain_sample.columns: features_for_dependence.append('DAY_OF_WEEK_is_weekend')\n","    if 'CRS_DEP_TIME_SIN' in X_explain_sample.columns: features_for_dependence.append('CRS_DEP_TIME_SIN')\n","\n","\n","    if features_for_dependence:\n","        for feature in features_for_dependence:\n","            explainer.plot_shap_dependence(\n","                model=model,\n","                X=X_explain_sample,\n","                feature=feature,\n","                filename=f\"{nn_model_filename.replace('.h5', '')}_shap_dependence_plot_{feature}.png\",\n","                task_type=task_type\n","            )\n","    else:\n","        logger.warning(\"No default features identified for SHAP dependence plots. Consider adding relevant features.\")\n","\n","\n","except Exception as e:\n","    logger.error(f\"An error occurred during SHAP explainability: {e}\", exc_info=True)\n","\n","logger.info(\"--- SHAP Explainability Test Complete ---\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"mq3fOr6_v59G","executionInfo":{"status":"ok","timestamp":1752410793816,"user_tz":-120,"elapsed":16217,"user":{"displayName":"Jai Kushwaha","userId":"01340704951735287618"}},"outputId":"5bff02a0-374c-4e54-cbd1-703b7e604a32"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO: --- Starting SHAP Explainability Test for Neural Network with Flights Data ---\n","INFO:__main__:--- Starting SHAP Explainability Test for Neural Network with Flights Data ---\n","2025-07-13 12:46:20,183 - scripts.data_loading - INFO - DataDownload initialized. Running in Colab: False\n","INFO:scripts.data_loading:DataDownload initialized. Running in Colab: False\n","2025-07-13 12:46:20,195 - scripts.data_preprocessor - INFO - DataPreprocessor initialized.\n","INFO:scripts.data_preprocessor:DataPreprocessor initialized.\n","2025-07-13 12:46:20,197 - scripts.data_evaluate - INFO - ModelEvaluator initialized. Models will be saved to 'models'.\n","INFO:scripts.data_evaluate:ModelEvaluator initialized. Models will be saved to 'models'.\n","2025-07-13 12:46:20,217 - scripts.data_evaluate - INFO - Evaluation plots will be saved to 'plots/model_evaluation'.\n","INFO:scripts.data_evaluate:Evaluation plots will be saved to 'plots/model_evaluation'.\n","2025-07-13 12:46:20,252 - scripts.model_explainability - INFO - ModelExplainer initialized. Explainability plots will be saved to 'plots/explainability'.\n","INFO:scripts.model_explainability:ModelExplainer initialized. Explainability plots will be saved to 'plots/explainability'.\n","INFO: \n","--- Step 1: Data Download ---\n","INFO:__main__:\n","--- Step 1: Data Download ---\n","2025-07-13 12:46:20,255 - scripts.data_loading - INFO - Downloading dataset 'patrickzel/flight-delay-and-cancellation-data-2019-2023-v2' to 'data/raw'...\n","INFO:scripts.data_loading:Downloading dataset 'patrickzel/flight-delay-and-cancellation-data-2019-2023-v2' to 'data/raw'...\n","2025-07-13 12:46:21,518 - scripts.data_loading - INFO - Dataset downloaded and unzipped successfully!\n","INFO:scripts.data_loading:Dataset downloaded and unzipped successfully!\n","2025-07-13 12:46:21,520 - scripts.data_loading - ERROR - Could not find the unzipped dataset directory containing 'flights_sample_100k.csv' in data/raw.\n","ERROR:scripts.data_loading:Could not find the unzipped dataset directory containing 'flights_sample_100k.csv' in data/raw.\n","INFO: Loaded and filtered flights data. Shape: (9758, 32)\n","INFO:__main__:Loaded and filtered flights data. Shape: (9758, 32)\n","INFO: \n","--- Step 2: Data Preprocessing for Modeling ---\n","INFO:__main__:\n","--- Step 2: Data Preprocessing for Modeling ---\n","2025-07-13 12:46:21,579 - scripts.data_preprocessor - INFO - Handling missing values...\n","INFO:scripts.data_preprocessor:Handling missing values...\n","2025-07-13 12:46:21,585 - scripts.data_preprocessor - INFO - Dropped 19 rows due to NaN values in 'ARR_DELAY'.\n","INFO:scripts.data_preprocessor:Dropped 19 rows due to NaN values in 'ARR_DELAY'.\n","2025-07-13 12:46:21,585 - scripts.data_preprocessor - INFO - DataFrame shape after handling missing values: (9739, 33)\n","INFO:scripts.data_preprocessor:DataFrame shape after handling missing values: (9739, 33)\n","2025-07-13 12:46:21,586 - scripts.data_preprocessor - INFO - Creating 'ELAPSED_TIME_DIFF' feature...\n","INFO:scripts.data_preprocessor:Creating 'ELAPSED_TIME_DIFF' feature...\n","2025-07-13 12:46:21,587 - scripts.data_preprocessor - WARNING - Required time column 'ACTUAL_ELAPSED_TIME' not found. Cannot create 'ELAPSED_TIME_DIFF'.\n","WARNING:scripts.data_preprocessor:Required time column 'ACTUAL_ELAPSED_TIME' not found. Cannot create 'ELAPSED_TIME_DIFF'.\n","2025-07-13 12:46:21,588 - scripts.data_preprocessor - INFO - Applying cyclical encoding to time columns...\n","INFO:scripts.data_preprocessor:Applying cyclical encoding to time columns...\n","2025-07-13 12:46:21,605 - scripts.data_preprocessor - INFO - Splitting city and state columns...\n","INFO:scripts.data_preprocessor:Splitting city and state columns...\n","2025-07-13 12:46:21,644 - scripts.data_preprocessor - INFO - Adding weekday and weekend columns...\n","INFO:scripts.data_preprocessor:Adding weekday and weekend columns...\n","2025-07-13 12:46:21,650 - scripts.data_preprocessor - INFO - Excluding specified columns...\n","INFO:scripts.data_preprocessor:Excluding specified columns...\n","2025-07-13 12:46:21,654 - scripts.data_preprocessor - INFO - Excluded columns: ['AIRLINE_DOT', 'AIRLINE_CODE', 'DOT_CODE', 'CANCELLED', 'DIVERTED', 'CANCELLATION_CODE', 'CRS_DEP_TIME', 'CRS_ARR_TIME', 'WHEELS_OFF', 'WHEELS_ON', 'DEP_TIME', 'ARR_TIME', 'FL_DATE', 'ORIGIN', 'DEST', 'DELAY_DUE_CARRIER', 'DELAY_DUE_WEATHER', 'DELAY_DUE_NAS', 'DELAY_DUE_SECURITY', 'DELAY_DUE_LATE_AIRCRAFT', 'ARR_DELAY']\n","INFO:scripts.data_preprocessor:Excluded columns: ['AIRLINE_DOT', 'AIRLINE_CODE', 'DOT_CODE', 'CANCELLED', 'DIVERTED', 'CANCELLATION_CODE', 'CRS_DEP_TIME', 'CRS_ARR_TIME', 'WHEELS_OFF', 'WHEELS_ON', 'DEP_TIME', 'ARR_TIME', 'FL_DATE', 'ORIGIN', 'DEST', 'DELAY_DUE_CARRIER', 'DELAY_DUE_WEATHER', 'DELAY_DUE_NAS', 'DELAY_DUE_SECURITY', 'DELAY_DUE_LATE_AIRCRAFT', 'ARR_DELAY']\n","2025-07-13 12:46:21,658 - scripts.data_preprocessor - INFO - Applying WOE encoding to all categorical features based on FLIGHT_STATUS.\n","INFO:scripts.data_preprocessor:Applying WOE encoding to all categorical features based on FLIGHT_STATUS.\n","2025-07-13 12:46:21,660 - scripts.data_preprocessor - INFO - Found 5 categorical columns for WOE encoding: ['ORIGIN_CITY_CITY', 'DEST_CITY_STATE', 'ORIGIN_CITY_STATE', 'DEST_CITY_CITY', 'AIRLINE']\n","INFO:scripts.data_preprocessor:Found 5 categorical columns for WOE encoding: ['ORIGIN_CITY_CITY', 'DEST_CITY_STATE', 'ORIGIN_CITY_STATE', 'DEST_CITY_CITY', 'AIRLINE']\n","2025-07-13 12:46:21,744 - scripts.data_preprocessor - INFO - Successfully applied WOE encoding to 5 categorical columns.\n","INFO:scripts.data_preprocessor:Successfully applied WOE encoding to 5 categorical columns.\n","2025-07-13 12:46:21,745 - scripts.data_preprocessor - INFO - Created WOE encoded columns: ['ORIGIN_CITY_CITY_WOE', 'DEST_CITY_STATE_WOE', 'ORIGIN_CITY_STATE_WOE', 'DEST_CITY_CITY_WOE', 'AIRLINE_WOE']\n","INFO:scripts.data_preprocessor:Created WOE encoded columns: ['ORIGIN_CITY_CITY_WOE', 'DEST_CITY_STATE_WOE', 'ORIGIN_CITY_STATE_WOE', 'DEST_CITY_CITY_WOE', 'AIRLINE_WOE']\n","2025-07-13 12:46:21,748 - scripts.data_preprocessor - INFO - Dropped original categorical columns: ['ORIGIN_CITY_CITY', 'DEST_CITY_STATE', 'ORIGIN_CITY_STATE', 'DEST_CITY_CITY', 'AIRLINE']\n","INFO:scripts.data_preprocessor:Dropped original categorical columns: ['ORIGIN_CITY_CITY', 'DEST_CITY_STATE', 'ORIGIN_CITY_STATE', 'DEST_CITY_CITY', 'AIRLINE']\n","2025-07-13 12:46:21,751 - scripts.data_preprocessor - INFO - Identifying high correlation features (threshold=0.9)...\n","INFO:scripts.data_preprocessor:Identifying high correlation features (threshold=0.9)...\n","2025-07-13 12:46:21,787 - scripts.data_preprocessor - INFO - Found 12 columns to drop due to high correlation.\n","INFO:scripts.data_preprocessor:Found 12 columns to drop due to high correlation.\n","INFO: Removed 12 columns due to high correlation during preprocessing for modeling.\n","INFO:__main__:Removed 12 columns due to high correlation during preprocessing for modeling.\n","INFO: Preprocessing for modeling complete. Final DataFrame shape: (9739, 20)\n","INFO:__main__:Preprocessing for modeling complete. Final DataFrame shape: (9739, 20)\n","INFO: \n","--- Step 3: Preparing Data for Model ---\n","INFO:__main__:\n","--- Step 3: Preparing Data for Model ---\n","INFO: Test set shape before scaling: X_test (1461, 19), y_test (1461,)\n","INFO:__main__:Test set shape before scaling: X_test (1461, 19), y_test (1461,)\n","2025-07-13 12:46:21,806 - scripts.data_evaluate - INFO - Scikit-learn model 'feature_scaler_classification' loaded successfully from models/feature_scaler_classification.joblib\n","INFO:scripts.data_evaluate:Scikit-learn model 'feature_scaler_classification' loaded successfully from models/feature_scaler_classification.joblib\n","INFO: Applied StandardScaler to 14 numerical features in X_test.\n","INFO:__main__:Applied StandardScaler to 14 numerical features in X_test.\n","INFO: Scaled X_test_df shape: (1461, 19)\n","INFO:__main__:Scaled X_test_df shape: (1461, 19)\n","INFO: \n","--- Step 4: Loading Neural Network Model ---\n","INFO:__main__:\n","--- Step 4: Loading Neural Network Model ---\n","WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","2025-07-13 12:46:21,886 - scripts.data_evaluate - INFO - Keras model 'neural_network_classification_model' loaded successfully from models/neural_network_classification_model.h5\n","INFO:scripts.data_evaluate:Keras model 'neural_network_classification_model' loaded successfully from models/neural_network_classification_model.h5\n","INFO: Neural Network model loaded successfully.\n","INFO:__main__:Neural Network model loaded successfully.\n","INFO: \n","--- Step 5: Generating SHAP Plots ---\n","INFO:__main__:\n","--- Step 5: Generating SHAP Plots ---\n","INFO: Using 1000 samples from scaled test data for SHAP explainability.\n","INFO:__main__:Using 1000 samples from scaled test data for SHAP explainability.\n","/usr/local/lib/python3.11/dist-packages/shap/explainers/_deep/deep_tf.py:94: UserWarning: Your TensorFlow version is newer than 2.4.0 and so graph support has been removed in eager mode and some static graphs may not be supported. See PR #1483 for discussion.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.\n","Expected: input_layer\n","Received: inputs=['Tensor(shape=(1000, 19))']\n","  warnings.warn(msg)\n","/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.\n","Expected: input_layer\n","Received: inputs=['Tensor(shape=(2000, 19))']\n","  warnings.warn(msg)\n","/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.\n","Expected: input_layer\n","Received: inputs=['Tensor(shape=(1000, 19))']\n","  warnings.warn(msg)\n","2025-07-13 12:46:28,867 - scripts.model_explainability - INFO - Saved SHAP summary bar plot to plots/explainability/neural_network_classification_model_shap_summary_plot.png\n","INFO:scripts.model_explainability:Saved SHAP summary bar plot to plots/explainability/neural_network_classification_model_shap_summary_plot.png\n","2025-07-13 12:46:29,295 - scripts.model_explainability - INFO - Saved SHAP summary dot plot to plots/explainability/neural_network_classification_model_shap_summary_plot_dot.png\n","INFO:scripts.model_explainability:Saved SHAP summary dot plot to plots/explainability/neural_network_classification_model_shap_summary_plot_dot.png\n","/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.\n","Expected: input_layer\n","Received: inputs=['Tensor(shape=(2000, 19))']\n","  warnings.warn(msg)\n","/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.\n","Expected: input_layer\n","Received: inputs=['Tensor(shape=(1000, 19))']\n","  warnings.warn(msg)\n","2025-07-13 12:46:36,347 - scripts.model_explainability - INFO - Saved SHAP dependence plot for 'CRS_ELAPSED_TIME' to plots/explainability/neural_network_classification_model_shap_dependence_plot_CRS_ELAPSED_TIME.png\n","INFO:scripts.model_explainability:Saved SHAP dependence plot for 'CRS_ELAPSED_TIME' to plots/explainability/neural_network_classification_model_shap_dependence_plot_CRS_ELAPSED_TIME.png\n","INFO: --- SHAP Explainability Test Complete ---\n","INFO:__main__:--- SHAP Explainability Test Complete ---\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 1000x700 with 0 Axes>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1000x700 with 0 Axes>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1000x700 with 0 Axes>"]},"metadata":{}}]},{"cell_type":"code","source":["import shap\n","import tensorflow as tf\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","# Load the neural network model\n","model = tf.keras.models.load_model('models/neural_network_classification_model.h5')\n","\n","# Ensure X_test_scaled_df is a numpy array for SHAP\n","X_test_scaled = X_test_scaled_df.to_numpy()\n","\n","# Select a background dataset for SHAP (e.g., a subset of test data)\n","# Using a small subset (e.g., 100 samples) for computational efficiency\n","background = X_test_scaled[np.random.choice(X_test_scaled.shape[0], 100, replace=False)]\n","\n","# Initialize SHAP DeepExplainer\n","explainer = shap.DeepExplainer(model, background)\n","\n","# Compute SHAP values for the test set\n","shap_values = explainer.shap_values(X_test_scaled)\n","\n","# Since this is a classification model, shap_values is a list (one per class)\n","# For binary classification, we'll focus on the positive class (index 1)\n","# Adjust index based on your model's output (e.g., 0 for single-class output or multi-class)\n","shap_values_class = shap_values[1] if len(shap_values) > 1 else shap_values[0]\n","\n","# Create a SHAP summary plot\n","feature_names = X_test_scaled_df.columns.tolist()\n","shap.summary_plot(shap_values_class, X_test_scaled, feature_names=feature_names, show=False)\n","plt.title(\"SHAP Summary Plot for Neural Network Model\")\n","plt.tight_layout()\n","plt.savefig(\"shap_summary_plot.png\")\n","plt.close()\n","\n","# Create individual SHAP force plots for the first few test samples\n","for i in range(min(3, X_test_scaled.shape[0])):  # Plot for first 3 samples\n","    shap.force_plot(explainer.expected_value[1] if len(shap_values) > 1 else explainer.expected_value,\n","                    shap_values_class[i], X_test_scaled[i], feature_names=feature_names, matplotlib=True, show=False)\n","    plt.title(f\"SHAP Force Plot for Test Sample {i+1}\")\n","    plt.tight_layout()\n","    plt.savefig(f\"shap_force_plot_sample_{i+1}.png\")\n","    plt.close()\n","\n","# Print feature importance based on mean absolute SHAP values\n","mean_shap_values = np.abs(shap_values_class).mean(axis=0)\n","feature_importance = pd.DataFrame({\n","    'Feature': feature_names,\n","    'Mean_SHAP_Value': mean_shap_values\n","}).sort_values(by='Mean_SHAP_Value', ascending=False)\n","\n","print(\"\\nFeature Importance based on Mean Absolute SHAP Values:\")\n","print(feature_importance)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":630},"id":"cCdLvKJu3pHW","executionInfo":{"status":"error","timestamp":1752411667702,"user_tz":-120,"elapsed":2687,"user":{"displayName":"Jai Kushwaha","userId":"01340704951735287618"}},"outputId":"4a38d6e1-4aac-4781-ebdb-24068e0c35d2"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","/usr/local/lib/python3.11/dist-packages/shap/explainers/_deep/deep_tf.py:94: UserWarning: Your TensorFlow version is newer than 2.4.0 and so graph support has been removed in eager mode and some static graphs may not be supported. See PR #1483 for discussion.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.\n","Expected: input_layer\n","Received: inputs=['Tensor(shape=(100, 19))']\n","  warnings.warn(msg)\n","/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.\n","Expected: input_layer\n","Received: inputs=['Tensor(shape=(200, 19))']\n","  warnings.warn(msg)\n","/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.\n","Expected: input_layer\n","Received: inputs=['Tensor(shape=(1461, 19))']\n","  warnings.warn(msg)\n"]},{"output_type":"error","ename":"AssertionError","evalue":"The shape of the shap_values matrix does not match the shape of the provided data matrix.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-25-528363155.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Create a SHAP summary plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mfeature_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test_scaled_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mshap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshap_values_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SHAP Summary Plot for Neural Network Model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtight_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/shap/plots/_beeswarm.py\u001b[0m in \u001b[0;36msummary_legacy\u001b[0;34m(shap_values, features, feature_names, max_display, plot_type, color, axis_color, title, alpha, show, sort, color_bar, plot_size, layered_violin_max_num_bins, class_names, class_inds, color_bar_label, cmap, show_values_in_legend, use_log_scale, rng)\u001b[0m\n\u001b[1;32m    662\u001b[0m             )\n\u001b[1;32m    663\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0mnum_features\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape_msg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfeature_names\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAssertionError\u001b[0m: The shape of the shap_values matrix does not match the shape of the provided data matrix."]}]},{"cell_type":"code","source":["# X_test_scaled_df.shape, y_test.shape\n","X_test_scaled.columns\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O91f7pUGws1w","executionInfo":{"status":"ok","timestamp":1752411339230,"user_tz":-120,"elapsed":6,"user":{"displayName":"Jai Kushwaha","userId":"01340704951735287618"}},"outputId":"43a12869-46e7-425c-8a92-dbf30dbb2fb8"},"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['FL_NUMBER', 'DEP_DELAY', 'TAXI_OUT', 'TAXI_IN', 'CRS_ELAPSED_TIME',\n","       'CRS_DEP_TIME_MINUTES', 'CRS_DEP_TIME_MINUTES_SIN',\n","       'CRS_DEP_TIME_MINUTES_COS', 'CRS_ARR_TIME_MINUTES',\n","       'CRS_ARR_TIME_MINUTES_SIN', 'CRS_ARR_TIME_MINUTES_COS',\n","       'ARR_TIME_MINUTES', 'DAY_OF_WEEK', 'IS_WEEKEND', 'ORIGIN_CITY_CITY_WOE',\n","       'DEST_CITY_STATE_WOE', 'ORIGIN_CITY_STATE_WOE', 'DEST_CITY_CITY_WOE',\n","       'AIRLINE_WOE'],\n","      dtype='object')"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["y.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":241},"id":"ygA3aWqv20GE","executionInfo":{"status":"ok","timestamp":1752411528042,"user_tz":-120,"elapsed":17,"user":{"displayName":"Jai Kushwaha","userId":"01340704951735287618"}},"outputId":"ffe1f5c9-88cb-4688-9269-5105e7590187"},"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    0\n","1    0\n","2    0\n","3    0\n","4    0\n","Name: FLIGHT_STATUS, dtype: int64"],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>FLIGHT_STATUS</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div><br><label><b>dtype:</b> int64</label>"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["import importlib\n","import scripts.data_evaluate # Make sure this matches your file path\n","\n","# Reload the module to pick up the changes\n","importlib.reload(scripts.data_evaluate)\n","\n","# Now, if you're importing ModelEvaluator directly, do it again\n","from scripts.data_evaluate import ModelEvaluator\n","\n","# If you have an existing 'evaluator' object, you might need to re-instantiate it\n","# For example, if you had:\n","# evaluator = ModelEvaluator(output_dir=\"models\", plots_dir=\"plots/model_evaluation\")\n","# You would re-run that line after the reload."],"metadata":{"id":"6RWUUt6_gEf6","executionInfo":{"status":"ok","timestamp":1752321518087,"user_tz":-120,"elapsed":10,"user":{"displayName":"Jai Kushwaha","userId":"01340704951735287618"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["import importlib\n","import scripts.data_evaluate\n","\n","# Reload the module to pick up the changes\n","importlib.reload(scripts.data_evaluate)\n","\n","# Now re-import or re-access DataModeling\n","from scripts.data_evaluate import ModelEvaluator\n","\n","evaluator = ModelEvaluator()\n","evaluator"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kQAZcu5LSbfz","executionInfo":{"status":"ok","timestamp":1752320823586,"user_tz":-120,"elapsed":26,"user":{"displayName":"Jai Kushwaha","userId":"01340704951735287618"}},"outputId":"fab08511-931d-43f2-d606-a881dfcb0098"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["2025-07-12 11:47:05,053 - scripts.data_evaluate - INFO - ModelEvaluator initialized. Models will be saved to 'models'.\n","INFO:scripts.data_evaluate:ModelEvaluator initialized. Models will be saved to 'models'.\n","2025-07-12 11:47:05,059 - scripts.data_evaluate - INFO - Evaluation plots will be saved to 'plots/model_evaluation'.\n","INFO:scripts.data_evaluate:Evaluation plots will be saved to 'plots/model_evaluation'.\n"]}]},{"cell_type":"code","source":["from scripts.data_preprocessor import DataPreprocessor\n","df_1 = preprocessor.encode_categorical_features(df)\n","\n","df_1.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":642},"id":"MbQpRW-6etS9","executionInfo":{"status":"ok","timestamp":1752301274287,"user_tz":-120,"elapsed":83,"user":{"displayName":"Jai Kushwaha","userId":"01340704951735287618"}},"outputId":"3a8102fe-9f36-459d-fa29-f2ab2f26aca6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["2025-07-12 06:21:14,575 - scripts.data_preprocessor - INFO - Applying WOE encoding to all categorical features based on FLIGHT_STATUS.\n","INFO:scripts.data_preprocessor:Applying WOE encoding to all categorical features based on FLIGHT_STATUS.\n","2025-07-12 06:21:14,581 - scripts.data_preprocessor - INFO - Found 6 categorical columns for WOE encoding: ['ORIGIN_CITY_STATE', 'DEST', 'DEST_CITY_CITY', 'DEST_CITY_STATE', 'AIRLINE', 'ORIGIN_CITY_CITY']\n","INFO:scripts.data_preprocessor:Found 6 categorical columns for WOE encoding: ['ORIGIN_CITY_STATE', 'DEST', 'DEST_CITY_CITY', 'DEST_CITY_STATE', 'AIRLINE', 'ORIGIN_CITY_CITY']\n","2025-07-12 06:21:14,606 - scripts.data_preprocessor - ERROR - Error applying WOE encoding: name 'ce' is not defined\n","Traceback (most recent call last):\n","  File \"/content/drive/MyDrive/Colab Notebooks/flight_delay/scripts/data_preprocessor.py\", line 244, in encode_categorical_features\n","    #         df_copy[col] = df_copy[col].fillna('Missing')\n","            ^^\n","NameError: name 'ce' is not defined\n","ERROR:scripts.data_preprocessor:Error applying WOE encoding: name 'ce' is not defined\n","Traceback (most recent call last):\n","  File \"/content/drive/MyDrive/Colab Notebooks/flight_delay/scripts/data_preprocessor.py\", line 244, in encode_categorical_features\n","    #         df_copy[col] = df_copy[col].fillna('Missing')\n","            ^^\n","NameError: name 'ce' is not defined\n"]},{"output_type":"execute_result","data":{"text/plain":["                  AIRLINE  FL_NUMBER DEST  DEP_DELAY  TAXI_OUT  TAXI_IN  \\\n","0           Allegiant Air       1668  SPI      -10.0       9.0      7.0   \n","1       PSA Airlines Inc.       5560  CLT       -7.0      16.0     11.0   \n","2  Southwest Airlines Co.       1944  DEN       25.0      15.0      7.0   \n","3  Southwest Airlines Co.       3081  STL        0.0      12.0      6.0   \n","4    Delta Air Lines Inc.        674  SEA       -2.0      14.0     11.0   \n","\n","   CRS_ELAPSED_TIME  DELAY_DUE_CARRIER  FLIGHT_STATUS  CRS_DEP_TIME_MINUTES  \\\n","0             160.0                0.0              0                   390   \n","1              79.0                0.0              0                   385   \n","2              80.0               10.0              1                  1035   \n","3             105.0                0.0              0                   335   \n","4             178.0                0.0              0                   700   \n","\n","   CRS_DEP_TIME_MINUTES_SIN  CRS_DEP_TIME_MINUTES_COS  CRS_ARR_TIME_MINUTES  \\\n","0                  0.991445                 -0.130526                   490   \n","1                  0.994056                 -0.108867                   464   \n","2                 -0.980785                 -0.195090                  1115   \n","3                  0.994056                  0.108867                   380   \n","4                  0.087156                 -0.996195                   878   \n","\n","   CRS_ARR_TIME_MINUTES_SIN  CRS_ARR_TIME_MINUTES_COS  ARR_TIME_MINUTES  \\\n","0                  0.843391                 -0.537300             458.0   \n","1                  0.898794                 -0.438371             456.0   \n","2                 -0.988362                  0.152123            1131.0   \n","3                  0.996195                 -0.087156             375.0   \n","4                 -0.636078                 -0.771625             856.0   \n","\n","  DEST_CITY_CITY DEST_CITY_STATE            ORIGIN_CITY_CITY  \\\n","0    Springfield              IL                 Punta Gorda   \n","1      Charlotte              NC  New Bern/Morehead/Beaufort   \n","2         Denver              CO                 Albuquerque   \n","3      St. Louis              MO                  Pittsburgh   \n","4        Seattle              WA                 Los Angeles   \n","\n","  ORIGIN_CITY_STATE  DAY_OF_WEEK  IS_WEEKEND  \n","0                FL            4           0  \n","1                NC            1           0  \n","2                NM            2           0  \n","3                PA            6           1  \n","4                CA            6           1  "],"text/html":["\n","  <div id=\"df-33d5ce26-9a6e-487f-8ff2-ed9fb11ffb0c\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>AIRLINE</th>\n","      <th>FL_NUMBER</th>\n","      <th>DEST</th>\n","      <th>DEP_DELAY</th>\n","      <th>TAXI_OUT</th>\n","      <th>TAXI_IN</th>\n","      <th>CRS_ELAPSED_TIME</th>\n","      <th>DELAY_DUE_CARRIER</th>\n","      <th>FLIGHT_STATUS</th>\n","      <th>CRS_DEP_TIME_MINUTES</th>\n","      <th>CRS_DEP_TIME_MINUTES_SIN</th>\n","      <th>CRS_DEP_TIME_MINUTES_COS</th>\n","      <th>CRS_ARR_TIME_MINUTES</th>\n","      <th>CRS_ARR_TIME_MINUTES_SIN</th>\n","      <th>CRS_ARR_TIME_MINUTES_COS</th>\n","      <th>ARR_TIME_MINUTES</th>\n","      <th>DEST_CITY_CITY</th>\n","      <th>DEST_CITY_STATE</th>\n","      <th>ORIGIN_CITY_CITY</th>\n","      <th>ORIGIN_CITY_STATE</th>\n","      <th>DAY_OF_WEEK</th>\n","      <th>IS_WEEKEND</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Allegiant Air</td>\n","      <td>1668</td>\n","      <td>SPI</td>\n","      <td>-10.0</td>\n","      <td>9.0</td>\n","      <td>7.0</td>\n","      <td>160.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>390</td>\n","      <td>0.991445</td>\n","      <td>-0.130526</td>\n","      <td>490</td>\n","      <td>0.843391</td>\n","      <td>-0.537300</td>\n","      <td>458.0</td>\n","      <td>Springfield</td>\n","      <td>IL</td>\n","      <td>Punta Gorda</td>\n","      <td>FL</td>\n","      <td>4</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>PSA Airlines Inc.</td>\n","      <td>5560</td>\n","      <td>CLT</td>\n","      <td>-7.0</td>\n","      <td>16.0</td>\n","      <td>11.0</td>\n","      <td>79.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>385</td>\n","      <td>0.994056</td>\n","      <td>-0.108867</td>\n","      <td>464</td>\n","      <td>0.898794</td>\n","      <td>-0.438371</td>\n","      <td>456.0</td>\n","      <td>Charlotte</td>\n","      <td>NC</td>\n","      <td>New Bern/Morehead/Beaufort</td>\n","      <td>NC</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Southwest Airlines Co.</td>\n","      <td>1944</td>\n","      <td>DEN</td>\n","      <td>25.0</td>\n","      <td>15.0</td>\n","      <td>7.0</td>\n","      <td>80.0</td>\n","      <td>10.0</td>\n","      <td>1</td>\n","      <td>1035</td>\n","      <td>-0.980785</td>\n","      <td>-0.195090</td>\n","      <td>1115</td>\n","      <td>-0.988362</td>\n","      <td>0.152123</td>\n","      <td>1131.0</td>\n","      <td>Denver</td>\n","      <td>CO</td>\n","      <td>Albuquerque</td>\n","      <td>NM</td>\n","      <td>2</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Southwest Airlines Co.</td>\n","      <td>3081</td>\n","      <td>STL</td>\n","      <td>0.0</td>\n","      <td>12.0</td>\n","      <td>6.0</td>\n","      <td>105.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>335</td>\n","      <td>0.994056</td>\n","      <td>0.108867</td>\n","      <td>380</td>\n","      <td>0.996195</td>\n","      <td>-0.087156</td>\n","      <td>375.0</td>\n","      <td>St. Louis</td>\n","      <td>MO</td>\n","      <td>Pittsburgh</td>\n","      <td>PA</td>\n","      <td>6</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Delta Air Lines Inc.</td>\n","      <td>674</td>\n","      <td>SEA</td>\n","      <td>-2.0</td>\n","      <td>14.0</td>\n","      <td>11.0</td>\n","      <td>178.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>700</td>\n","      <td>0.087156</td>\n","      <td>-0.996195</td>\n","      <td>878</td>\n","      <td>-0.636078</td>\n","      <td>-0.771625</td>\n","      <td>856.0</td>\n","      <td>Seattle</td>\n","      <td>WA</td>\n","      <td>Los Angeles</td>\n","      <td>CA</td>\n","      <td>6</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-33d5ce26-9a6e-487f-8ff2-ed9fb11ffb0c')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-33d5ce26-9a6e-487f-8ff2-ed9fb11ffb0c button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-33d5ce26-9a6e-487f-8ff2-ed9fb11ffb0c');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-2adaba30-fd35-45e5-b00e-9fa95dca06ba\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2adaba30-fd35-45e5-b00e-9fa95dca06ba')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-2adaba30-fd35-45e5-b00e-9fa95dca06ba button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df_1"}},"metadata":{},"execution_count":30}]},{"cell_type":"markdown","source":["## Gradio APP\n"],"metadata":{"id":"j8jXnIdTfGHs"}},{"cell_type":"code","source":["import gradio as gr\n","import pandas as pd\n","import os\n","import sys # <--- ADDED THIS IMPORT\n","import logging\n","\n","# Configure Logging\n","logger = logging.getLogger(__name__)\n","logger.setLevel(logging.INFO)\n","# Remove handlers if already present to avoid duplication in case app.py is run multiple times\n","if logger.handlers:\n","    for handler in list(logger.handlers):\n","        logger.removeHandler(handler)\n","\n","console_handler = logging.StreamHandler()\n","console_handler.setLevel(logging.INFO)\n","console_formatter = logging.Formatter('%(levelname)s: %(message)s')\n","console_handler.setFormatter(console_formatter)\n","logger.addHandler(console_handler)\n","\n","\n","# Define paths (ensure these match where your main.py saves the outputs)\n","base_dir = '.' # Assuming app.py runs from the project root or adjust accordingly\n","if 'google.colab' in sys.modules: # Check if running in Colab context for paths\n","    base_dir = '/content/drive/MyDrive/Colab Notebooks/flight_delay'\n","\n","raw_data_dir = os.path.join(base_dir, \"data/raw\")\n","processed_data_dir = os.path.join(base_dir, \"data/processed\")\n","reports_dir = os.path.join(base_dir, \"reports\")\n","eda_plots_dir = os.path.join(base_dir, \"plots/eda\")\n","model_eval_plots_dir = os.path.join(base_dir, \"plots/model_evaluation\")\n","models_dir = os.path.join(base_dir, \"models\")\n","model_params_dir = os.path.join(base_dir, \"model_params\")\n","\n","\n","# --- Helper functions to load and display content ---\n","\n","def get_basic_data_info():\n","    \"\"\"Loads raw data and provides basic information and target variable definition.\"\"\"\n","    try:\n","        csv_file_path = os.path.join(raw_data_dir, 'flights_sample_10k.csv')\n","        df_raw = pd.read_csv(csv_file_path)\n","\n","        # Filter non-cancelled flights as in your main pipeline\n","        df_filtered_cancelled = df_raw[df_raw['CANCELLED'] == 0].copy()\n","\n","        info_text = \"### **1.2.1 Basic Data Info & Target Variable Definition**\\n\\n\"\n","        info_text += f\"**Raw Data Shape:** {df_raw.shape}\\n\\n\"\n","        info_text += f\"**Filtered (Non-Cancelled) Data Shape:** {df_filtered_cancelled.shape}\\n\\n\"\n","        info_text += \"**First 5 Rows of Filtered Data:**\\n\"\n","        info_text += f\"```\\n{df_filtered_cancelled.head().to_string()}\\n```\\n\\n\"\n","        info_text += \"**Target Variable (`FLIGHT_STATUS`) Definition:**\\n\"\n","        info_text += \"The target variable `FLIGHT_STATUS` is a binary classification target derived from `ARR_DELAY`.\\n\"\n","        info_text += \"  - `FLIGHT_STATUS = 1` if `ARR_DELAY > 15` minutes (flight is considered 'delayed').\\n\"\n","        info_text += \"  - `FLIGHT_STATUS = 0` if `ARR_DELAY <= 15` minutes (flight is considered 'on-time' or 'minor delay').\\n\\n\"\n","\n","        # Simulate target distribution if not explicitly saved\n","        if 'ARR_DELAY' in df_filtered_cancelled.columns:\n","            df_filtered_cancelled['FLIGHT_STATUS'] = (df_filtered_cancelled['ARR_DELAY'] > 15).astype(int)\n","            info_text += \"**Simulated Target Distribution (from filtered raw data):**\\n\"\n","            info_text += f\"```\\n{df_filtered_cancelled['FLIGHT_STATUS'].value_counts().to_string()}\\n```\"\n","        else:\n","            info_text += \"Note: 'ARR_DELAY' column not found to simulate target distribution in raw data view.\"\n","\n","        return info_text\n","    except FileNotFoundError:\n","        return f\"Error: Data file not found at {os.path.join(raw_data_dir, 'flights_sample_100k.csv')}. Please ensure the pipeline has run successfully.\"\n","    except Exception as e:\n","        return f\"An error occurred: {e}\"\n","\n","def get_data_profiling_link():\n","    \"\"\"Provides a link to the generated data profiling report.\"\"\"\n","    report_path = os.path.join(reports_dir, \"flight_data_profile_report_pre_processing.html\")\n","    if os.path.exists(report_path):\n","        return f\"### **1.3 Data Profiling Link**\\n\\n\" \\\n","               f\"A comprehensive data profiling report (generated using `pandas-profiling`) is available:\\n\" \\\n","               f\"- [Open Data Profile Report]({report_path})\\n\\n\" \\\n","               f\"**Note:** If running locally, click the link to open the HTML report in your browser. If in Colab, you might need to navigate to the file path in your Google Drive and open it from there.\"\n","    else:\n","        return f\"### **1.3 Data Profiling Link**\\n\\n\" \\\n","               f\"Data profiling report not found at: `{report_path}`. Please ensure the pipeline has generated it.\"\n","\n","def get_eda_plots():\n","    \"\"\"Returns a list of paths to EDA plots.\"\"\"\n","    plot_files = []\n","    # List all plots generated in your pipeline's Step 5\n","    plots = [\n","        \"all_column_distributions_eda.png\",\n","        \"airline_flight_counts_eda.png\",\n","        \"top_20_destination_visits_eda.png\",\n","        \"avg_arrival_delay_by_airline_eda.png\",\n","        \"total_delays_by_year_eda.png\",\n","        \"monthly_delays_by_year_eda.png\",\n","        \"monthly_delay_trend_highlight_eda.png\",\n","        \"delay_reason_breakdown_eda.png\"\n","    ]\n","\n","    found_plots_info = \"### **1.2.2 EDA Plots & Inferences**\\n\\n\"\n","    found_plots_info += \"Explore the visualizations below to understand key trends and patterns in the flight delay data.\\n\\n\"\n","\n","    for plot in plots:\n","        path = os.path.join(eda_plots_dir, plot)\n","        if os.path.exists(path):\n","            plot_files.append(path)\n","            found_plots_info += f\"- **{plot.replace('.png', '').replace('_', ' ').title()}**\\n\"\n","            # Add a brief inference for each plot based on common expectations from flight data EDA\n","            if \"column_distributions\" in plot:\n","                found_plots_info += \"  *Inference:* Provides an overview of feature distributions, helping to identify skewed data, outliers, or categorical balance issues.\\n\"\n","            elif \"airline_flight_counts\" in plot:\n","                found_plots_info += \"  *Inference:* Shows which airlines operate the most flights, indicating their market share and potential impact on overall delays.\\n\"\n","            elif \"destination_visits\" in plot:\n","                found_plots_info += \"  *Inference:* Highlights the busiest destination airports, which can be hotspots for delays due to high traffic.\\n\"\n","            elif \"arrival_delay_by_airline\" in plot:\n","                found_plots_info += \"  *Inference:* Compares average arrival delays across different airlines, revealing which airlines tend to be more punctual or delayed.\\n\"\n","            elif \"total_delays_by_year\" in plot:\n","                found_plots_info += \"  *Inference:* Illustrates the yearly trend of total flight delays, showing if delay incidents are increasing or decreasing over time.\\n\"\n","            elif \"monthly_delays_by_year\" in plot:\n","                found_plots_info += \"  *Inference:* Breaks down delays by month and year, revealing seasonal patterns and yearly variations.\\n\"\n","            elif \"monthly_delay_trend_highlight\" in plot:\n","                found_plots_info += \"  *Inference:* Focuses on specific months or periods with high delay occurrences, indicating peak travel times or anomaly periods.\\n\"\n","            elif \"delay_reason_breakdown\" in plot:\n","                found_plots_info += \"  *Inference:* Identifies the primary causes of delays (e.g., carrier, weather, NAS), which is crucial for targeted interventions.\\n\"\n","            found_plots_info += \"\\n\"\n","        else:\n","            logger.warning(f\"EDA plot not found: {path}\")\n","\n","    if not plot_files:\n","        return \"No EDA plots found. Please ensure the pipeline has generated them.\", []\n","\n","    return found_plots_info, plot_files\n","\n","\n","def get_preprocessing_summary():\n","    \"\"\"Summarizes the data cleaning and preprocessing steps.\"\"\"\n","    summary_text = \"### **1.4 Data Cleaning and Preprocessing Summary**\\n\\n\"\n","    summary_text += \"The raw flight data undergoes several crucial preprocessing steps to prepare it for machine learning:\\n\\n\"\n","    summary_text += \"1.  **Initial Filtering:** Cancelled flights (`CANCELLED == 1`) are removed as our target is arrival delay.\\n\"\n","    summary_text += \"2.  **Target Variable Creation:** A new binary target `FLIGHT_STATUS` is created from `ARR_DELAY` (1 if arrival delay > 15 mins, 0 otherwise).\\n\"\n","    summary_text += \"3.  **Missing Value Imputation:** Missing values in delay reason columns (`DELAY_DUE_CARRIER`, etc.) are filled, likely with zeros, indicating no delay attributed to that reason.\\n\"\n","    summary_text += \"4.  **Elapsed Time Difference:** A feature `ELAPSED_TIME_DIFF` is calculated from scheduled and actual times, providing insights into flight duration deviations.\\n\"\n","    summary_text += \"5.  **Cyclical Encoding:** Time-based features like `CRS_DEP_TIME`, `CRS_ARR_TIME` are transformed using sine and cosine functions to capture their cyclical nature.\\n\"\n","    summary_text += \"6.  **City/State Split:** Combined city, state information in `DEST_CITY` and `ORIGIN_CITY` is separated into distinct `_CITY` and `_STATE` columns for better granularity.\\n\"\n","    summary_text += \"7.  **Weekday/Weekend Features:** `FL_DATE` is used to create binary features indicating whether a flight occurs on a `WEEKDAY` or `WEEKEND`.\\n\"\n","    summary_text += \"8.  **Categorical Encoding:** Categorical features (e.g., `AIRLINE`, `ORIGIN`, `DEST`, `DEST_STATE`) are converted into numerical representations, typically using one-hot encoding or similar methods, suitable for machine learning models.\\n\"\n","    summary_text += \"9.  **High Correlation Feature Removal:** Features with a high correlation (e.g., >0.9) are identified and one of the pair is removed to prevent multicollinearity and improve model stability.\\n\"\n","    summary_text += \"10. **Feature Exclusion:** Various columns deemed irrelevant or redundant for modeling (e.g., original identifiers, cancellation details, raw time columns, original delay values) are dropped.\\n\\n\"\n","    summary_text += \"The preprocessed data is then saved as `preprocessed_flight_data_for_modeling.csv`.\"\n","    return summary_text\n","\n","def get_modeling_and_evaluation_results():\n","    \"\"\"Loads and displays model evaluation results and plots.\"\"\"\n","    results_md = \"### **2. Data Modeling**\\n\\n\"\n","    results_md += \"Our pipeline trains and evaluates several classification models for predicting flight delays:\\n\"\n","    results_md += \"- **Baseline Model:** Predicts the majority class (no delay).\\n\"\n","    results_md += \"- **Logistic Regression:** A simple linear model.\\n\"\n","    results_md += \"- **Neural Network:** A more complex deep learning model.\\n\\n\"\n","\n","    results_md += \"### **3. Data Evaluation and KPIs (Classification)**\\n\\n\"\n","    results_md += \"Models are evaluated using metrics relevant for classification tasks, including:\\n\"\n","    results_md += \"- **Accuracy:** Overall correctness.\\n\"\n","    results_md += \"- **Precision:** Proportion of true positive predictions that were actually positive.\\n\"\n","    results_md += \"- **Recall:** Proportion of actual positives that were correctly identified.\\n\"\n","    results_md += \"- **F1-Score:** Harmonic mean of precision and recall.\\n\"\n","    results_md += \"- **ROC AUC:** Area Under the Receiver Operating Characteristic Curve, indicating the model's ability to distinguish between classes.\\n\\n\"\n","\n","    metrics_table_path = os.path.join(model_eval_plots_dir, \"classification_model_metrics_table.csv\")\n","    metrics_display = \"\"\n","    if os.path.exists(metrics_table_path):\n","        try:\n","            df_metrics = pd.read_csv(metrics_table_path)\n","            metrics_display = \"**Overall Model Performance (Test Set):**\\n\"\n","            metrics_display += df_metrics.to_markdown(index=False) + \"\\n\\n\"\n","        except Exception as e:\n","            metrics_display = f\"Could not load metrics table: {e}\\n\\n\"\n","    else:\n","        metrics_display = f\"Metrics table not found at: `{metrics_table_path}`. Please ensure the pipeline has generated it.\\n\\n\"\n","\n","    results_md += metrics_display\n","\n","    plot_files = []\n","    # List all evaluation plots generated in your pipeline's Step 7\n","    eval_plots = [\n","      \"Logistic_Regression_Test_ROC_Curve.png\",\n","      \"Logistic_Regression_Test_Confusion_Matrix.png\",\n","      \"Neural_Network_Classification_Test_ROC_Curve.png\",\n","      \"Neural_Network_Classification_Test_Confusion_Matrix.png\"\n","      ]\n","\n","    found_plots_info = \"### **Model Evaluation Plots (Test Set):**\\n\\n\"\n","\n","    for plot in eval_plots:\n","        path = os.path.join(model_eval_plots_dir, plot)\n","        if os.path.exists(path):\n","            plot_files.append(path)\n","            found_plots_info += f\"- **{plot.replace('.png', '').replace('_', ' ').title()}**\\n\"\n","            if \"ROC_Curve\" in plot:\n","                found_plots_info += \"  *Inference:* Visualizes the trade-off between the true positive rate and false positive rate. A curve closer to the top-left corner indicates better performance.\\n\"\n","            elif \"Confusion_Matrix\" in plot:\n","                found_plots_info += \"  *Inference:* Shows the counts of true positives, true negatives, false positives, and false negatives, providing a clear picture of classification errors.\\n\"\n","            found_plots_info += \"\\n\"\n","        else:\n","            logger.warning(f\"Model evaluation plot not found: {path}\")\n","\n","    if not plot_files:\n","        found_plots_info = \"No model evaluation plots found. Please ensure the pipeline has generated them.\"\n","\n","    return results_md, found_plots_info, plot_files\n","\n","def get_shap_plots():\n","    \"\"\"Returns a list of paths to SHAP explanation plots.\"\"\"\n","    shap_plots_dir = os.path.join(base_dir, \"plots/model_explainability\") # Ensure this path is defined globally or passed\n","    os.makedirs(shap_plots_dir, exist_ok=True) # Ensure dir exists for the app too\n","\n","    plot_files = []\n","    shap_plots = [\n","        \"logistic_regression_shap_summary.png\",\n","        \"neural_network_shap_summary.png\",\n","        # Add other SHAP plots if you generate them (e.g., dependence plots)\n","    ]\n","\n","    found_plots_info = \"### **4.1 SHAP Feature Importance Plots**\\n\\n\"\n","    found_plots_info += \"These plots show the contribution of each feature to the model's predictions.\\n\\n\"\n","\n","    for plot in shap_plots:\n","        path = os.path.join(shap_plots_dir, plot)\n","        if os.path.exists(path):\n","            plot_files.append(path)\n","            found_plots_info += f\"- **{plot.replace('.png', '').replace('_', ' ').title()}**\\n\"\n","            found_plots_info += \"  *Inference:* Each point represents an instance in the dataset. The position on the x-axis shows the SHAP value, indicating the feature's impact on the prediction. Color often indicates the feature's actual value (e.g., high or low).\\n\\n\"\n","        else:\n","            logger.warning(f\"SHAP plot not found: {path}\")\n","\n","    if not plot_files:\n","        return \"No SHAP plots found. Please ensure the pipeline has generated them in `plots/model_explainability`.\", []\n","\n","    return found_plots_info, plot_files\n","\n","def get_model_explainability_info():\n","    \"\"\"Placeholder for Model Explainability.\"\"\"\n","    explain_md = \"### **4. Model Explainability**\\n\\n\"\n","    explain_md += \"Understanding why a model makes certain predictions is crucial, especially in high-stakes domains like aviation.\\n\"\n","    explain_md += \"This section will be dedicated to explaining model predictions using techniques such as:\\n\"\n","    explain_md += \"- **SHAP (SHapley Additive exPlanations):** To explain individual predictions and global feature importance by showing the contribution of each feature to the prediction.\\n\"\n","    explain_md += \"- **LIME (Local Interpretable Model-agnostic Explanations):** To explain the predictions of any machine learning model by approximating it with a local, interpretable model.\\n\\n\"\n","    explain_md += \"**Current Status:** This functionality is still under development. Once implemented, you will be able to select a model and input features to see how different factors influence the prediction of a flight delay.\\n\\n\"\n","    explain_md += \"*(Future enhancement: Interactive SHAP/LIME plots, feature importance rankings, and example predictions with explanations.)*\"\n","    return explain_md\n","\n","\n","# --- Gradio Interface ---\n","\n","with gr.Blocks(theme=gr.themes.Soft(), title=\"Flight Delay Prediction ML Pipeline Dashboard\") as demo:\n","    gr.Markdown(\n","        \"\"\"\n","        # ✈️ Flight Delay Prediction ML Pipeline Dashboard\n","        Welcome to the interactive dashboard for the Flight Delay Prediction Machine Learning project!\n","        This application guides you through the end-to-end ML pipeline, from data understanding and preprocessing\n","        to model training, evaluation, and future explainability.\n","        \"\"\"\n","    )\n","\n","    with gr.Tab(\"1. Data Overview & EDA\"):\n","        gr.Markdown(\"## Data Understanding and Exploratory Data Analysis (EDA)\")\n","        with gr.Accordion(\"1.1 Use Case & Pipeline Structure\", open=False):\n","            gr.Markdown(\n","                \"\"\"\n","                ### **1.1 Use Case Definition and Process Pipeline Structure**\n","                **Use Case:** Predict whether a flight will be significantly delayed (arrival delay > 15 minutes) or on-time/minor delay. This is a **binary classification** problem.\n","\n","                **Overall Pipeline Structure:**\n","                1.  **Data Download:** Retrieves raw flight data from Kaggle.\n","                2.  **Data Loading & Initial Filtering:** Loads data and removes cancelled flights.\n","                3.  **Data Profiling (Pre-preprocessing):** Generates an initial report on data quality and characteristics.\n","                4.  **Data Preprocessing for Visualization (EDA):** Prepares data for exploratory plots (e.g., creating temporary target for EDA).\n","                5.  **Data Visualization (EDA):** Generates insightful plots to understand data patterns.\n","                6.  **Data Preprocessing for Modeling:** Cleans, transforms, and engineers features for machine learning models.\n","                7.  **Data Modeling and Evaluation:** Trains and evaluates various classification models.\n","                8.  **Model Explainability (Future):** Provides insights into model predictions.\n","                \"\"\"\n","            )\n","\n","        with gr.Tab(\"Basic Data Info\"):\n","            gr.Markdown(\"---\")\n","            gr.Markdown(\"### 1.2.1 Basic Info about Data and Target Variable Definition\")\n","            basic_info_output = gr.Markdown(get_basic_data_info())\n","\n","        with gr.Tab(\"EDA Plots\"):\n","            gr.Markdown(\"---\")\n","            eda_info_output = gr.Markdown(\"Loading EDA plot information...\")\n","            eda_plot_gallery = gr.Gallery(\n","                label=\"Exploratory Data Analysis Plots\",\n","                columns=[4], rows=[2], object_fit=\"contain\", height=\"auto\"\n","            )\n","            demo.load(get_eda_plots, inputs=None, outputs=[eda_info_output, eda_plot_gallery])\n","\n","        with gr.Tab(\"Data Preprocessing Summary\"):\n","            gr.Markdown(\"---\")\n","            gr.Markdown(\"### 1.3 Data Cleaning and Preprocessing for Modeling\")\n","            preprocessing_summary_output = gr.Markdown(get_preprocessing_summary())\n","\n","        with gr.Tab(\"Data Profiling Report\"):\n","            gr.Markdown(\"---\")\n","            gr.Markdown(\"### 1.4 Data Profiling Link\")\n","            profiling_link_output = gr.Markdown(get_data_profiling_link())\n","\n","    with gr.Tab(\"2. Model Training & Evaluation\"):\n","        gr.Markdown(\"## Machine Learning Model Training and Performance Evaluation\")\n","        gr.Markdown(\"---\")\n","        model_eval_summary_output = gr.Markdown(\"Loading model evaluation results...\")\n","        model_eval_plot_gallery = gr.Gallery(\n","            label=\"Model Evaluation Plots (Test Set)\",\n","            columns=[4], rows=[2], object_fit=\"contain\", height=\"auto\"\n","        )\n","        demo.load(get_modeling_and_evaluation_results, inputs=None, outputs=[model_eval_summary_output, model_eval_plot_gallery])\n","\n","    with gr.Tab(\"3. Model Explainability\"):\n","        gr.Markdown(\"## Understanding Model Decisions\")\n","        gr.Markdown(\"---\")\n","        # explainability_output = gr.Markdown(get_model_explainability_info()) # REMOVE THIS LINE\n","\n","        shap_info_output = gr.Markdown(\"Loading SHAP plot information...\")\n","        shap_plot_gallery = gr.Gallery(\n","            label=\"SHAP Explanations\",\n","            columns=[2], rows=[1], object_fit=\"contain\", height=\"auto\"\n","        )\n","        demo.load(get_shap_plots, inputs=None, outputs=[shap_info_output, shap_plot_gallery])\n","        # Optionally, keep the general info above the plots if desired\n","        gr.Markdown(get_model_explainability_info())\n","\n","\n","if __name__ == \"__main__\":\n","    # Ensure all directories exist before trying to load/save files\n","    os.makedirs(raw_data_dir, exist_ok=True)\n","    os.makedirs(processed_data_dir, exist_ok=True)\n","    os.makedirs(reports_dir, exist_ok=True)\n","    os.makedirs(eda_plots_dir, exist_ok=True)\n","    os.makedirs(model_eval_plots_dir, exist_ok=True)\n","    os.makedirs(models_dir, exist_ok=True)\n","    os.makedirs(model_params_dir, exist_ok=True)\n","\n","    # You would typically run your main pipeline first to generate outputs\n","    # For demonstration, we assume your main.py has already run and saved artifacts.\n","    logger.info(\"Starting Gradio App. Ensure your ML pipeline (main.py) has been executed to generate necessary reports and plots.\")\n","    demo.launch(share=True) # Set share=True to get a public link for Colab"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":645},"id":"EBgHVFQ3C8Qs","executionInfo":{"status":"ok","timestamp":1752602639549,"user_tz":-120,"elapsed":1901,"user":{"displayName":"Jai Kushwaha","userId":"01340704951735287618"}},"outputId":"40653fce-074c-41ed-98ec-e781541a2a3e"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO: Starting Gradio App. Ensure your ML pipeline (main.py) has been executed to generate necessary reports and plots.\n","INFO:__main__:Starting Gradio App. Ensure your ML pipeline (main.py) has been executed to generate necessary reports and plots.\n"]},{"output_type":"stream","name":"stdout","text":["Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://dfa178a44a3fc3e08b.gradio.live\n","\n","This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://dfa178a44a3fc3e08b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"4MjUd1N7SHnO"},"execution_count":null,"outputs":[]}]}